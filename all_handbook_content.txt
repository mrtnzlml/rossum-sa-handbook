---
title: 'Duplicates detection'
sidebar_position: 1
---

import WIP from '../\_wip.md';

## Installation

<WIP issue="https://github.com/rossumai/university/issues/404" />

## Basic usage

<WIP issue="https://github.com/rossumai/university/issues/404" />

## Available configuration options

<WIP issue="https://github.com/rossumai/university/issues/404" />


TODO: update based on https://knowledge-base.rossum.ai/docs/duplicate-handling-extension and https://gitlab.rossum.cloud/exe/sex/-/blob/master/bases/sex/duplicate_handling/models/logic_actions.py#L85-86

```json
{
  "debug": false,
  "configurations": [
    {
      "logic": [
        {
          "rules": [
            {
              "id": 1,

              // One of: "field", "filename", "relation"
              "attribute": "field",

              // Datapoint schema ID
              "field_schema_id": "document_id"
            }
            // …
          ],
          "scope": {
            // One of: "queue", "workspace", "organization"
            "object": "queue",
            "statuses": [
              "failed_import",
              "split",
              "to_review",
              "reviewing",
              "in_workflow",
              "confirmed",
              "rejected",
              "exporting",
              "exported",
              "failed_export",
              "postponed",
              "deleted"
            ]
          },
          "actions": [
            {
              "type": "fill_field",
              "field_to_fill": "is_rossum_duplicate",
              "value_to_fill": "true"
            }
          ],

          // Defines how to combine rules. You can specify a list of rule IDs or use logical "and"
          // operations between rule IDs. Each list element acts as a logical "or" operation.
          "matching_flow": ["1and2", "3"]
        }
      ],
      "trigger_events": ["annotation_content"],
      "trigger_actions": ["initialize", "started", "user_update", "updated"]
    }
  ]
}
```
---
sidebar_position: 1
title: 'Duplicates detection: Configuration examples'
sidebar_label: 'Configuration examples'
---

# Configuration examples

## Detect duplicates based on fields and relations

This is probably the most common use case for duplicates detection. It detects duplicates based on the following:

- Content of `document_id` field
- Content of `supplier_id` field (since two suppliers can use the same numbering scheme)
- Compares binary content of the files ("relation")

If duplicate is detected, field `is_rossum_duplicate` is set to `true`.

<details>
  <summary>Recommended configuration of the `is_rossum_duplicate` datapoint.</summary>

```json
{
  "rir_field_names": [],
  "constraints": {
    "required": true
  },
  "score_threshold": 0.0,
  "default_value": "false",
  "category": "datapoint",
  "id": "is_rossum_duplicate",
  "label": "is_rossum_duplicate",
  "hidden": true,
  "disable_prediction": true,
  "type": "enum",
  "can_export": false,
  "ui_configuration": {
    "type": "data",
    "edit": "disabled"
  },
  "options": [
    {
      "value": "true",
      "label": "true"
    },
    {
      "value": "false",
      "label": "false"
    }
  ]
}
```

</details>

### Based the `/annotations/search` endpoint and MQL query

```json
{
  "configurations": [
    {
      "logic": [
        {
          "actions": [
            {
              "type": "mark_duplicate"
            },
            {
              "type": "fill_field",
              "field_to_fill": "is_rossum_duplicate",
              "value_to_fill": "true"
            }
          ],
          "search_query": {
            "query": {
              "$and": [
                {
                  "status": {
                    "$nin": ["split", "purged", "deleted"]
                  }
                },
                {
                  "field.document_id.string": {
                    "$ne": ""
                  }
                },
                {
                  "field.document_id.string": {
                    "$eq": "@{document_id}"
                  }
                },
                {
                  "field.sender_match.string": {
                    "$ne": ""
                  }
                },
                {
                  "field.sender_match.string": {
                    "$eq": "@{supplier_id}"
                  }
                }
              ]
            }
          }
        }
      ],
      "trigger_events": ["annotation_content"],
      "trigger_actions": ["initialize", "started", "updated"]
    },
    {
      "logic": [
        {
          "rules": [
            {
              "id": 1,
              "attribute": "relation"
            }
          ],
          "actions": [
            {
              "type": "mark_duplicate"
            },
            {
              "type": "fill_field",
              "field_to_fill": "is_rossum_duplicate",
              "value_to_fill": "true"
            }
          ],
          "matching_flow": ["1"]
        }
      ],
      "trigger_events": ["annotation_content"],
      "trigger_actions": ["initialize"]
    }
  ]
}
```

### Based the `/annnotations` endpoint

```json
{
  "configurations": [
    {
      "logic": [
        {
          "rules": [
            {
              "id": 1,
              "attribute": "field",
              "field_schema_id": "document_id"
            },
            {
              "id": 2,
              "attribute": "field",
              "field_schema_id": "supplier_id"
            },
            {
              "id": 3,
              "attribute": "relation"
            }
          ],
          "scope": {
            "object": "queue",
            "statuses": [
              "failed_import",
              "split",
              "to_review",
              "reviewing",
              "in_workflow",
              "confirmed",
              "rejected",
              "exporting",
              "exported",
              "failed_export",
              "postponed"
            ]
          },
          "actions": [
            {
              "type": "mark_duplicate"
            },
            {
              "type": "fill_field",
              "field_to_fill": "is_rossum_duplicate",
              "value_to_fill": "true"
            }
          ],
          "matching_flow": ["1and2", "3"]
        }
      ],
      "trigger_events": ["annotation_content"],
      "trigger_actions": ["initialize", "started", "user_update", "updated"]
    }
  ]
}
```

Later, users can decide what to do with the `is_rossum_duplicate` information. Typically, we display warning or error message.
---
title: 'Export pipelines: REST API export'
sidebar_label: '3. REST API export'
sidebar_position: 3
---

import WebhookEndpoints from '../\_webhook_endpoints.md';
import WIP from '../\_wip.md';

# REST API export

```py
try:
        return httpx.Request(
            url=request.url,
            method=request.method,
            headers=request.headers,
            params=request.params,
            files=files,
            # If request.data is a string, it is encoded by HTTPX and used to override content.
            data=request.data,  # type: ignore[arg-type]
            content=request.content,
        )
    except httpx.InvalidURL as e:
        raise InvalidURLError(f"{e!s}. URL: {request.url}")
```

```py
class RestAPIExportRequest(BaseModel):
    url: str
    method: RestAPIExportRequestMethod
    file_key: str | None = None
    file_name: str | None = None
    file_content: str | bytes | None = None
    headers: dict[str, str] = Field({})
    params: dict[str, PrimitiveData] = Field({})
    data: str | dict[str, str] | None = None
    content: Any | None = None

    @model_validator(mode="after")
    def validate_export_documents(self) -> Self:
        if self.file_name and not self.file_key:
```

The **REST API Export** extension is responsible for sending exported data from Rossum to an external REST API. It is designed to be a part of the [Export Pipeline](./index.md) and works with [Custom Format Templating](./custom-format-templating.md) to prepare the data before transmission.

## Installation

REST API export extension is provided and maintained by Rossum.ai in the form of webhook. To start using it, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `REST API export`
   1. Trigger events: `Export`
   1. Extension type: `Webhook`
   1. URL (see below)
1. In "Advanced settings" select **Token owner** (should have Admin access)
1. Click **Create the webhook**.
1. Fill in the `Configuration` field (see [Configuration examples](#configuration-examples) for some examples).

Webhook URL endpoints:

<WebhookEndpoints
  eu1="https://elis.rest-api-export.rossum-ext.app/"
  eu2="https://shared-eu2.rest-api-export.rossum-ext.app/"
  us="https://us.rest-api-export.rossum-ext.app/"
  jp="https://shared-jp.rest-api-export.rossum-ext.app/"
/>

## Configuration examples

This extension works as a part of the [Export Pipeline](./index.md) and it expects a payload file to be generated using [Custom format templating extension](./custom-format-templating.md).

### Simple REST API call

A basic example of exporting data via a REST API.

```json
{
  "export_reference_key": "export_annotation_to_csv",
  "request": {
    "url": "https://webhook.site/XXX-ZZZ",
    "method": "POST",
    "headers": {
      "Content-Type": "text/plain"
    },
    "content": "#{file_content}"
  }
}
```

### REST API call with OAuth2

The request can be extended to use OAuth2:

```json
{
  "export_reference_key": "export_annotation_to_csv",
  "auth": {
    "url": "http://custom.url/token",
    "method": "POST",
    "data": {
      "username": "your_username",
      "password": "{secret.password}"
    }
  },
  "request": {
    "url": "https://webhook.site/XXX-ZZZ",
    "method": "POST",
    "headers": {
      "Content-Type": "text/plain",
      "Authorization": "Bearer {secret.access_token}"
    },
    "content": "#{file_content}"
  }
}
```

The `access_token` is automatically retrieved using given credentials and saved to hook secrets for later reuse. A more complex example using OAuth2 authentication, with response storage and a condition controlling execution:

```json
{
  "export_reference_key": "export_annotation_to_csv",
  "request": {
    "url": "https://webhook.site/XXX-ZZZ",
    "method": "POST",
    "headers": {
      "Content-Type": "text/json"
    },
    "content": "#{file_content}"
  },
  "auth": {
    "url": "http://custom.url/token",
    "method": "POST",
    "data": {
      "scope": "invoice.create invoice.read",
      "client_id": "{secret.client_id}",
      "grant_type": "client_credentials",
      "client_secret": "{secret.client_secret}"
    },
    "headers": {
      "Content-Type": "application/x-www-form-urlencoded"
    }
  },
  "condition": "@{api_gate}",
  "response_headers_reference_key": "export_reply_headers",
  "response_payload_reference_key": "export_reply_payload"
}
```

The `response_headers_reference_key` stores the headers of the reply with added `status_code`, `headers` converted to json strucutre and `raw` headers. See the following example:

```json
{
  "status_code": 200,
  "headers": {
    "access_control_allow_origin": "*",
    "alt_svc": "h3=\":443\"; ma=2592000",
    "content_type": "application/json",
    "date": "Tue, 25 Jun 2024 08:02:26 GMT",
    "vary": "Accept-Encoding",
    "transfer_encoding": "chunked"
  },
  "raw": [
    ["Access-Control-Allow-Origin", "*"],
    ["Alt-Svc", "h3=\":443\"; ma=2592000"],
    ["Content-Type", "application/json"],
    ["Date", "Tue, 25 Jun 2024 08:02:26 GMT"],
    ["Vary", "Accept-Encoding"],
    ["Transfer-Encoding", "chunked"]
  ]
}
```

The `response_payload_reference_key` stores the full body from the reply.

Both of them are stored as documents in Rossum and can be retrieved via API. There is an extension prepared for you, extracts important information from the response, key values which can be then stored to the Rossum annontation [Data value extractor](./data-value-extractor.md).

For sending original document file e.g in the API POST request use `#{original_file}` For exampe this is useful when attachning original invoice file to existing invoice object in an ERP system. In the followoing exmaple the `export_reference_key` is not used because it is not needed, we are senting only the original document file, not usinfg a previously generated payload.

```json
{
  "request": {
    "url": "https://apiexport.myserver.com/attachment",
    "method": "POST",
    "content": "#{original_file}",
    "headers": {
      "Content-Type": "application/pdf"
    }
  },
  "response_headers_reference_key": "api_attach_export_reply_headers",
  "response_payload_reference_key": "api_attach_export_reply_payload"
}
```

You can use condition that controls whether the export is triggered. When it's empty or "false" (case insensitive), this section won't be evaluated. Otherwise, it will proceed. E.g. if the referred field is non-empty (`!= ""`) it will start export and skip it if it is empty (`== ""`). The condition definition follows the [JSON templating](./../json-templating/index.md) syntax, e.g. `"condition": "@{api_gate}"`.

### Sending `multipart/form-data`

The following config will translate in an HTTP POST request with `Content-Type: multipart/form-data`. Both file and additional data will be sent. For sending the file, the `file_key` is used as a form-data `name; filename` and content type are taken from the saved document.

```json
{
  "export_reference_key": "exported_annotation_csv",
  "request": {
    "url": "https://webhook.site/XXX-ZZZ",
    "method": "POST",
    "headers": {
      "Authorization": "Bearer {secret.token}"
    },
    "file_name": "@{document_id}.json",
    "file_key": "file",
    "request_data": {
      "other_field": "@{vendor}"
    }
  }
}
```

See also [JSON templating](../json-templating/index.md).

### Sending `application/x-www-form-urlencoded`

Specifically, this example is for Azure API Management:

```json
{
  "export_reference_key": "exported_annotation_csv",
  "auth": {
    "url": "",
    "method": "POST",
    "headers": {
      "Ocp-Apim-Subscription-Key": "",
      "Content-Type": "application/x-www-form-urlencoded"
    },
    "data": {
      "grant_type": "",
      "client_id": "",
      "client_secret": "{secret.client_secret}",
      "scope": ""
    }
  },
  "request": {
    "url": "",
    "method": "POST",
    "content": "#{file_content}",
    "headers": {
      "Content-Type": "application/json",
      "Ocp-Apim-Subscription-Key": ""
    }
  }
}
```

## Status Code Resolver

The `status_code_resolver` is a tool that allows you to define error handling, that means how HTTP status codes from the API response are handled in Rossum.

- **error**: A list of status codes that should be considered as errors. They will cause the export process to fail and will be displayes as errors on the document in Rossum. The content of the error message is the content sent by the API.
- **warning**: A list of status codes that should be considered warnings. They will NOT cause the export process to fail, but will be shown as warning message. The content of the warning message is the content sent by the API.
- All other status codes will be considered successful.

By default when no error handling by `status_code_resolver` is defined, all status codes are considered successful.
The resolver only applies to the export request, not the authentication request.

If you are using the OAuth2 authentication method, keep in mind that the hook uses the HTTP status code 401 (Unauthorized) to detect when authentication is required. So in that case, it will always try to authenticate first, regardless of the status code resolver. However, if the export request fails again after a successful authentication, the hook will consider the status code according to the resolver. For example, if you have 401 listed as an error status code, the export process will fail if the export request returns 401 after a successful authentication.

Example configuration with status code resolver:

```json
{
  "request": {
    "url": "https://apiexport.yourserver.com/payload",
    "method": "POST",
    "content": "#{file_content}",
    "headers": {
      "Content-Type": "application/xml"
    }
  },
  "export_reference_key": "export_annotation_to_xml_bas64",
  "status_code_resolver": {
    "error": [403, 500],
    "warning": [201, 202, 204, 206]
  },
  "response_headers_reference_key": "api_xml_export_reply_headers",
  "response_payload_reference_key": "api_xml_export_reply_payload"
}
```

## Available configuration options

### Export Object

The export object consists of the following parameters:

| Attribute                                     | Type   | Description                                                                                                                                                                                                                                                                                                          |
|-----------------------------------------------|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `export_reference_key`                        | str    | A unique key referencing the exported data prepared by the [Custom format templating extension](./custom-format-templating.md).                                                                                                                                                                                      |
| `request`                                     | object | Defines the REST API request (URL, method, headers, and body content).                                                                                                                                                                                                                                               |
| `auth` _(optional)_                           | object | Configuration for authentication, supporting OAuth2.                                                                                                                                                                                                                                                                 |
| `condition` _(optional)_                      | str    | Reference to a `schema_id` in `annotation.content` controlling execution. When it's empty or "false" (case insensitive), this section won't be evaluated. Otherwise, it will proceed. The condition definition follows the [JSON templating](./../json-templating/index.md) syntax e.g. `"condition": "@{api_gate}"` |
| `response_headers_reference_key` _(optional)_ | str    | Key to store API response headers (including `status_code`).                                                                                                                                                                                                                                                         |
| `response_payload_reference_key` _(optional)_ | str    | Key to store the full response body from the API.                                                                                                                                                                                                                                                                    |
| `status_code_resolver` _(optional)_           | object | Defines status code handling.                                                                                                                                                                                                                                                                                        |
| `mode` _(optional)_                           | str    | Resiliency mode for the export request. Mode "wait" is designed for slow APIs so the timeout is high. Mode "retry" is designed for fast but flaky APIs so the request is retried a few times in case it fails. Defaults to "wait".                                                                                   |

### Request Object

The `request` object contains parameters defining the HTTP request:

| Attribute | Type   | Description                                                             |
| --------- | ------ | ----------------------------------------------------------------------- |
| `url`     | str    | The target API endpoint URL.                                            |
| `method`  | str    | The HTTP method (`POST`, `PUT`, etc.).                                  |
| `headers` | object | HTTP headers for the request.                                           |
| `content` | str    | The body of the request (supports placeholders like `#{file_content}`). |

### Authentication Object (`auth`)

The authentication object defines how credentials are used to obtain an access token.

| Attribute | Type   | Description                                    |
| --------- | ------ | ---------------------------------------------- |
| `url`     | str    | The authentication endpoint URL.               |
| `method`  | str    | The HTTP method (`POST`, `GET`, etc.).         |
| `headers` | object | Headers for authentication request.            |
| `data`    | object | Credentials and grant type for authentication. |

### Status Code Resolver Object

Dictionary defining how to to handle different HTTP status codes in Rossum.

| Attribute | Type             | Description                                                   |
| --------- | ---------------- | ------------------------------------------------------------- |
| `error`   | list of integers | List of error status codes (export fails).                    |
| `warning` | list of integers | List of warning status codes (export continues with warning). |
---
title: 'Export pipelines: Custom format templating'
sidebar_label: '2. Custom format templating'
sidebar_position: 2
---

import WebhookEndpoints from '../\_webhook_endpoints.md';

# Custom format templating

**Custom format templating** extension allows users to create custom files in almost any shape or form. It works by defining a custom template and later calling it on export with the annotation data.

## Installation

**Custom format templating** extension is provided and maintained by Rossum.ai in the form of webhook. To start using it, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `Custom format templating`
   1. Trigger events: `Export`
   1. Extension type: `Webhook`
   1. URL (see below)
1. In "Advanced settings" select **Token owner** (should have Admin access)
1. In the "Additional notification metadata" enable `Schemas`
1. Click **Create the webhook**.

<WebhookEndpoints
  eu1="https://elis.custom-format-templating.rossum-ext.app/"
  eu2="https://shared-eu2.custom-format-templating.rossum-ext.app/"
  us="https://us.custom-format-templating.rossum-ext.app/"
  jp="https://shared-jp.custom-format-templating.rossum-ext.app/"
/>

## Available configuration options

```json
{
  "export_configs": [
    {
      // Optional. Specifies resulting file content encoding. Default: "utf-8".
      "content_encoding": "utf-8",

      // Name of the export template used for later reference in the export pipeline.
      "export_reference_key": "example_invoice_template",

      // In-line template of the file to be exported. You can either use this in-line version or
      // use `file_content_template_multiline` for multiple lines instead.
      "file_content_template": "…",

      // Multiline template of the file to be exported. You can either use this multipline version
      // or use `file_content_template` for in-line templates instead.
      "file_content_template_multiline": [
        // Rows of the template. Each row is a new line (no need for "\n" separators).
        "…",
        "…"
      ]
    }
    // …
  ]
}
```

In the template definitions (`file_content_template` or `file_content_template_multiline`) you can reference any field from annotation object by using dot syntax (for example `{{field.document_id}}`).

It is also possible to access the annotation payload like so: `{{payload['document']['original_file_name']}}`. This way, you can create a link to the annotation directly in the template, for example: `https://example.rossum.app/document/{{payload['annotation']['id']}}`

:::warning

Maximum five export configs can be defined per annotation export.

:::

:::warning

It is important to **sanitize fields** used in the template (remove new lines, special characters, etc. so that these special characters conform to the expected format specification). It is not possible to control the whole document format so the sanitization must be performed on a field-by-field basis.

:::

## Obtaining generated files

Outputs from the **Custom format templating** webhook are not available anywhere in the UI. Instead, they are stored as a "[document relation](https://elis.rossum.ai/api/docs/#document-relation)". To get the actual document content, you need to call two API endpoints. First, you need to get the relevant relation:

```bash
curl 'https://elis.rossum.ai/api/v1/document_relations?type=export&annotation=1234567&key=export_annotation_to_csv' \
--header 'Authorization: Bearer XYZ'
```

You can omit the `key` to get all available relations for given annotation. The response will look something like this:

```json
{
  "pagination": { "total": 1, "total_pages": 1, "next": null, "previous": null },
  "results": [
    {
      "id": 1234567,
      "type": "export",
      "key": "export_annotation_to_csv",
      "annotation": "https://elis.rossum.ai/api/v1/annotations/1234567",
      // highlight-start
      "documents": ["https://elis.rossum.ai/api/v1/documents/1234567"],
      // highlight-end
      "url": "https://elis.rossum.ai/api/v1/document_relations/1234567"
    }
  ]
}
```

To get the actual generated document content, you need to call the following API endpoint (see the highlighted document URL above):

```bash
curl 'https://elis.rossum.ai/api/v1/documents/1234567/content' \
--header 'Authorization: Bearer XYZ'
```

Note that you do not need to know these technical details. This extension is typically to be used in combination with [REST API Export extension](./rest-api-export.md) which knows how to work with it.

## Configuration examples

### Original document (base64)

```json
{
  "export_configs": [
    {
      "export_reference_key": "export_original_file",
      "file_content_template": "{{file.original_file|b64encode}}"
    }
  ]
}
```

### Custom CSV

Define CSV header fields as well as the actual datapoints to be exported:

```json
{
  "export_configs": [
    {
      "export_reference_key": "export_annotation_to_csv",
      "file_content_template": "Document ID,Document Type,Quantity\n{{field.document_id}},{{field.document_type}},{{field.line_items[0].item_quantity}}"
    }
  ]
}
```

Alternatively, it is possible to leverage `file_content_template_multiline` for better readability:

```json
{
  "export_configs": [
    {
      "export_reference_key": "export_annotation_to_csv",
      "file_content_template_multiline": [
        "Document ID,Document Type,Quantity",
        "{{field.document_id}},{{field.document_type}},{{field.line_items[0].item_quantity}}"
      ],
      "content_encoding": "utf-8"
    }
  ]
}
```

Both of the configurations should generate something like this:

```csv
Document ID,Document Type,Quantity
123456,tax_invoice,750
```

It is also possible to iterate line items:

```json
{
  "export_configs": [
    {
      "export_reference_key": "export_annotation_to_csv",
      "file_content_template_multiline": [
        "Document ID,Document Type,Item Description,Item Quantity",
        "{% for item in field.line_items %}{{field.document_id}},{{field.document_type}},{{item.item_description}},{{item.item_quantity}}\n{% endfor %}"
      ],
      "content_encoding": "utf-8"
    }
  ]
}
```

The generated CSV would now contain all the line items, for example:

```csv
Document ID,Document Type,Item Description,Item Quantity
123456,tax_invoice,AWS services #1,750
123456,tax_invoice,AWS services #2,750
123456,tax_invoice,AWS services #3,750
```

Learn how to get the generated file in [Obtaining generated files](#obtaining-generated-files) section.

### Custom XML

Similarly to other formats, custom XML can be defined using the following template (notice the `|upper` and `|default` [filters](https://jinja.palletsprojects.com/en/stable/templates/#filters)):

```json
{
  "export_configs": [
    {
      "export_reference_key": "export_annotation_to_xml",
      "content_encoding": "utf-8",
      "file_content_template_multiline": [
        "<ROSSUM>",
        "  <INVOICE>",
        "    <HEADER>",
        "      <DOCUMENT_ID>{{ field.document_id }}</DOCUMENT_ID>",
        "      <DOCUMENT_TYPE>{{ field.document_type }}</DOCUMENT_TYPE>",
        "      <DOCUMENT_LANGUAGE>{{ field.language }}</DOCUMENT_LANGUAGE>",
        "      <DATE_ISSUE>{{ field.date_issue }}</DATE_ISSUE>",
        "      <DATE_DUE>{{ field.date_due }}</DATE_DUE>",
        // highlight-start
        "      <CURRENCY>{{ field.currency | upper }}</CURRENCY>",
        // highlight-end
        "      <AMOUNT_TOTAL>{{ field.amount_total }}</AMOUNT_TOTAL>",
        // highlight-start
        "      <AMOUNT_TOTAL_TAX>{{ field.amount_total_tax | default(0,true) }}</AMOUNT_TOTAL_TAX>",
        // highlight-end
        "    </HEADER>",
        "  </INVOICE>",
        "</ROSSUM>"
      ]
    }
  ]
}
```

:::tip

If the fields may include any of the following chars (`>`, `<`, `&`, or `"`) you **SHOULD** escape it unless the variable contains well-formed and trusted HTML/XML like so: `{ field.name|e }`

You can learn more about this (and other) filter(s) here: https://jinja.palletsprojects.com/en/stable/templates/#working-with-manual-escaping

:::

### Custom JSON

Creates custom JSON templates with some header fields and line items (iteration over the line items is highlighted).

```json
{
  "export_configs": [
    {
      "export_reference_key": "export_annotation_to_json",
      "content_encoding": "utf-8",
      "file_content_template_multiline": [
        "{",
        "  \"document_id\": \"{{ field.document_id }}\",",
        "  \"document_type\": \"{{ field.document_type }}\",",
        "  \"line_items\": [",
        // highlight-start
        "    {% for item in field.line_items %}{",
        // highlight-end
        "      \"code\": \"{{ item.item_code }}\",",
        "      \"description\": \"{{ item.item_description }}\",",
        "      \"quantity\": {{ item.item_quantity }},",
        "      \"amount\": {{ item.item_amount }},",
        // highlight-start
        "    }{% if not loop.last %},{% endif %}",
        "  {% endfor %}]",
        // highlight-end
        "}"
      ]
    }
  ]
}
```

### Custom TXT (The Tree)

This is example of the Jinja2 language capabilities. The following example is a small Jinja program that generates simple ASCII art representation of a tree. The tree height is a parameter which can be changed, see line 15 in the example config.

```json
{
  "export_configs": [
    {
      "export_reference_key": "export_tree_ascii_art",
      "content_encoding": "utf-8",
      "file_content_template_multiline": [
        "{%- macro draw_tree(height) -%}",
        "{%- set width = height * 2 - 1 %}",
        "{%- for level in range(1, height + 1) -%}",
        "{%- set padding = (width - (level * 2 - 1)) // 2 %}",
        "{{ ' ' * padding }}{{ '*' * (level * 2 - 1) }}{{ ' ' * padding }}",
        "{%- endfor %}",
        "{{ ' ' * ((width - 1) // 2) }}|{{ ' ' * ((width - 1) // 2) }}",
        "{%- endmacro %}",
        // highlight-start
        "{{ draw_tree(5) }}",
        // highlight-end
        ""
      ]
    }
  ]
}
```

Result is a generated TXT file representing a tree with crown height of 5 rows:

```text
    *
   ***
  *****
 *******
*********
    |
```

### Custom EDIFACT

```json
{
  "export_configs": [
    {
      "content_encoding": "utf-8-sig",
      "export_reference_key": "export_edifact",
      "file_content_template_multiline": [
        "{% set ns = namespace(counter=33) %}",
        "UNA:+.?*'",
        "UNB+UNOB:4::+123456789+123456789+{{field.unique_id}}'",
        "UNH+{{field.unique_id}}+INVOIC:D:10A:UN+++++SCAN'",
        "BGM+{{document_type}}+{{invoice_number}}+43'",
        "DTM+137:{{field.invoice_date}}+:102'",
        "DTM+243:{{scan_date}}:102'",
        "FTX+INV+3++40{{field.hand_written_segment}}+de'",
        "RFF+AFF:{{field.unique_id}}'",
        "RFF+AWK:Acknowledgement_{{field.unique_id}}.pdf'",
        "RFF+AJK:{{field.invoice_type_id}}X'",
        "RFF+ANJ:AuthNum_Test'",
        "RFF+ON:{{field.order_number}}'",
        "RFF+IP:{{import_number}}'",
        "NAD+SU+::92++{{field.sender_name}}+{{field.sender_street}}+{{field.sender_city}}++{{field.sender_zip}}+{{field.sender_country}}'",
        "FII+SU+bankaccount:account holder+Default Swift:25:5:Defaultbankkey:25:131+GB'",
        "RFF+PQ:qrcodeTest'",
        "RFF+AHP:{{field.sender_vat_id}}'",
        "CTA+AP+person:department'",
        "COM+066666666:TE'",
        "NAD+UC+::92++Dummy +Dummy +Dummy ++12345+IN'",
        "NAD+BY+{{field.buyer_org_id}}::234++{{field.recipient_name}}+{{field.recipient_street}}+{{field.recipient_city}}++{{field.recipient_zip}}+{{field.recipient_country}}'",
        "NAD+MS+A1730832::234++Example +Example +Example ++91052+DE'",
        "CUX+2:{{field.currency}}:4:1+6:{{field.api_currency}}:18:1+{{field.exchange_rate}}+ECR'",
        "DTM+134:{{field.scan_date}}:102'",
        "LIN+1++500123456789:BP'",
        "QTY+47:1.000:PCE'",
        "MOA+66:{{field.amount_due}}'",
        "MOA+203:{{field.amount_due}}'",
        "PRI+INV:{{field.amount_due}}:::1.000:PCE'",
        "UNS+S'",
        "MOA+39:{{field.amount_due}}'",
        "MOA+340:{{field.amount_total_base}}'",
        "{% for item in field.tax_details %}TAX+7+VAT:::C1+AR++:::{{item.tax_detail_rate}}'\nMOA+125:{{item.tax_detail_total}}'\nMOA+124:{{field.amount_total_base}}'{% set ns.counter = ns.counter + 1 %}{% if not loop.last %}\n{% endif %}{% endfor %}",
        "ALC+C+:+++SC:::'",
        "MOA+8:0.00'",
        "UNT+{{ns.counter}}+0'",
        "UNZ+1+00000000000001'"
      ]
    }
  ]
}
```
---
title: 'Export pipelines: Custom format templating purge'
sidebar_label: '1. Custom format templating purge'
sidebar_position: 1
---

import WebhookEndpoints from '../\_webhook_endpoints.md';
import WIP from '../\_wip.md';

# Custom format templating purge

Extensions like [Custom format templating](./custom-format-templating.md) and [REST API export](./rest-api-export.md) (and possibly others) store their API responses and artefacts in the annotation metadata. When re-exporting, we need to remove these old documents and responses to ensure a clean start. The purge extension helps us achieve this.

Additionally, this extension takes care of a cleanup when purging the original documents from Rossum (by default, all generated artifacts would stay in the system orphaned). Deleting such orphaned artifacts might be very important for **compliance reasons**.

## Installation

"Custom format templating purge" extension is provided and maintained by Rossum.ai in the form of webhook. To start using it, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `Custom format templating purge`
   1. Trigger events: `Export` and document status `Changed` (!)
   1. Extension type: `Webhook`
   1. URL (see below)
1. In "Advanced settings" select **Token owner** (should have Admin access)
1. In the "Additional notification metadata" enable `Schemas`
1. Click **Create the webhook**.

### Webhook URL endpoints

<WebhookEndpoints
  eu1="https://elis.custom-format-templating-purge.rossum-ext.app/"
  eu2="https://shared-eu2.custom-format-templating-purge.rossum-ext.app/"
  us="https://us.custom-format-templating-purge.rossum-ext.app/"
  jp="https://shared-jp.custom-format-templating-purge.rossum-ext.app/"
/>

## Basic usage

No additional configuration is required. This extension should be run first in the extension chain.
---
title: 'Export pipelines: Data value extractor'
sidebar_label: '4. Data value extractor'
sidebar_position: 4
---

import WebhookEndpoints from '../\_webhook_endpoints.md';

# Data value extractor

The Data Value Extractor serves to extract data from a document that is linked in annotation's metadata. The main use case is to process data from [REST API Export](./rest-api-export.md) as a part of the [Export Pipeline](./index.md).

## Installation

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `Data value extractor`
   1. Trigger events: `Export`
   1. Extension type: `Webhook`
   1. URL (see below)
1. In "Advanced settings" select **Token owner** (should have Admin access)
1. Click **Create the webhook**.

<WebhookEndpoints
  eu1="https://elis.data-value-extractor.rossum-ext.app/"
  eu2="https://shared-eu2.data-value-extractor.rossum-ext.app/"
  us="https://us.data-value-extractor.rossum-ext.app/"
  jp="https://shared-jp.data-value-extractor.rossum-ext.app/"
/>

## Available configuration options

Simple extraction example.

```json
{
  "extract": [
    {
      "format": "json",
      "source_reference_key": "ifs_export_reply_payload",
      "extract_rules": [
        {
          "value_path": "MessageId[0].value",
          "target_schema_id": "ifs_reply_message_id"
        }
      ]
    }
  ]
}
```

More complex configuration example using extraction from two different `source_reference_key` and two `extract_rules` in the second one. There is also the `condition` used, which is reference to a document ID in the annotation which triggers the execution of the extraction.

```json
{
  "extract": [
    {
      "format": "json",
      "extract_rules": [
        {
          "value_path": "doc_id",
          "target_schema_id": "erp_doc_id"
        }
      ],
      "source_reference_key": "api_xml_export_reply_payload"
    },
    {
      "format": "json",
      "condition": "@{api_gate}",
      "extract_rules": [
        {
          "value_path": "status_code",
          "target_schema_id": "erp_api_status_code"
        },
        {
          "value_path": "headers.etag",
          "target_schema_id": "erp_api_etag"
        }
      ],
      "source_reference_key": "api_xml_export_reply_headers"
    }
  ]
}
```

## Parameters

### Extract Object

The extract object consists of the following parameters:

| Attribute              | Type   | Description                                                                                                                                                                                                                                                                                               |
| ---------------------- | ------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `format`               | str    | File format. Currently, only `json` value is supported.                                                                                                                                                                                                                                                   |
| `condition`            | str    | Reference to `annotation.content` `schema_id` that holds evaluated value. When it's empty or "false" (case insensitive), this section won't be evaluated. Otherwise, it will proceed. The condition follows the [JSON templating](./../json-templating/index.md) syntax e.g. `"condition": "@{api_gate}"` |
| `source_reference_key` | str    | Relation key into metadata for source document.                                                                                                                                                                                                                                                           |
| `extract_rules`        | object | Rules to update annotation's content.                                                                                                                                                                                                                                                                     |

The `extract_rules` object defines how values are extracted and stored:

| Attribute          | Type | Description                                                                                                     |
| ------------------ | ---- | --------------------------------------------------------------------------------------------------------------- |
| `value_path`       | str  | Query to get the value from the referred document. In case of `format=json`, it should be in `jmespath` syntax. |
| `target_schema_id` | str  | Annotation's `schema_id` to be updated.                                                                         |
---
title: 'Export pipelines'
sidebar_position: 1
---

import WIP from '../\_wip.md';
import RossumInternalOnly from '../\_rossum_internal_only.md';

After documents are processed in Rossum, the extracted data typically need to be exported to the downstream systems. The Rossum team has prepared an "Export pipeline" for this very purpose.

<RossumInternalOnly url="https://rossumai.atlassian.net/l/cp/t2we9106" />

## Components of Export pipelines

The export pipeline consists of the following components chained together:

1. [Custom format templating purge](./custom-format-templating-purge.md), a cleaning mechanism that prepares the pipeline for export.
1. [Custom format templating](./custom-format-templating.md), prepares the desired format for an export.
1. [REST API export](./rest-api-export.md), exports the prepared data to REST API and stores the reply.
1. [Data value extractor](./data-value-extractor.md) extracts important data from the API reply and stores them in the annotation object, e.g. downstream document ID, HTTP status codes.
1. [Export evaluator](./export-evaluator.md) that decides whether the export is successful or it has failed.
1. Finally, [SFTP Export](../sftp-s3-import-export/index.md), upload the prepared data to SFTP or S3 file storage.

## How to use Export pipelines

All the components of Export pipelines are typically connected by the standard extension chaining mechanism "run-after". Here are several extension chains demonstrated:

### Simple SFTP export pipeline

1. Custom format templating prepares extracted data in desired format.
2. SFTP export stores data in on an SFTP (or S3).

Configuration of `export_rules` of your SFTP extension will look like this then to accept the custom format templating output:

```json
{
  "path_to_directory": "/Path_To_Your_Directory",
  "export_object_configurations": [
    {
      "type": "custom_format",
      "filename_template": "{export_order_name}.json",
      "export_reference_key": "your_reference_key"
    }
  ]
}
```

### Simple API export pipeline

1. (Optional) Pipeline cleaning cleans previous export data (relevant for debugging).
2. Custom format templating prepares extracted data in desired format.
3. REST API calls an external API service and sends the prepared data. The extension also stores returned values including status code.
4. Extract data to store needed information in the document (e.g. status code)
5. Export evaluator that based on condition decides whether the export is succesfful (e.g. status code = 200, 201).

### Complex API export pipeline

<WIP />
---
title: 'Export pipelines: Export evaluator'
sidebar_label: '5. Export evaluator'
sidebar_position: 5
---

import WIP from '../\_wip.md';

# Export evaluator

Export evaluator is a piece of a custom code that evaluates the result of API calls and decides whether the export is successful or it has failed.

## Basic export evaluator

The easiest export evaluator simply checks whether the condition is met or not:

```py
from rossum_python import RossumPython

def rossum_hook_request_handler(payload):
    r = RossumPython.from_payload(payload)

    if eval(payload.get("settings", {}).get("condition")):
        r.show_error("Draft invoice not created.")

    return r.hook_response()
```

Settings example:

```json
{
  "condition": "r.field.api1_status_code != '201'"
}
```

Note that this example uses `eval` function to evaluate the condition which is considered dangerous if the input is untrusted.

## Coupa export evaluator

Coupa REST API typically returns responses in the following format:

```json
{
  "errors": {
    "request": [
      {
        "invoice-header": [
          "Original Invoice Date can't be blank",
          "Original Invoice Number can't be blank"
        ],
        "invoice-header.invoice_lines": ["Total must be negative, make price/quantity negative"],
        "invoice-header.invoice_lines.tax_lines": [
          "Tax Rate should be present on invoices in Italy"
        ]
      }
    ]
  }
}
```

To process such responses, we can use the following evaluator (assuming the API responses and statuses are stored in the appropriate datapoints):

```py
import json
from rossum_python import RossumPython

def rossum_hook_request_handler(payload):
    x = RossumPython.from_payload(payload)

    handle_api_response(x, x.field.api1_status_code, x.field.api1_response_body, "Coupa draft creation", is_error=True)
    handle_api_response(x, x.field.api2_status_code, x.field.api2_response_body, "Coupa image scan", is_error=True)
    handle_api_response(x, x.field.api3_status_code, x.field.api3_response_body, "Coupa backlink", is_error=True)
    handle_api_response(x, x.field.api4_status_code, x.field.api4_response_body, "Coupa submission", is_error=False)

    return x.hook_response()

def handle_api_response(x, status_code, response_body, message, is_error):
    if int(status_code) < 400:
        return

    response_body = try_parse_json(response_body)
    errors = response_body.get('errors', {}).get('request', [{}])[0] if isinstance(response_body, dict) else None

    if errors:
        for key, messages in errors.items():
            for msg in messages:
                display_msg = f"<b>{message} ({key})</b>: {msg.strip()}"
                (x.show_error if is_error else x.show_warning)(display_msg)
    else:
        (x.show_error if is_error else x.show_warning)(f"<b>{message}</b>: unhandled error")

def try_parse_json(response_body):
    try:
        return json.loads(response_body)
    except json.JSONDecodeError:
        return response_body
```
---
sidebar_position: 1
sidebar_label: 'Deployment patterns'
title: 'Sandboxes: Deployment patterns'
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import WIP from '../\_wip.md';

# Deployment patterns

Each company has its own deployment pattern. The most common ones (supported by Rossum Sandboxing tool) are:

## Single environment for sandbox and production

This is the simplest pattern for companies who do not have the need for separate sandbox environment.

![](./_img/one-environment-diagram.png)

<Tabs groupId="prd">
  <TabItem value="prd2" label="v2 (latest)" default>

To start using this patten, simply initialize a new project in some empty directory using the following command:

```bash
prd2 init
```

This command will create a directory structure according to your choices when executing the `init` command. For example:

```bash
? ORG-LEVEL directory name: production-org
? ORG ID: 123123
? Base API URL: (e.g., https://my-org.rossum.app/api/v1) https://example.rossum.app/api/v1
? API token: b1946ac92492d2347c6235b4d2611184
? SUBDIR name: default
? subdir regex (OPTIONAL):
? Would you like to specify another **SUBDIR** inside production-org? No
? Would you like to specify another **ORG-LEVEL** directory? No
╭──────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Initialized a new PRD directory in "."                                                           │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
```

Will create the following tree structure:

```text
.
├── prd_config.yaml
└── production-org
    ├── credentials.yaml
    └── default
```

The project should be ready to go! Try it by pulling all the changes from your remote organization:

```bash
prd2 pull production-org
```

Once you commit this initial version to Git, you can update any configuration and deploy it to the same organization using the following command:

```bash
prd2 push
```

  </TabItem>
  <TabItem value="prd" label="v1 (deprecated)">

To start using this patten, simply initialize a new project in some empty directory using the following command:

```bash
prd init
```

This command will create a new `credentials.json` file. Fill in your username and password for `source` (you can leave `target` section as is for this simple setup):

```json title="credentials.json"
{
  "source": {
    "username": "CHANGE ME",
    "password": "CHANGE ME"
  },
  "target": {
    "username": "...",
    "password": "..."
  }
}
```

Also configure the `source_api_base` in `prd_config.yaml` file. By default, it would be:

```yaml title="prd_config.yaml"
source_api_base: 'https://api.elis.rossum.ai/v1'
use_same_org_as_target: true
```

Now you can pull (and eventually commit into Git) your whole organization configuration:

```bash
prd pull
```

Once you commit this initial version to Git, you can update any configuration and deploy it to the same organization using the following command:

```bash
prd push
```

  </TabItem>
</Tabs>

## Two environments for sandbox and production

A bit more advanced setup with two environments: sandbox and production. Typically, the solution is first deployed to the sandbox organization (`source`) and once tested, released to production (`target`).

![](./_img/two-environments-diagram.png)

<Tabs groupId="prd">
  <TabItem value="prd2" label="v2 (latest)" default>

<WIP />

TODO:
prd2 deploy template

  </TabItem>
  <TabItem value="prd" label="v1 (deprecated)">

To use this pattern, follow the same steps as outlined in the [Simple environment example](#single-environment-for-sandbox-and-production) and first init your local repository:

```bash
prd init
```

However, now setup both `source` and `target` organizations:

```json title="credentials.json"
{
  "source": {
    "username": "CHANGE ME",
    "password": "CHANGE ME"
  },
  "target": {
    "username": "CHANGE ME",
    "password": "CHANGE ME"
  }
}
```

Notice that `use_same_org_as_target` is now set to false:

```yaml title="prd_config.yaml"
source_api_base: 'https://api.elis.rossum.ai/v1'
target_api_base: 'https://api.elis.rossum.ai/v1'
use_same_org_as_target: false
```

To push local changes to the sandbox, run `push` command:

```bash
prd push
```

And to release changes to production, run `release` command:

```bash
# Preview the changes first:
prd release --plan-only

# Run the actual release:
prd release
```

The local repository should always contain both `source` and `target` organization configurations. They can both be updated by calling `prd pull` command. You can also run `prd pull source` or `prd pull target` to update only one.

  </TabItem>
</Tabs>

## Three environments for sandbox, UAT, and production

Finally, the most complex pattern with three environments: sandbox, UAT, and production. In this scenario, several environments are chained one after the other.

![](./_img/three-environments-diagram.png)

<Tabs groupId="prd">
  <TabItem value="prd2" label="v2 (latest)" default>

<WIP />

  </TabItem>
  <TabItem value="prd" label="v1 (deprecated)">

For sandbox and UAT environment configurations, see the [Two environments example](#two-environments-for-sandbox-and-production).

To configure the remaining production environment, it is necessary create a new Git branch and maintain there the configuration (from UAT to Production). Alternatively, for better transparency, create a new folder to store this configuration. This is necessary because `prd` currently doesn't support multi-target configurations out of the box.

  </TabItem>
</Tabs>
---
title: 'Sandboxes'
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import PaidFeature from '../\_paid_feature.md';
import WIP from '../\_wip.md';

<PaidFeature />

Rossum Sandboxes allow for isolated development of the solution and easy deployments of the tested solution to production.

Using Sandboxes currently requires installation of an external tooling available at: https://github.com/rossumai/deployment-manager

## Installation

<Tabs groupId="prd">
  <TabItem value="prd2" label="v2 (latest)" default>

First, download the installation script for our Sandboxing tool `deployment-manager` from its Rossum GitHub repository:

```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/rossumai/deployment-manager/main/install.sh)"
```

The script will automatically run and install the Sandboxing tool in the `~/.local/bin` folder making it available globally under the command `prd2`.

To upgrade to the latest version, run:

```bash
prd2 update
```

You can find more information here: https://github.com/rossumai/deployment-manager

Alternatively, you can install the tool manually (advanced):

```bash
cd $(mktemp -d)
git clone git@github.com:rossumai/deployment-manager.git
cd deployment-manager

python3 -m venv .
source bin/activate
python3 -m pip install pipx

python3 -m pipx install . --force
```

:::warning[Using Microsoft Windows?]

Sandboxes are currently not supported on Windows. You can, however, use WSL to run the `prd2` command: https://learn.microsoft.com/en-us/windows/wsl/install

:::

  </TabItem>
  <TabItem value="prd" label="v1 (deprecated)">

First, download the installation script for our Sandboxing tool `deployment-manager` from its Rossum GitHub repository:

```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/rossumai/deployment-manager/main/install.sh)"
```

The script will automatically run and install the Sandboxing tool in the `~/.local/bin` folder making it available globally under the command `prd`.

To upgrade to the latest version, run:

```bash
prd update
```

You can find more information here: https://github.com/rossumai/deployment-manager

:::warning[Using Microsoft Windows?]

Sandboxes are currently not supported on Windows. You can, however, use WSL to run the `prd` command: https://learn.microsoft.com/en-us/windows/wsl/install

:::

  </TabItem>
</Tabs>

## Available CLI commands

<Tabs groupId="prd">
  <TabItem value="prd2" label="v2 (latest)" default>

Complete list of commands and their parameters can be found when running `prd2 --help`:

```text
Usage: prd2 [OPTIONS] COMMAND [ARGS]...

Options:
  --version  Show the version and exit.
  --help     Show this message and exit.

Commands:
  deploy           Group of commands related to deploying from source to...
  hook             Group of commands related to working with hooks
  init             Creates a new project directory with the specified...
  migrate-mapping  Updates the current mapping.yaml to conform with the...
  pull             Downloads all Rossum objects from the user's default...
  purge            Deletes all objects in Rossum based on IDs in the...
  push             Updates local files that were changed into Rossum.
  update           Updates the PRD command to the latest version.
```

For more information about each command, run `prd2 <command> --help` (example for pull command):

```text
Usage: prd2 pull [OPTIONS] [DESTINATIONS]...

  Downloads all Rossum objects from the user's default (first) organization.
  Creates a local organization directory structure with the configs of these
  objects. In case the directory already exists, it first deletes its contents
  and then downloads them anew.

Options:
  -c, --commit                    Commits the pulled changes automatically.
  -a, --all                       Downloads all remote files and overwrites
                                  the local ones in the selected destinations.
  -s, --skip-objects-without-subdir
                                  If there are objects whose subdir cannot be
                                  determined, user is not manually prompted -
                                  objects are not downloaded.
  -m, --message TEXT              Commit message for pulling.
  --help                          Show this message and exit.
```

  </TabItem>
  <TabItem value="prd" label="v1 (deprecated)">

`prd` is a CLI tool and offers the following main commands:

- `prd init`: Initialize a new project (creates mainly `credentials.json` and `prd_config.yaml` files). When called with a project name, it also initialized an empty Git repository.
- `prd pull`: Pulls all objects from both `source` and `target` organizations (as per your configuration). It is possible to explicitly specify `source`/`target` to pull only that one environment, for example: `prd pull source`
- `prd push`: Pushes the latest changes to the `source` organization. This is effectively a counterpart of the `pull` command.
- `prd release`: Pushes the latest changes to the `target` organization. Visit [Deployment patterns](./deployment-patterns.md#two-environments-for-sandbox-and-production) to learn more about this use-case.
- `prd purge`: Removes all objects in the target organization.
- `prd purge unused_schemas`: Removes old unused schemas.

Complete list of commands and their parameters can be found when running `prd --help`.

  </TabItem>
</Tabs>

## Available configuration options

<Tabs groupId="prd">
  <TabItem value="prd2" label="v2 (latest)" default>

The only necessary configuration is in the `example-org/credentials.yaml` and in the `prd_config.yaml` files right after running `prd2 init`. The `init` command will guide you through the process so you don't have to worry about the configuration later (in fact, it is not advised to update this config manually; instead re-run the `init` command).

Here is how the credentials file looks like:

```yaml title="example-org/credentials.yaml"
token: b1946ac92492d2347c6235b4d2611184
```

And here is how the `prd_config.yaml` file looks like (example):

```yaml title="prd_config.yaml"
directories:
  production-org:
    org_id: '123123'
    api_base: https://api.elis.rossum.ai/v1
    subdirectories:
      default:
        regex: ''
```

  </TabItem>
  <TabItem value="prd" label="v1 (deprecated)">

The only necessary configuration is in the `credentials.json` and in the `prd_config.yaml` files right after running `prd init`.

First, we will setup credentials in the `credentials.json` file, where we can work with username and password combination ([Example 1](#example-1)), or alternatively we can use `token` ([Example 2](#example-2)).

#### Example 1

Authentication using username and password.

```json title="credentials.json"
{
  // Source organization (typically the only one needed).
  "source": {
    // Username under which `prd` will be calling the API.
    "username": "...",

    // Password for the user under which `prd` will be calling the API.
    "password": "..."
  },

  // Target organization in case it is necessary to release into a different organization
  // from source. The configuration is identical with `source` parameter.
  "target": {
    "username": "...",

    "password": "..."
  }
}
```

#### Example 2

Alternatively, you can use API token instead of username and password (if you have it):

```json title="credentials.json"
{
  "source": {
    // highlight-start
    "token": "..." // use API token instead of username/password
    // highlight-end
  },
  "target": {
    "token": "..."
  }
}
```

Finally, to set up the organization's URL, we need to edit the `prd_config.yaml` file, where we specify the source API URL. If the target API URL is different from the source, we can also specify `target_api_base`.

If the URLs are identical, you can add the `use_same_org_as_target` parameter with the value `true`. In this case, you can remove `target_api_base`.

```yaml title="prd_config.yaml"
# You can specify source and target API URL:
source_api_base: 'https://api.elis.rossum.ai/v1'
target_api_base: 'https://api.elis.rossum.ai/v1'

# Or add this to your YAML file in case the source and target are identical (and omit the target_api_base):
use_same_org_as_target: true
```

  </TabItem>
</Tabs>

## Configuring `mapping.yaml` file (DEPRECATED)

:::danger[DEPRECATED (for v1 only)]

Note, this section describes how to configure the `mapping.yaml` file which is only used in the deprecated v1 version of `prd`. The latest version of `prd2` does not use this file.

Learn how to upgrade to the latest version here: [Migration guide from v1 to v2](./migrate-v1-to-v2.md).

:::

Note that this file is automatically generated by the `prd pull` command. It is typically not necessary to configure it manually unless some more advanced use-case is needed.

```yaml
# The main key describing `source` organization details:
ORGANIZATION:
  # ID of the `source` organization:
  id: 123456
  # Name of the `source` organization:
  name: MyOrganization (Sandbox)
  targets:
    # ID of the `target` organization (if any):
    - target_id: 654321
  WORKSPACES:
    # IDs of the `source` workspaces:
    - id: 123456
      # Name of the `source` workspace:
      name: 'My Test Workspace'
      targets:
        # ID(s) of the `target` workspace(s) (there can be none or more than one):
        - target_id: null
      QUEUES:
        # IDs of the `source` queues belonging to the `source` workspace above:
        - id: 123456
          # Name of the `source` queue:
          name: Invoices
          targets:
            # ID(s) of the `target` queue(s) (there can be none or more than one):
            - target_id: null
              # (optional) You could override attribute on the target using `attribute_override`
              ## - for example you can have `name:` on `target` different from the `source`
              attribute_override:
                name: Invoices (PROD)
          INBOX:
            # IDs of the `source` inbox belonging to the `source` queue above:
            id: 123456
            # Name of the `source` inbox:
            name: Invoices
            targets:
              # ID(s) of the `target` inbox(s) (there can be none or more than one):
              - target_id: null
  HOOKS:
    # IDs of the `source` hooks:
    - id: 123456
      # Name of the `source` hook:
      name: Supplier Data Matching
      targets:
        # ID(s) of the `target` hook(s) (there can be none or more than one):
        - target_id: null
  SCHEMAS:
    # IDs of the `source` schemas:
    - id: 123456
      # Name of the `source` schema:
      name: Tax invoices (US) schema
      targets:
        # ID(s) of the `target` schema(s) (there can be none or more than one):
        - target_id: null
```
---
sidebar_position: 2
sidebar_label: 'Migrate from v1 to v2'
title: 'Sandboxes: Migration guide from v1 to v2'
---

# Migration guide from v1 to v2

The second version of our Sandboxing tool `prd` is **not backward compatible** with the first version. We recommend downloading the whole project again via the new version and going from there. You can keep using the old version (v1) command under `prd` and new version (v2) command under `prd2`.

To get the latest `prd` version, run the following command:

```bash
prd update
```

This will make the `prd2` command available, so you can initialize and download the project from scratch. First, you need to initialize a new project (in the relevant folder):

```bash
prd2 init
```

This command will prompt you for the following information (your options and preferences might slightly differ):

```bash
? ORG-LEVEL directory name: sandbox-org
? ORG ID: 255255
? Base API URL: (e.g., https://my-org.rossum.app/api/v1) https://example.rossum.app/api/v1
? API token: b1946ac92492d2347c6235b4d2611184
? SUBDIR name: default
? subdir regex (OPTIONAL):
? Would you like to specify another **SUBDIR** inside sandbox-org? No
? Would you like to specify another **ORG-LEVEL** directory? No
╭──────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Initialized a new PRD directory in "."                                                           │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
```

In case you made a mistake, or you want to add new folders, run the `init` command again. Later, you can pull your project configuration from the Rossum app:

```bash
prd2 pull sandbox-org
```

Learn more on how to work with this new version [here](./index.md)

Once everything is migrated and working, you can delete the original folder structure (created via `prd`) and use the new one instead (created via `prd2`).
---
title: 'Workday: Import configuration'
sidebar_label: 'Import configuration'
sidebar_position: 1
---

# Import configuration

All the necessary import configurations can be part of one large config:

```json
{
  "configurations": [
    {
      "ds_collection_name": "workday_suppliers"
      // …
    },
    {
      "ds_collection_name": "workday_purchase_orders"
      // …
    },
    {
      "ds_collection_name": "workday_tax_applicability"
      // …
    }
    // …
  ]
}
```

For better readability, the following section contains individual configurations (alphabetically sorted).

## Cost Centers

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {},
        "service": "Financial_Management",
        "operation": "Get_Cost_Centers",
        "replication": {
          "id_key_name": "Organization_Data.ID",
          "differential_replication": true
        }
      },
      "response": {
        "extraction_jmespath": "Cost_Center[].Cost_Center_Data"
      },
      "ds_collection_name": "workday_cost_center"
    }
  ]
}
```

## Entities

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {},
        "service": "Financial_Management",
        "operation": "Get_Company_Organizations",
        "replication": {
          "id_key_name": "Organization_Data.ID",
          "differential_replication": true
        }
      },
      "response": {
        "extraction_jmespath": "Company_Organization[].Company_Organization_Data[]"
      },
      "ds_collection_name": "workday_entity"
    }
  ]
}
```

## Invoice Types

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {},
        "service": "Resource_Management",
        "operation": "Get_Invoice_Types"
      },
      "response": {
        "extraction_jmespath": "Invoice_Type[].Invoice_Type_Data[]"
      },
      "ds_collection_name": "workday_invoice_type"
    }
  ]
}
```

## Projects

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {},
        "service": "Resource_Management",
        "operation": "Get_Projects",
        "replication": {
          "id_key_name": "Workday_Project_ID",
          "differential_replication": true
        }
      },
      "response": {
        "extraction_jmespath": "Project[].Project_Data "
      },
      "ds_collection_name": "workday_project"
    }
  ]
}
```

## Purchase Items

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {
          "Request_Criteria": {
            "Item_Updated_To": "{current_datetime}",
            "Item_Updated_From": "{last_successful_import}"
          }
        },
        "service": "Resource_Management",
        "operation": "Get_Purchase_Items",
        "replication": {
          "id_key_name": "Purchase_Item_ID",
          "differential_replication": true
        }
      },
      "response": {
        "extraction_jmespath": "Purchase_Item[].Purchase_Item_Data"
      },
      "ds_collection_name": "workday_purchase_item"
    }
  ]
}
```

## Purchase Orders

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {
          "Response_Group": {
            "Include_Attachment_Data": false
          },
          "Request_Criteria": {
            "Updated_To_Date": "{current_datetime}",
            "Updated_From_Date": "{last_successful_import}"
          }
        },
        "service": "Resource_Management",
        "operation": "Get_Purchase_Orders",
        "replication": {
          "id_key_name": "Document_Number",
          "differential_replication": true
        }
      },
      "response": {
        "extraction_jmespath": "Purchase_Order[].Purchase_Order_Data[]"
      },
      "ds_collection_name": "workday_purchase_order"
    }
  ]
}
```

## Suppliers

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {
          "Response_Group": {
            "Include_Attachment_Data": false
          },
          "Request_Criteria": {
            "Updated_To_Date": "{current_datetime}",
            "Updated_From_Date": "{last_successful_import}"
          }
        },
        "service": "Resource_Management",
        "operation": "Get_Suppliers",
        "replication": {
          "id_key_name": "Supplier_ID",
          "differential_replication": true
        }
      },
      "response": {
        "extraction_jmespath": "Supplier[].Supplier_Data"
      },
      "ds_collection_name": "workday_suppliers"
    }
  ]
}
```

## Tax Applicability

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.2"
      },
      "request": {
        "payload": {},
        "service": "Financial_Management",
        "operation": "Get_Tax_Applicabilities",
        "replication": {
          "id_key_name": "Tax_Applicability_ID",
          "differential_replication": true
        }
      },
      "response": {
        "extraction_jmespath": "Tax_Applicability[].Tax_Applicability_Data"
      },
      "ds_collection_name": "workday_tax_applicability"
    }
  ]
}
```
---
title: 'Workday: Export configuration'
sidebar_label: 'Export configuration'
sidebar_position: 2
---

# Export configuration

## PO-backed invoice

```json
{
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.1"
      },
      "request": {
        "mapping": {
          "Add_Only": true,
          "Supplier_Invoice_Data": {
            "Memo": "@{memo}",
            "Submit": true,
            "Invoice_Date": "@{date_issue}",
            "Company_Reference": {
              "ID": [
                {
                  "type": "Organization_Reference_ID",
                  "_value_1": "@{entity_wd}"
                }
              ]
            },
            "Currency_Reference": {
              "ID": [
                {
                  "type": "Currency_ID",
                  "_value_1": "@{currency}"
                }
              ]
            },
            "External_PO_Number": "@{order_id}",
            "Supplier_Reference": {
              "ID": [
                {
                  "type": "Supplier_ID",
                  "_value_1": "@{supplier_wd}"
                }
              ]
            },
            "Control_Amount_Total": "@{amount_total}",
            "Suppliers_Invoice_Number": "@{document_id}",
            "Invoice_Line_Replacement_Data": {
              "$FOR_EACH_SCHEMA_ID$": {
                "mapping": {
                  "Quantity": "@{item_quantity}",
                  "Unit_Cost": "@{item_amount_base}",
                  "Line_Order": "10000001",
                  "Extended_Amount": "@{item_total_base}",
                  "Item_Description": "@{item_description}",
                  "Purchase_Order_Line_Reference": {
                    "ID": [
                      {
                        "type": "Line_Number",
                        "_value_1": "@{item_order_line_nr_wd}",
                        "parent_id": "@{item_document_number_po_wd}",
                        "parent_type": "Document_Number"
                      }
                    ]
                  }
                },
                "schema_id": "line_item"
              }
            },
            "Supplier_Connection_Reference": {
              "$IF_SCHEMA_ID$": {
                "mapping": {
                  "ID": [
                    {
                      "type": "Supplier_Connection_ID",
                      "_value_1": "@{account_num}"
                    }
                  ]
                },
                "schema_id": "account_num",
                "fallback_mapping": {}
              }
            },
            "Statutory_Invoice_Type_Reference": {
              "ID": [
                {
                  "type": "Invoice_Type_ID",
                  "_value_1": "@{invoice_type_wd}"
                }
              ]
            }
          }
        },
        "service": "Resource_Management",
        "operation": "Submit_Supplier_Invoice"
      }
    }
  ]
}
```

## Non-PO-backed invoices

```json
{
  "debug": true,
  "configurations": [
    {
      "wsdl": {
        "domain": "wd3-impl-services1.workday.com",
        "tenant": "…",
        "api_version": "v39.1"
      },
      "request": {
        "mapping": {
          "Add_Only": true,
          "Supplier_Invoice_Data": {
            "Memo": "@{memo}",
            "Submit": true,
            "Tax_Amount": {
              "$IF_SCHEMA_ID$": {
                "mapping": "@{amount_total_tax}",
                "schema_id": "amount_total_tax"
              }
            },
            "Invoice_Date": "@{date_issue}",
            "Company_Reference": {
              "ID": [
                {
                  "type": "Organization_Reference_ID",
                  "_value_1": "@{entity_wd}"
                }
              ]
            },
            "Currency_Reference": {
              "ID": [
                {
                  "type": "Currency_ID",
                  "_value_1": "@{currency}"
                }
              ]
            },
            "Supplier_Reference": {
              "ID": [
                {
                  "type": "Supplier_ID",
                  "_value_1": "@{supplier_wd}"
                }
              ]
            },
            "Control_Amount_Total": "@{amount_total}",
            "Suppliers_Invoice_Number": "@{document_id}",
            "Default_Tax_Option_Reference": {
              "$IF_SCHEMA_ID$": {
                "mapping": {
                  "ID": [
                    {
                      "type": "Tax_Option_ID",
                      "_value_1": "@{tax_option_id}"
                    }
                  ]
                },
                "schema_id": "tax_option_id",
                "fallback_mapping": {}
              }
            },
            "Invoice_Line_Replacement_Data": {
              "$FOR_EACH_SCHEMA_ID$": {
                "mapping": {
                  "Line_Order": "10000001",
                  "Extended_Amount": "@{item_total_base}",
                  "Item_Description": "@{item_description}",
                  "Worktags_Reference": [
                    {
                      "ID": [
                        {
                          "type": "Cost_Center_Reference_ID",
                          "_value_1": "@{item_cost_center_wd}"
                        }
                      ]
                    },
                    {
                      "$IF_SCHEMA_ID$": {
                        "mapping": {
                          "ID": [
                            {
                              "type": "Project_ID",
                              "_value_1": "@{item_project_wd}"
                            }
                          ]
                        },
                        "schema_id": "item_project_wd",
                        "fallback_mapping": {}
                      }
                    }
                  ],
                  "Purchase_Item_Reference": {
                    "ID": [
                      {
                        "type": "Purchase_Item_ID",
                        "_value_1": "@{item_purchase_item_wd}"
                      }
                    ]
                  },
                  "Tax_Applicability_Reference": {
                    "$IF_SCHEMA_ID$": {
                      "mapping": {
                        "ID": [
                          {
                            "type": "Tax_Applicability_ID",
                            "_value_1": "@{item_tax_applicability_id}"
                          }
                        ]
                      },
                      "schema_id": "item_tax_applicability_id",
                      "fallback_mapping": {}
                    }
                  }
                },
                "schema_id": "line_item"
              }
            },
            "Supplier_Connection_Reference": {
              "$IF_SCHEMA_ID$": {
                "mapping": {
                  "ID": [
                    {
                      "type": "Supplier_Connection_ID",
                      "_value_1": "@{account_num}"
                    }
                  ]
                },
                "schema_id": "account_num",
                "fallback_mapping": {}
              }
            },
            "Statutory_Invoice_Type_Reference": {
              "ID": [
                {
                  "type": "Invoice_Type_ID",
                  "_value_1": "@{invoice_type_wd}"
                }
              ]
            }
          }
        },
        "service": "Resource_Management",
        "operation": "Submit_Supplier_Invoice"
      }
    }
  ]
}
```
---
title: 'Workday'
sidebar_position: 1
---

import WebhookEndpoints from '../\_webhook_endpoints.md';
import WIP from '../\_wip.md';

## Installation

<WIP />

<WebhookEndpoints
  eu1="https://elis.rossum.ai/svc/workday/api/v1/import"
/>

## Basic usage

<WIP />

## Available configuration options

<WIP />
import Admonition from '@theme/Admonition';

<Admonition type="info" icon="🔒" title="Paid feature">
  This is a paid feature and requires involvement of Rossum Sales and/or Rossum Professional Services. Consider contacting the respective teams using the following form: https://rossum.ai/form/contact/
</Admonition>
---
sidebar_position: 1
sidebar_label: 'Automating emails'
title: 'Emails & email ingestion: Automating emails'
---

# Automating emails

## Automatic rejection based on condition

Rossum allows you to define a condition (_trigger_) and an _email template_ to be sent when the condition is true. In case the template is of type "rejection", this also sets the annotation's status to be `rejected`. This can be used to inform your vendors (or anyone else you specify in the email template) about a problematic document:

First, go to queue settings where you want to set up automatic rejections, specifically into the "Emails" section:

![alt text](_img/image.png)

Scroll down to "Document rejection":

![alt text](_img/image-1.png)

Create a new rejection template. If you would like to send automated emails to your vendors, you can use the variable `{{sender_email}}` (assuming the vendor sent the email into Rossum).

Toggle the automatic sending based on a trigger:

![alt text](_img/image-2.png)

And finally, select the condition you want from the list (for details):

![alt text](_img/image-3.png)

:::tip

For rejecting documents with missing fields, both the "required field missing" and "selected fields missing" triggers require a very high AI engine confidence (0.95) that the field is not on the document. However, you can also the following trigger that is checking for an empty value (by using a regex):

![alt text](_img/image-4.png)

:::
---
title: 'Emails & email ingestion'
sidebar_position: 1
---

## Introduction

_Emails_ in Rossum represent a single email sent to or from the platform ([docs](https://elis.rossum.ai/api/docs/#email)). They can be a part of an _email thread_ ([docs](https://elis.rossum.ai/api/docs/#email-thread)) which is a grouping of related emails similar to what you are used to in email clients (e.g., Gmail). These email objects also have a link to related annotations and documents = documents that were sent as email attachments and processed into annotations in Rossum.

:::warning[Work in progress]

TBD: queue email settings breakdown, email templates, configuring notifications (queue + extension)

:::
---
sidebar_position: 2
sidebar_label: 'Email body converter'
title: 'Emails & email ingestion: Email body converter'
---

import WebhookEndpoints from '../\_webhook_endpoints.md';

# Email body converter

A simple extension that can convert the email HTML body into a PDF and upload it to a queue as a new document.

An additional feature is that it can also convert HTML attachments into PDFs.

The original API endpoint documentation can be found [here](https://elis.rossum.ai/svc/email-converter/api/redoc).

## Installation

Email body converter is a webhook maintained by Rossum. To use it, follow these steps:

1. Log in to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill in the following fields:
   1. Name: `Email body converter`
   1. Trigger events: `email.received`
   1. Extension type: `Webhook`
   1. URL (see below)
   1. In "Advanced settings" select **Token owner** (should have Admin access)
1. Click **Create the webhook**.
1. Fill in the `Configuration` field. See [Available configuration options](#available-configuration-options) below.

<WebhookEndpoints
  eu1="https://elis.rossum.ai/svc/email-converter/api/v1/convert"
  us="https://us.app.rossum.ai/svc/email-converter/api/v1/convert"
/>

## Available configuration options

```json
{
  // Each object in the `configurations` list represents a specific configuration (distinguished by
  // the queue IDs).
  "configurations": [
    {
      // Required! List of queue IDs this configuration applies to. A single configuration can be
      // used for multiple queues, specified in this list.
      "queue_ids": [172636],

      // Minimum number of characters in the email body to convert it to PDF. Default is 0.
      "minimal_email_character_count": 5,

      // Skip conversion if supported files are present (`true` to skip, `false` to convert the
      // email body to PDF). Supported files include email attachments supported by Rossum and any
      // additional files converted to PDF as part of the webhook call (e.g., HTML attachments
      // converted to PDF). Default is `false`.
      "skip_if_supported_files_present": false,

      // Optional. List of attachment types to convert to PDF. Supported values: "html", "txt"
      "convert_attachments": ["html", "txt"],

      // Optional. Specifies the style for TXT files, which are first converted to HTML and then
      // to PDF. This configuration is added as an HTML style tag to affect the appearance of
      // the TXT in the converted PDF.
      "txt_style": "@page { size: letter landscape; margin: 2cm; } pre { white-space: pre-wrap; }"
    }
  ]
}
```
---
title: 'Rossum Aurora'
sidebar_position: 1
sidebar_label: 'Rossum Aurora'
---

# Rossum Aurora

Aurora is an umbrella term for our state-of-the-art AI, launched in February 2024. It refers to our proprietary Transactional Large Language Model (T-LLM) that is trained on millions of transactional documents from a number of use cases and industries. The T-LLM is used as the basis for all customer/use case specific Aurora models.

T-LLM comes with a set of pre-trained fields. When you create a new queue in Rossum, you can leverage these fields for out of the box extraction. Most of the pre-trained fields are related to the Accounts Payable and Receivable use case. Aurora models don’t require regular retraining. They learn continuously from each document that is processed (user-confirmed) in the queue.

## What makes Rossum Aurora unique?

### The largest dataset on the market

Our T-LLM is trained on 100’s of millions of transactional documents with rich annotations.

### Accuracy and confidence scores

Each field is marked with a confidence score that allows users to navigate quickly to fields requiring review. Automation of a document can be controlled by setting field-level thresholds.

![aurora-accuracy](./img/aurora-accuracy.png)

### Speed of learning

10x fewer training examples needed to reach the desired accuracy.

![aurora-accuracy-2](./img/aurora-accuracy2.png)

### No hallucinations

Generative models carry some notable risks, such as hallucinations, prompt injection, or data leakage.

To address these concerns, we employ a discriminative decoder that operates within the confines of the input document. This decoder ensures that the generated answers stay within the boundaries of the existing information and provides confidence levels crucial for trustworthy automation.

### Specific for transactional documents

Transactional documents such as invoices or orders have a rich visual structure where information is not in long sentences but scattered all over the page.

Conventional LLMs have been trained on continuous texts or optimized to take pictures and scenes as visual input. Instead, our T-LLMs have been optimized for semi-structured documents and are focused on both text and layout information.

### Global Language Support

276 languages fully supported, with instant learning across different languages.
30 languages are supported with handwriting.

## How is Rossum Aurora trained?

The T-LLM foundation model and the Aurora Engine are shared across all of the customers. Those are trained periodically by Rossum. The data used for the retraining is restricted to documents Rossum received a permission to use.

The Custom AI layer is trained continuously. Every `confirmed` or `exported` document contributes to the knowledge of the model. By default, the knowledge is restricted to a queue. This means that to maintain and benefit from past knowledge, the documents have to stay in the queue. Deletion and purge of `confirmed`/`exported` documents from the queue removes the knowledge from the AI database.

### Document Splitting and Classification

The AI that allows Rossum to split and classify the documents is trained as a separate model. It uses the whole text of a document to predict the type of a document and the beginning and end of documents in a single bundle.

## Best Practices

### How to separate documents into queues?

The AI is linked to the schema of fields you would like to extract. That means that all the documents in one queue have to comply with the same schema.
From this it can be inferred that each queue has to be limited to the same document type (e.g. AWB, Commercial Invoice, etc). Moreover, if the fields you want to extract from the same document type differ per region, use case or customer, it is advisable not to mix those in one queue.

:::info

If one use case requires line items and another does not, you would put documents for each use case into separate queues. If you mix them together and on some documents annotate line items and on others ignore them, the AI knowledge will be confused and AI predictions will perform poorly.

:::

Rossum now supports languages in many different scripts and theoretically should work across languages even if mixed together in one queue. Our experience is that separating queues per language script produces superior results. Note that there is no need to separate languages that use the same script (e.g. Latin).

#### Schema best practices

Schema is a list of fields that you will use to process the documents. There are 4 different kinds of fields:

- `Captured` - the values for these fields are predicted by Aurora AI. The AI learns from the values that you capture in these fields. To ensure the best performance, please follow the best practices for annotations (see next section).
- `Data` - the values for these fields are populated from the Master Data Hub, the upload window, or other extensions.
- `Manual` - these fields are there for users to manually type in values and they are not linked to any data source.
- `Formula` - these fields allow you to create formulas to set field values, whether it’s for normalizing data, performing calculations, or doing text operations.

When setting up the schema in your Rossum Aurora queue, please make sure to:

- Always set the correct value source (`captured`, `data`, `manual`, or `formula`)
- Don't change the schema ID after the team started processing the documents (annotations are connected to the IDs, and if the IDs are changed, the predictions will stop appearing until the AI learns which values you want to capture in the specific field)
- Set up fields for the individual datapoints to be captured from your documents
- If you want to utilize the pre-trained fields, do not change any of the IDs. You can change the label. Do not use the pre-trained field IDs for other fields.

#### AI training best practices

To provide AI with the best possible knowledge, please refer to our [AI training best practices annotation guide](./index.md)
---
title: 'AI training best practices'
sidebar_position: 1
---

import QuizComponent from '@site/src/components/QuizComponent';

Rossum's AI-powered document processing doesn't need complex templates to be built for each vendor layout to be able to predict where the values should be.

What the AI needs is precise and consistent data which we can get from User feedback - annotations.

Annotations refer to all the captured data from a document. You can recognize them by the blue bounding boxes (b-boxes) that appear on the document after it is processed in Rossum.

Please review these resources before starting:

- [Annotations Guide](https://rossum.ai/help/article/annotations-guide-and-rules-to-follow/)
- [Interactive Bounding Boxes](https://rossum.ai/help/article/interactive-bounding-boxes-in-rossum/)

:::info

When you first start using our product, it may not be clear what triggers AI learning. You upload your documents and apply corrections, but when will you start seeing improvements? It's simple. Every document must be `confirmed` or `exported`, depending on your settings. The entity confirming the document must be a real person, not an automated process or external script. Depending on your AI engine, you will be able to see changes in newly uploaded documents either immediately or after an agreed-upon time.

:::

## 🚀 Three key concepts to maintain good AI performance {#three-key-concepts-to-maintain-good-ai-performance}

### Precision

1. Check that [bounding boxes](https://rossum.ai/help/article/interactive-bounding-boxes-in-rossum/) are correctly applied to the value.
   - No overlapping with another bounding box.
   - The value is captured fully.

### Accuracy

1. If value is placed in the wrong field, correct it.
2. If there are typos or other issues, try adjusting the bounding box to get the correct reading.

### Consistency

1. If you have multiple values in a document, it is recommended to always select them from the same position. This is especially important when working with similar or identical layouts. Sometimes, it may be difficult to remember how you annotated a similar document. In such cases, you may want to consult previously confirmed documents or use the following rules to help maintain consistency.
   - Prefer values that appear earlier in the document.
   - If multiple values can fit, choose the one that is closer to the header part of the document.
   - If multiple values fit but are scattered across multiple pages, choose the one that is closest to the first page (or on the first page if possible).
   - If the value usually accompanies other fields' values, choose the location that is close to these other fields' values.
     In case multiple values are on the same level (on the right and left of the bottom of the document) just decide to go always closer to the right side of the document.

2. Capture all and only the values in the documents.
   - If there is data in the document that has a corresponding field in the schema for extraction, capture it in each document where it is present, even if you may not need it to be extracted for a particular vendor or in a particular case.
   - Amounts should also always be captured, even if the value on the document is "0".
   - Conversely, if a value is not present on the invoice, please do not enter it manually.

For in detail explanation please reach out to [Annotations Guide](https://rossum.ai/help/article/annotations-guide-and-rules-to-follow/).

## 🛟 Common issues {#common-issues}

1. The AI has predicted the correct value, but the reading of the text is incorrect
   - Re-adjust the Bounding Box so that the OCR is applied again
   - If, after a couple of attempts the value is not corrected, change the value manually

2. The AI has predicted the correct value, but only partially or included extra text
   - Correct the position of the Bounding Box so that it goes around the correct data
   - The learning is then stored & will be applied to later annotations

3. The date format is read incorrectly
   - The date format is pre-defined by the schema
   - The interpretation of ambiguous dates relies the document region that is set for the queue
   - Re-adjust the bounding box or ask your Admin to adjust the field to the correct date format, if the formatting is consistently not correct

## 🤔 Considerations {#considerations}

1. Always try to annotate text for names. Avoid logos or visual representations of data.
2. Handwritten data is not currently supported, even though it may be partially recognized.
3. Watermark recognition is not currently supported. If your use case requires this functionality, please contact a Rossum representative for further assistance.

## 🏆 Priority of rules {#priority-of-rules}

It may happen that some of the recommendations contradict each other in particular cases.

- The principles of Precision and Accuracy should take priority.
- When applying the Consistency principle, be reasonable. For example, if there is a logo at the top of the first page and a written sender name in the footer of the document, choose the footer, even if the recommendation is to prefer the header.
- Another example is when two values always appear together in the footer, but one of them is also present in the header section (e.g., Sender Name and Sender Address). In this case, you can choose to annotate both together in the footer, but be consistent and do not occasionally switch to another location. In the end, consistency itself is more important

<QuizComponent
question="Can bounding boxes overlap?"
answers={[
{ text: 'Yes, they can' },
{ text: 'Yes, as long as the values are identical' },
{ text: 'No, they cannot', isCorrect: true }
]}>
Bounding boxes in Rossum.ai cannot overlap because each box is designed to capture a single, distinct data field from a document. This one-to-one relationship between a bounding box and a data field is fundamental to how the platform processes information accurately.

The primary reason for prohibiting overlapping boxes is to avoid **ambiguity**. Rossum's AI links the text contained within a bounding box to a specific field in the document's schema (e.g., "Invoice Number," "Total Amount," "Order Date").

If two boxes were to overlap, the AI wouldn't be able to determine which piece of data belongs to which field. For instance, if the boxes for "Invoice Number" and "Order Number" overlapped on the same string of text, the system would be confused about how to label that data correctly. This strict rule ensures that every captured value is unambiguously assigned to its intended field.
</QuizComponent>

## 🙋 Q&A {#qa}

### When should I use multiple queues for my documents?

Different queues should be used if there is a different set of fields to capture from the documents (e.g., if in one case you are capturing tables, and in another, you are not, the documents should be separated into different queues).

:::tip

✅ One queue - no need to separate: You have `Document X` with line items and `Document Y` without line items. You capture line items in `Document X` and skip them in `Document Y`. You can have one queue because you train the AI to capture line items where they are present and do not attempt to capture them where they do not exist. This way, you differentiate various layouts and achieve better training.

⛔ Two queues are required: You have `Document Z` and `Document W`. Both have line items. You capture line items in `Document Z`, and for some reason, you do not want to spend time correcting/extracting line items from `Document W`. Then, you can't have one unified queue for data capture. Load these documents in two different queues to maximize extraction performance.

⛔ Multiple queues are required when you have small overlaps in the extracted fields across different document types.

:::

Documents in unique scripts should be in separate queues (e.g., documents in Latin script should be in one queue, and documents in Cyrillic script in another).

Documents from different regions should also be sent to separate queues to ensure correct date and number parsing.

### How can I correct inaccurate annotations to improve AI predictions?

If you have manually processed a document and later discovered mistakes in the annotations, you can correct them to prevent the AI from repeating these errors.

#### Using Export for documents

If the document has already been exported, you will need to return it to the "Review" status. To change the document's status to "Review", simply select the document and move it to the same queue without re-extracting the data, this action will keep the document in the same queue and change the status. Then, correct the data points and export it again.

#### Using Confirmed status

If you are using the "Confirmed" status without exporting, you can open the document in this status and make the necessary edits. Once the changes are made, confirm it again.

:::info

Depending on your version of the AI engine, changes may affect predictions either immediately or after an agreed-upon period. For more information, please contact Rossum.

:::

:::danger

Always make sure you fully understand the consequences of re-confirming or re-exporting a document.

Ensure that exporting again or making changes to a confirmed document does not cause complications with integrations or existing business logic.
For example, if you have an integration with a downstream system triggered every time you export data, re-exporting may cause duplicates or errors in that system.

:::

### The field does not learn from annotations

1. Check your version of the AI engine. While one version learns almost instantly, another requires some agreed-upon time to pick up confirmed or exported documents. If you are unsure which AI engine you are using, you can review that on the page `Automation > Automation settings` and in the grid is a column with used technology. In case you are very unsure contact a Rossum representative.
2. Ensure that you are teaching the AI to extract data into a field that is supposed to capture data directly from the document, never use one that is calculated or matched from another data source.<br />
   A common example is fields used for matching data from a document with another data source. These fields use programmatic logic, not AI. If you want to modify the logic of this matching, contact your development team or consult a Rossum representative.
3. Ensure that the field has the correct "Value Source." For AI-driven extraction, the "Value Source" must be set to "Captured." You can find this setting in the "Queue Settings -> Fields" section.
   ![value-source](img/value-source.png)
   Any other type of "Value Source" will prevent AI learning.
4. For "Captured" fields, ensure that a representative set of annotated documents has been confirmed or exported.

If none of the above steps help, contact a Rossum representative.

### The field does not capture more than one line of data

If you are using a custom field to capture multiple lines of data, please consult a Rossum representative. By default, the AI learns from a single line, except for out-of-the-box address fields.

### Magic Grid disappeared for some annotations

Please check if the new option for annotating tables, [Aurora for Complex Tables](https://rossum.ai/help/article/aurora-for-complex-tables/), is available. If not, contact your Rossum representative for further assistance.

The reason the Grid may not be fully or partially displayed is due to a shift in technology towards more advanced AI recognition that no longer requires the grid. For example, if some data points in your table are nested under others, the system may be unable to create a reliable grid, as multiple fields may appear in a single column. In such cases, the grid will not be shown.

Aurora for Complex Tables avoids these issues since it does not use a grid. However, the annotation process is different, so be sure to familiarize yourself with the best practices.

### Some pre-trained fields strip characters during data extraction

Certain pre-trained fields are modified when extracted. You can learn more about pre-trained fields in our [documentation](https://elis.rossum.ai/api/docs/#identifiers).

Below is a list of fields that undergo modification (please refer to the documentation for the most up-to-date information):

| Attr. rir_field_names | Field label          | Modification                                 |
| --------------------- | -------------------- | -------------------------------------------- |
| `account_num`         | Bank Account         | Whitespaces are stripped                     |
| `var_sym`             | Variable symbol      | Possible non-numeric characters are stripped |
| `customer_id`         | Customer Number      | Whitespaces are stripped                     |
| `document_id`         | Document Identifier  | Whitespaces are stripped                     |
| `order_id`            | Order Number         | Whitespaces are stripped                     |
| `recipient_dic`       | Recipient Tax Number | Whitespaces are stripped                     |
| `recipient_ic`        | Recipient Company ID | Possible non-numeric characters are stripped |
| `sender_dic`          | Supplier Tax Number  | Whitespaces are stripped                     |
| `sender_ic`           | Supplier Company ID  | Possible non-numeric characters are stripped |

If you need to prevent these modifications, please contact a Rossum representative.

### Dates are mixing formats

In some cases, despite the date format you have chosen in the schema for your date field, you may notice extraction errors. Specifically, days, months, and years may be confused.
This occurs because the formatting you define per field in the schema represents the desired output format, not how the system actually interprets dates.

In cases where the date is not **ambiguous**, the system does attempt to follow the format you specified. However, for more complex dates, it uses a different approach.
What are **ambiguous** dates?

- Dates with incomplete years (e.g., "24" could represent either a day or a year).
- Dates where both the day and month are 12 or less, such as `12/11/2024`. In this case, the system cannot determine whether "12" represents the day or the month.

We understand that your documents may not consistently follow one format. For example, one document might use the European format `DD/MM/YYYY`, while another uses the American format `MM/DD/YYYY`. For this reason, the system cannot rely solely on the specified format.
**To determine the correct date, the system evaluates all variations and chooses the date closest to the document’s arrival time.** This approach works reliably for recent documents, but it may produce errors for older documents or those containing older dates.

Let's check the example:

1. Today is December 23rd, 2024
2. Document has date _9/11/24_
3. Your schema assumes American format `MM/DD/YYYY`. Then it gives us the following variations: _9/11/2024_, _11/09/2024_, _9/24/2011_, _11/24/2009_.
4. The closest date to December 23rd, 2024 is _11/09/2024_. This date will be selected.
Secrets:

```json
{
  "type": "sftp",
  "password": "MySuperSecretPassword"
}
```
Secrets (without a passphrase):

```json
{
  "type": "sftp",
  "ssh_key": "-----BEGIN OPENSSH PRIVATE KEY-----\nabcd…wxyz\n-----END OPENSSH PRIVATE KEY-----"
}
```

Secrets (with passphrase):

```json
{
  "type": "sftp",
  "ssh_key_passphrase": "<passphrase for private key>",
  "ssh_key": "-----BEGIN OPENSSH PRIVATE KEY-----\nabcd…wxyz\n-----END OPENSSH PRIVATE KEY-----"
}
```

The easiest way to convert the SSH key to one-line format is to use the following command:

```bash
awk '{printf "%s\\n", $0}' id_rsa_demo.txt
```
---
title: 'SFTP and S3 imports/exports'
sidebar_position: 1
---

import WIP from '../\_wip.md';
import WebhookEndpoints from '../\_webhook_endpoints.md';
import RossumInternalOnly from '../\_rossum_internal_only.md';

## Installation

File storage (SFTP and S3) import/export extensions are available in the Rossum store. To install them:

1. Login to your Rossum account.
1. Navigate to **Extensions → Rossum Store**.
1. Search one of the following extensions and "Add" them:
   1. **Import Master Data From SFTP**
   1. **Import Master Data from S3**
   1. **Import Documents From SFTP**
   1. **Import Documents From S3**
   1. **Export To SFTP**
   1. **Export To S3**

Alternatively, it is also possible to install them manually (advanced):

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `SFTP/S3 import/export`
   1. Trigger events: `Scheduled` for import or `Export` for exports
   1. Extension type: `Webhook`
   1. URL (see import and export endpoints below)
   1. In "Advanced settings" select **Token owner** (should have Admin access)
1. Click **Create the webhook**.

### Dataset import endpoints

<WebhookEndpoints
  eu1="https://elis.task-manager.rossum-ext.app/api/v1/tasks/file-storage-dataset-import"
  eu2="https://shared-eu2.task-manager.rossum-ext.app/api/v1/tasks/file-storage-dataset-import"
  us="https://shared-us2.task-manager.rossum-ext.app/api/v1/tasks/file-storage-dataset-import"
  jp="https://shared-jp.task-manager.rossum-ext.app/api/v1/tasks/file-storage-dataset-import"
/>

<RossumInternalOnly url="https://rossumai.atlassian.net/l/cp/PV1jzmqK" />

### Document import endpoints

<WebhookEndpoints
  eu1="https://elis.task-manager.rossum-ext.app/api/v1/tasks/file-storage-document-import"
  eu2="https://shared-eu2.task-manager.rossum-ext.app/api/v1/tasks/file-storage-document-import"
  us="https://shared-us2.task-manager.rossum-ext.app/api/v1/tasks/file-storage-document-import"
  jp="https://shared-jp.task-manager.rossum-ext.app/api/v1/tasks/file-storage-document-import"
/>

<RossumInternalOnly url="https://rossumai.atlassian.net/l/cp/PV1jzmqK" />

### Export endpoints

<WebhookEndpoints
  eu1="https://elis.rossum.ai/svc/file-storage-export/api/v1/export"
  eu2="https://shared-eu2.rossum.app/svc/file-storage-export/api/v1/export"
  us="https://shared-us2.rossum.app/svc/file-storage-export/api/v1/export"
  jp="https://shared-jp.rossum.app/svc/file-storage-export/api/v1/export"
/>

<RossumInternalOnly url="https://rossumai.atlassian.net/l/cp/S1coKmuC" />

## Available configuration options

Common import/export configuration:

```json
{
  "credentials": {
    // Credentials section depends on whether you are using S3 or SFTP (see below)
    // …
  },
  "import_rules": [
    {
      "dataset_name": "PURCHASE_ORDERS_v1",
      "import_methods": {
        "replace_method": {
          "path": "/datasets",
          "dynamic": false,
          "file_format": "xlsx",
          "file_match_regex": "PURCHASE_ORDERS\\.xlsx"
        }
      },
      "result_actions": {
        "failure": [
          {
            "path": "/datasets/failed_imports",
            "type": "move"
          }
        ],
        "success": [
          {
            "path": "/datasets/archive",
            "type": "move"
          }
        ]
      }
    }
  ]
}
```

Credentials for **SFTP-related** import/export are specified in the following format:

```json
{
  "credentials": {
    "host": "sftp.example.com",
    "port": "22",
    "type": "sftp",
    "username": "example",
    // (Optional) Version of your SFTP server. Needed only exceptionally when automatic detection of
    // the version fails.
    // TODO: only for "sftp" type, see: https://elis.task-manager.rossum-ext.app/api/docs#tag/Tasks/operation/file_storage_document_import_api_v1_tasks_file_storage_document_import_post
    "sftp_version": 5
  }
  // …
}
```

Credentials for **S3-related** import/export:

```json
{
  "credentials": {
    "host": "sftp.example.com",
    "port": "22",
    "type": "s3",
    "username": "example"
  }
  // …
}
```

## Available configuration options

Available configuration options are described in the API documentation:

- Import: [Scheduled Imports - File storage](https://elis.rossum.ai/svc/scheduled-imports/api/docs#tag/File-Storage/operation/import_dataset_from_file_storage_api_file_storage_v1_dataset_import_post)
- Export: [File Storage Export](https://elis.rossum.ai/svc/file-storage-export/api/docs)

TODO ^^

## Logging and observability

### Extensions Logs

- URL `https://[org].rossum.app/settings/extensions/logs`
- The import job is not triggered directly, but using scheduler. Thus successfull record (type `INFO`) in the Extensions Logs does not necessary means the downstream import job was sucessfull, but it is a good start for observation

### Master Data Hub

- URL: `https://[org].rossum.app/svc/master-data-hub/web/management`
- Directly in the MDH, there is a status screen "Upload Status", regardless of the origin of "upload".
- There is also note with the more detailed info in case of some error.

![Upload Status](./img/upload-status.png)
---
sidebar_position: 1
title: 'SFTP and S3 imports/exports: Configuration examples'
sidebar_label: 'Configuration examples'
---

import ConfigurationSshKey from './\_configuration_ssh_key.md';
import ConfigurationUsernamePassword from './\_configuration_username_password.md';

# Configuration examples

Here you can find examples of the most common real-world use cases for exports and imports to/from SFTP and S3 extension.

## Import documents from SFTP

Configuration:

```json
{
  "credentials": {
    "host": "sftp.example.com",
    "port": "22",
    "type": "sftp",
    "username": "rossum-demo"
  },
  "import_rules": [
    {
      "path": "/documents",
      "queue_id": 123456, // change accordingly
      "result_actions": {
        "failure": [
          {
            "path": "/documents/failed_imports",
            "type": "move"
          }
        ],
        "success": [
          {
            "path": "/documents/archive",
            "type": "move"
          }
        ]
      },
      "file_match_regex": ".+"
    }
  ]
}
```

### Using SSH key

<ConfigurationSshKey />

### Using username and password

<ConfigurationUsernamePassword />

## Import master data from SFTP

Configuration:

```json
{
  "credentials": {
    "host": "sftp.example.com",
    "port": "22",
    "type": "sftp",
    "username": "rossum-demo"
  },
  "import_rules": [
    {
      "dataset_name": "PURCHASE_ORDERS_v1",
      "import_methods": {
        "replace_method": {
          "path": "/datasets",
          "dynamic": false,
          "file_format": "xlsx",
          "file_match_regex": "PURCHASE_ORDERS\\.xlsx"
        }
      },
      "result_actions": {
        "failure": [
          {
            "path": "/datasets/failed_imports",
            "type": "move"
          }
        ],
        "success": [
          {
            "path": "/datasets/archive",
            "type": "move"
          }
        ]
      }
    }
  ]
}
```

### Using SSH key

<ConfigurationSshKey />

### Using username and password

<ConfigurationUsernamePassword />

## Import master data from S3

There are few differences from the SFTP imports (see above), but it is very similar.

:::warning

Please note, that the attribute `path` **starts without slash** (as opposed to the SFTP configuration).

:::

```json
{
  "credentials": {
    "type": "s3",
    "bucket_name": "s3-master-data-bucket"
  },
  "import_rules": [
    {
      "dataset_name": "suppliers",
      "import_methods": {
        "replace_method": {
          "path": "datasets/inbox",
          "file_format": "csv",
          "file_match_regex": "suppliers\\.csv",
          "encoding": "windows-1250"
        }
      },
      "result_actions": {
        "failure": [
          {
            "path": "datasets/failed_imports",
            "type": "move"
          }
        ],
        "success": [
          {
            "path": "datasets/archive",
            "type": "move"
          }
        ]
      }
    }
  ]
}
```

### Using S3 Credentials

There is only one credential option for the `s3`. It consists of `Access Key ID` and `Secret Access Key`. For more information about these credentials and how to obtain it, see the official AWS documentation here: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html

```json
{
  "type": "__change_me__",
  "access_key_id": "__change_me__",
  "secret_access_key": "__change_me__"
}
```

## Export to SFTP

Configuration example:

```json
{
  "credentials": {
    "host": "sftp.example.com",
    "port": "22",
    "type": "sftp",
    "username": "rossum-demo"
  },
  "export_rules": [
    {
      "path_to_directory": "/export",
      "export_object_configurations": [
        {
          "type": "annotation_content",
          "format": "csv",
          "filename_template": "invoice_data-{annotation.id}.csv"
        },
        {
          "type": "document",
          "filename_template": "invoice_file-{annotation.id}" // No file extension for `document` export type!
        },
        {
          "type": "custom_format",
          "filename_template": "invoice_data-{annotation.id}.csv",
          "export_reference_key": "export_annotation_to_csv"
        }
      ]
    }
  ]
}
```

Notice the various `export_object_configurations` types, each having its own configuration. The supported types:

- `annotation_content`: exports data in a Rossum native format
- `document`: exports the original document
- `custom_format` exports custom data using the [custom format templating](../export-pipeline/custom-format-templating.md) extension

### Using SSH key

<ConfigurationSshKey />

### Using username and password

<ConfigurationUsernamePassword />

### Using dynamic SFTP folder

Configuration for a schema data point `sftp_folder` containing the appropriate address on the SFTP.

```json
{
  // …
  "export_rules": [
    {
      "path_to_directory": "/{sftp_folder}"
      // …
    }
  ]
}
```
---
title: 'Structured formats import: Sistema di Interscambio (SDI Italy)'
sidebar_label: 'Italy: Sistema di Interscambio'
sidebar_position: 2
---

# Sistema di Interscambio (SDI Italy)

https://www.fatturapa.gov.it/en/norme-e-regole/documentazione-fattura-elettronica/formato-fatturapa/

```json
{
  "fields": [
    {
      "schema_id": "recipient_name",
      "selectors": [
        "FatturaElettronicaHeader/CessionarioCommittente/DatiAnagrafici/Anagrafica/Denominazione"
      ]
    },
    {
      "schema_id": "recipient_vat_id",
      "selectors": [
        "FatturaElettronicaHeader/CessionarioCommittente/DatiAnagrafici/IdFiscaleIVA/IdCodice"
      ]
    },
    {
      "schema_id": "sender_name",
      "selectors": [
        "FatturaElettronicaHeader/CedentePrestatore/DatiAnagrafici/Anagrafica/Denominazione"
      ]
    },
    {
      "schema_id": "sender_vat_id",
      "selectors": [
        "FatturaElettronicaHeader/CedentePrestatore/DatiAnagrafici/IdFiscaleIVA/IdCodice"
      ]
    },
    {
      "schema_id": "iban",
      "selectors": [
        "FatturaElettronicaBody/DatiPagamento/DettaglioPagamento/IBAN"
      ]
    },
    {
      "schema_id": "document_type_code",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/TipoDocumento"
      ]
    },
    {
      "schema_id": "document_id",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/Numero"
      ]
    },
    {
      "schema_id": "order_id",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiOrdineAcquisto/IdDocumento"
      ]
    },
    {
      "schema_id": "date_issue",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/Data"
      ]
    },
    {
      "schema_id": "date_due",
      "selectors": [
        "FatturaElettronicaBody/DatiPagamento/DettaglioPagamento/DataScadenzaPagamento"
      ]
    },
    {
      "schema_id": "payment_terms_start_date",
      "selectors": [
        "//FatturaElettronicaBody/DatiPagamento/DettaglioPagamento/DataRiferimentoTerminiPagamento"
      ]
    },
    {
      "schema_id": "payment_terms_days",
      "selectors": [
        "//FatturaElettronicaBody/DatiPagamento/DettaglioPagamento/GiorniTerminiPagamento"
      ]
    },
    {
      "schema_id": "amount_total",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/ImportoTotaleDocumento"
      ]
    },
    {
      "schema_id": "currency_code",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/Divisa"
      ]
    },
    {
      "schema_id": "bollo_amount",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/DatiBollo/ImportoBollo"
      ]
    },
    {
      "schema_id": "tipo_ritenuta",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/DatiRitenuta[TipoRitenuta!='RT04']/TipoRitenuta"
      ]
    },
    {
      "schema_id": "ritenuta_amount",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/DatiRitenuta[TipoRitenuta!='RT04']/ImportoRitenuta"
      ]
    },
    {
      "schema_id": "ritenuta_rate",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/DatiRitenuta[TipoRitenuta!='RT04']/AliquotaRitenuta"
      ]
    },
    {
      "schema_id": "causale_pagameno",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/DatiRitenuta[TipoRitenuta!='RT04']/CausalePagamento"
      ]
    },
    {
      "schema_id": "enasarco_amount",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/DatiRitenuta[TipoRitenuta='RT04']/ImportoRitenuta"
      ]
    },
    {
      "fields": [
        {
          "schema_id": "item_description",
          "selectors": [
            "Descrizione"
          ]
        },
        {
          "schema_id": "item_quantity",
          "selectors": [
            "Quantita"
          ]
        },
        {
          "schema_id": "item_amount_base",
          "selectors": [
            "PrezzoUnitario"
          ]
        },
        {
          "schema_id": "item_total_base",
          "selectors": [
            "PrezzoTotale"
          ]
        },
        {
          "schema_id": "item_rate",
          "selectors": [
            "AliquotaIVA"
          ]
        },
        {
          "schema_id": "item_vat_code",
          "selectors": [
            "Natura"
          ]
        },
        {
          "schema_id": "item_code",
          "selectors": [
            "CodiceArticolo/CodiceValore"
          ]
        }
      ],
      "schema_id": "line_items",
      "selectors": [
        "FatturaElettronicaBody/DatiBeniServizi/DettaglioLinee"
      ]
    },
    {
      "fields": [
        {
          "schema_id": "item_tipo_cassa",
          "selectors": [
            "TipoCassa"
          ]
        },
        {
          "schema_id": "item_contributo_cassa",
          "selectors": [
            "ImportoContributoCassa"
          ]
        },
        {
          "schema_id": "item_aliquoata_iva_cassa",
          "selectors": [
            "AliquotaIVA"
          ]
        },
        {
          "schema_id": "item_cassa_alcassa",
          "selectors": [
            "AlCassa"
          ]
        },
        {
          "schema_id": "item_cassa_tax_code",
          "selectors": [
            "Natura"
          ]
        },
        {
          "schema_id": "item_discount_amount",
          "selectors": [
            "ScontoMaggiorazione[Tipo='SC']/Importo"
          ]
        },
        {
          "schema_id": "item_discount_rate",
          "selectors": [
            "ScontoMaggiorazione[Tipo='SC']/Percentuale"
          ]
        }
      ],
      "schema_id": "cassa_items",
      "selectors": [
        "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/DatiCassaPrevidenziale"
      ]
    },
    {
      "fields": [
        {
          "schema_id": "tax_detail_rate",
          "selectors": [
            "AliquotaIVA"
          ]
        },
        {
          "schema_id": "tax_detail_tax",
          "selectors": [
            "Imposta"
          ]
        },
        {
          "schema_id": "tax_detail_base",
          "selectors": [
            "ImponibileImporto"
          ]
        },
        {
          "schema_id": "tax_detail_code",
          "selectors": [
            "Natura"
          ]
        }
      ],
      "schema_id": "tax_details",
      "selectors": [
        "FatturaElettronicaBody/DatiBeniServizi/DatiRiepilogo"
      ]
    }
  ],
  "trigger_condition": {
    "selector": "FatturaElettronicaBody/DatiGenerali/DatiGeneraliDocumento/Numero",
    "file_type": "xml"
  }
}
```
---
title: 'Structured formats import'
sidebar_position: 1
---

import WebhookEndpoints from '../\_webhook_endpoints.md';
import WIP from '../\_wip.md';

Structured formats import allows for importing and processing of non-visual documents such as JSON or XML files. It not only correctly extracts the information from these files, but also renders a minimalistic PDF representation for easier manual reviews.

## Installation

:::warning

The support for ingesting XML or JSON files needs to be enabled by Rossum team.

:::

<details>
  <summary>Rossum team info</summary>

1. In Django admin under **Organization Group -> Features** check the `stored_only_mime_types` field and if present, remove the `application/xml` and `text/xml` values from the list. If the values are not there or the field does not exist, continue.

1. In Django admin under **Queue -> Queue Settings**, add the desired mime types to the `accepted_mime_types` list:

```json
{
  "accepted_mime_types": [
    "application/xml",
    "text/xml"
    // …
  ]
}
```

</details>

Structured formats import is a webhook maintained by Rossum. In order to use it, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `Structured formats import`
   1. Trigger events: `Upload - Created`
   1. Extension type: `Webhook`
   1. URL (see below)
1. Click **Create the webhook**.
1. Fill `Configuration` field (see [Configuration examples](./configuration-examples.md))
1. Assign an API token user

<WebhookEndpoints
  eu1="https://elis.task-manager.rossum-ext.app/api/v1/tasks/structured-formats-import"
  eu2="https://shared-eu2.task-manager.rossum-ext.app/api/v1/tasks/structured-formats-import"
  us="https://us.task-manager.rossum-ext.app/api/v1/tasks/structured-formats-import"
  jp="https://shared-jp.task-manager.rossum-ext.app/api/v1/tasks/structured-formats-import"
/>

## Basic usage

:::info Note
The extension supports multiple configurations. Even when using a single configuration, make sure it's defined in an array named `configurations`.
:::

```json
{
  // Various independent configurations that can be conditionally
  // triggered via `trigger_condition`:
  "configurations": [
    {
      "trigger_condition": {
        // supported values: "xml" and "json"
        "file_type": "xml"
      },

      // Fields to be extracted from the source file
      // and assigned to given datapoints:
      "fields": [
        {
          "schema_id": "recipient_name",

          // If many selectors are specified, they serve as a fallback list.
          // Selectors don't need to specify the root element (see sample XML below)
          "selectors": ["Header/Recipient/Name"]
        },
        ...
      ]
    }
  ]
}
```

Sample source file:

```xml
<Invoice>
  <Header>
    <Recipient>
      <Name>Hello world</Name>
    </Recipient>
  </Header>
</Invoice>
```

## Available configuration options

```json
{
  // Various independent configurations that can be conditionally triggered via `trigger_condition`:
  "configurations": [
    {
      "trigger_condition": {
        "file_type": "xml"
      },

      // Optional. Whether the original XML/JSON file should be split into smaller ones:
      "split_selectors": ["/RecordLabel/Productions/Production"],

      // Fields to be extracted from the source file and assigned to given datapoints:
      "fields": [
        {
          "schema_id": "document_id",

          // If many selectors are specified, they serve as a fallback list.
          "selectors": ["./Metadata/ID"]
        }
      ],

      // Optional specification of the original PDF file that should be extracted from the source
      // file (base64 encoded):
      "pdf_file": {
        "name_selectors": [
          "cac:AdditionalDocumentReference/cac:Attachment/cbc:EmbeddedDocumentBinaryObject/@filename"
        ],

        // Content should be base64 encoded:
        "content_selectors": [
          "cac:AdditionalDocumentReference/cac:Attachment/cbc:EmbeddedDocumentBinaryObject"
        ]
      }
    }
    // …
  ]
}
```
---
title: 'Structured formats import: Configuration examples'
sidebar_label: 'Configuration examples'
sidebar_position: 1
---

# Configuration examples

## Generic XML import

:::warning[todo]

_How to + examples_

:::

## Generic JSON import

:::warning[todo]

_How to + examples_

:::

## XML: Document splitting

XML documents can be split into multiple documents using the `split_selectors` configuration:

```json
{
  "configurations": [
    {
      "trigger_condition": {
        "file_type": "xml"
      },
      // highlight-start
      "split_selectors": ["/RecordLabel/Productions/Production"],
      // highlight-end
      "fields": [
        {
          "schema_id": "document_id",
          "selectors": ["./Metadata/ID"]
        }
        // …
      ]
    }
  ]
}
```

The field selectors are then relative to the newly split document.

## XML: PEPPOL BIS Billing 3.0

Basic configuration (works with the default Rossum.ai schema for invoices) and the following PEPPOL BIS Billing 3.0 example: https://github.com/OpenPEPPOL/peppol-bis-invoice-3/blob/0f63848fc46fe4ab87d1860a18bfe381c41e01ff/rules/examples/base-example.xml

```json
{
  "configurations": [
    {
      "trigger_condition": {
        "file_type": "xml"
      },
      "fields": [
        {
          "schema_id": "document_id",
          "selectors": ["cbc:ID"]
        },
        {
          "schema_id": "order_id",
          "selectors": ["cbc:BuyerReference"]
        },
        {
          "schema_id": "date_issue",
          "selectors": ["cbc:IssueDate"]
        },
        {
          "schema_id": "date_due",
          "selectors": ["cbc:DueDate"]
        },
        {
          "schema_id": "currency",
          "selectors": ["cbc:DocumentCurrencyCode"]
        },
        {
          "schema_id": "amount_due",
          "selectors": ["cac:LegalMonetaryTotal/cbc:TaxInclusiveAmount"]
        },
        {
          "schema_id": "amount_total_base",
          "selectors": ["cac:LegalMonetaryTotal/cbc:TaxExclusiveAmount"]
        },
        {
          "schema_id": "amount_total_tax",
          "selectors": ["cac:TaxTotal/cbc:TaxAmount"]
        },
        {
          "schema_id": "sender_name",
          "selectors": ["cac:AccountingSupplierParty/cac:Party/cac:PartyName/cbc:Name"]
        },
        {
          "schema_id": "sender_address",
          "selectors": ["cac:AccountingSupplierParty/cac:Party/cac:PostalAddress/cbc:StreetName"]
        },
        {
          "schema_id": "recipient_name",
          "selectors": ["cac:AccountingCustomerParty/cac:Party/cac:PartyName/cbc:Name"]
        },
        {
          "schema_id": "recipient_address",
          "selectors": ["cac:AccountingCustomerParty/cac:Party/cac:PostalAddress/cbc:StreetName"]
        },
        {
          "schema_id": "terms",
          "selectors": ["cac:PaymentTerms/cbc:Note"]
        },
        {
          "fields": [
            {
              "schema_id": "item_quantity",
              "selectors": ["cbc:InvoicedQuantity"]
            },
            {
              "schema_id": "item_code",
              "selectors": ["cac:Item/cac:StandardItemIdentification/cbc:ID"]
            },
            {
              "schema_id": "item_description",
              "selectors": ["cac:Item/cbc:Description", "cac:Item/cbc:Name"]
            },
            {
              "schema_id": "item_amount",
              "selectors": ["cac:Price/cbc:PriceAmount"]
            }
          ],
          "schema_id": "line_items",
          "selectors": ["cac:InvoiceLine"]
        }
      ]
    }
  ]
}
```

## XML with base64 encoded PDF

It is common that the actual PDF file is embedded in the XML document in base64 encoded format. It can be used instead of rendering the default minimal PDF representation:

```json
{
  "configurations": [
    {
      "fields": [
        // …
      ],
      "trigger_condition": {
        "file_type": "xml"
      },
      // highlight-start
      "pdf_file": {
        "name_selectors": [
          "cac:AdditionalDocumentReference/cac:Attachment/cbc:EmbeddedDocumentBinaryObject/@filename"
        ],
        "content_selectors": [
          "cac:AdditionalDocumentReference/cac:Attachment/cbc:EmbeddedDocumentBinaryObject"
        ]
      }
      // highlight-end
    }
  ]
}
```
---
title: 'Line items grouping'
sidebar_position: 1
---

import WebhookEndpoints from '../\_webhook_endpoints.md';
import WIP from '../\_wip.md';

**Line items grouping** extension allows for grouping line items based on given SQL criteria. This is handy when the downstream system has some restrictions such as only one unique line item per invoice.

## Installation

Line items grouping service is provided by Rossum.ai in the form of webhook. To start using the extension, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `Line items grouping`
   1. Trigger events: `Document content: Initialize, Started, Updated`
   1. Queues where the extension should be executed
   1. Extension type: `Webhook`
   1. URL (see [Available endpoints](#available-endpoints) below)
1. Click **Create the webhook**.
1. Fill `Configuration` field (visit [Configuration examples](./configuration-examples.md) page).

### Available endpoints

<WebhookEndpoints
  eu1="https://elis.line-items-grouping.rossum-ext.app/"
  eu2="https://shared-eu2.line-items-grouping.rossum-ext.app/"
  us="https://us.line-items-grouping.rossum-ext.app/"
  jp="https://shared-jp.line-items-grouping.rossum-ext.app/"
/>

## Basic usage

<WIP issue="https://github.com/rossumai/university/issues/380" />

## Available configuration options

<WIP issue="https://github.com/rossumai/university/issues/380" />
---
sidebar_position: 1
sidebar_label: 'Configuration examples'
title: 'Line items grouping: Configuration examples'
---

# Configuration examples

:::danger[Here be dragons]

Datapoint `line_item_grouped` **must exist** in the schema otherwise the extension will keep adding new line items and not removing the old ones! This datapoint name is currently hardcoded and cannot be changed.

The recommended schema datapoint:

```json
{
  "category": "multivalue",
  "id": "line_items_grouped",
  "label": "Line Items (grouped)",
  "children": {
    "category": "tuple",
    // highlight-start
    "id": "line_item_grouped",
    // highlight-end
    "label": "line_item_grouped",
    "children": [
      {
        "rir_field_names": [],
        "constraints": { "required": false },
        "default_value": null,
        "category": "datapoint",
        "id": "item_code_grouped",
        "label": "Code",
        "type": "string"
      },
      {
        "rir_field_names": [],
        "constraints": { "required": false },
        "default_value": null,
        "category": "datapoint",
        "id": "item_description_grouped",
        "label": "Description",
        "type": "string"
      }
      // Add more datapoints here as needed…
    ],
    "rir_field_names": []
  },
  "min_occurrences": null,
  "max_occurrences": null,
  "default_value": null,
  "rir_field_names": []
}
```

Additionally, the `line_items` table must exist in the schema as well. This is, however, the typical default.

:::

## Group line items by item code

The following SQL groups the line items by the value in `item_code` datapoint.

```sql
SELECT
    MAX(item_code) as item_code_grouped,
    MAX(item_description) as item_description_grouped
    COALESCE(SUM(NULLIF(item_quantity, '')), '') AS item_quantity_grouped
FROM
    inmemory_line_items
GROUP BY
    item_code
```

:::warning

When using `SUM` function, it is important to call it like this:

```sql
COALESCE(SUM(NULLIF(item_quantity, '')), '') AS item_quantity_grouped
```

Using simple `SUM(item_quantity)` would incorrectly turn empty datapoints into `0` which might not be desirable (imagine turning missing total amount `""` into `0`, for example).

:::

Full configuration would look like this (the SQL can be copy-pasted but must be inline):

```json
{
  "transformations": [
    {
      "data_sources": [
        {
          "schema_id": "line_items",
          "table_name": "inmemory_line_items"
        }
      ],
      "sql_statement": "SELECT\n    MAX(item_code) as item_code_grouped,\n    MAX(item_description) as item_description_grouped\n    COALESCE(SUM(NULLIF(item_quantity, '')), '') AS item_quantity_grouped\nFROM\n    inmemory_line_items\nGROUP BY\n    item_code",
      "output_schema_id": "line_items_grouped",
      "allow_target_update": true
    }
  ]
}
```

## Group line items conditionally

In some cases (for example, when integrating with NetSuite), it is necessary to group only so called "inventory items" and keep "expenses" intact. This can be achieved using a bit more verbose `GROUP BY` clause:

```sql
SELECT
    MAX(item_type) as item_type_grouped,
    COALESCE(SUM(NULLIF(item_quantity, '')), '') AS item_quantity_grouped,
    MAX(item_description) as item_description_grouped,
    MAX(item_ns_item_match) as item_ns_item_match_grouped
FROM
    inmemory_line_items
GROUP BY
-- highlight-start
    CASE
        WHEN item_type = 'inventory_item' THEN item_ns_item_match
        ELSE item_index -- a unique identifier for each row to prevent grouping for 'expense' type rows
    END;
-- highlight-end
```

The `item_index` from the SQL above is a [formula field](../rossum-formulas/formula-fields.md) with the following definition (to give each row a unique number):

```py
field._index
```

The SQL, of course, needs to be copied to the actual configuration which could look like this, for example:

```json
{
  "transformations": [
    {
      "data_sources": [
        {
          "schema_id": "line_items",
          "table_name": "inmemory_line_items"
        }
      ],
      "sql_statement": "SELECT\n    MAX(item_type) as item_type_grouped,\n    COALESCE(SUM(NULLIF(item_quantity, '')), '') AS item_quantity_grouped,\n    MAX(item_description) as item_description_grouped,\n    MAX(item_ns_item_match) as item_ns_item_match_grouped\nFROM\n    inmemory_line_items\nGROUP BY\n    CASE\n        WHEN item_type = 'inventory_item' THEN item_ns_item_match\n        ELSE item_index -- a unique identifier for each row to prevent grouping for 'expense' type rows\n    END;",
      "output_schema_id": "line_items_grouped",
      "allow_target_update": false
    }
  ]
}
```
---
sidebar_position: 2
sidebar_label: 'Alternative Python solution'
title: 'Line items grouping: Alternative Python solution'
---

# Alternative Python solution

Consider using the following simple Python code (as a [serverless function](../rossum-formulas/serverless-functions.md)) that replaces the whole functionality of this extension (no need for any webhook):

```py
from collections import defaultdict
from rossum_python import RossumPython, is_empty, default_to, is_set


def sum_values(values):
    """Sums values if there are any, otherwise returns an empty string (not zero)."""
    return sum(v for v in values if is_set(v)) if any(is_set(v) for v in values) else ''


def rossum_hook_request_handler(payload):
    """Group and sum VAT rows by rate, copying into export field."""
    x = RossumPython.from_payload(payload)

    # Reset the target table:
    x.field.tax_details_export = []

    vat_rate_groups = defaultdict(lambda: {
        'tax_detail_base_export': [],
        'tax_detail_tax_export': [],
        'tax_detail_total_export': [],
        'tax_detail_description_export': None
    })

    for row in x.field.tax_details:
        group = vat_rate_groups[row.tax_detail_rate_normalized.attr.value]
        group['tax_detail_base_export'].append(row.tax_detail_base_normalized)
        group['tax_detail_tax_export'].append(row.tax_detail_tax_normalized)
        group['tax_detail_total_export'].append(row.tax_detail_total_normalized)
        group['tax_detail_description_export'] = row.tax_detail_description

    x.field.tax_details_export = [
        {
            'tax_detail_rate_export': rate,
            'tax_detail_base_export': sum_values(values['tax_detail_base_export']),
            'tax_detail_tax_export': sum_values(values['tax_detail_tax_export']),
            'tax_detail_total_export': sum_values(values['tax_detail_total_export']),
            'tax_detail_description_export': values['tax_detail_description_export'],
        }
        for rate, values in vat_rate_groups.items()
    ]

    return x.hook_response()
```

Alternatively, you can use `pandas` to do the same thing (note however, that it can have a performance impact since `pandas` is a heavy dependency):

```py
import pandas as pd
from txscript import TxScript, is_empty, default_to, is_set


def sum_values(values):
    """Sums values if there are any, otherwise returns an empty string (not zero)."""
    return sum(v for v in values if is_set(v)) if any(is_set(v) for v in values) else ''


def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    # Reset the target table:
    t.field.line_items_grouped = []

    # Collect all relevant data:
    data = []
    for row in t.field.line_items:
        data.append({
            "item_rate_grouped": row.item_rate.attr.value,  # Must use attr.value because of the `groupby` call!
            "item_description_grouped": row.item_description,
            "item_total_base_grouped": row.item_total_base,
            "item_tax_grouped": row.item_tax,
            "item_amount_total_grouped": row.item_amount_total,
        })

    # Group the data if any:
    if len(data) > 0:
        # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html
        t.field.line_items_grouped = (
            pd.DataFrame(data)
            .groupby('item_rate_grouped')
            .agg({
                "item_description_grouped": "first",
                "item_total_base_grouped": sum_values,
                "item_tax_grouped": sum_values,
                "item_amount_total_grouped": sum_values
            })
            .reset_index().to_dict("records")
        )

    return t.hook_response()
```
<!-- To create the short URL, go to any Confluence page and click: "Share" and then "Copy link" -->

:::info[Additional info for Rossum employees]

Please visit the following restricted link to learn more: <a href={props.url} target="_blank" rel="noopener noreferrer">{props.url}</a>

:::
| Environment   | Webhook URL                                            |
| :------------ | :----------------------------------------------------- |
| EU1 Ireland   | {props.eu1 ? <a href={props.eu1}>{props.eu1}</a>: "—"} |
| EU2 Frankfurt | {props.eu2 ? <a href={props.eu2}>{props.eu2}</a>: "—"} |
| US east coast | {props.us ? <a href={props.us}>{props.us}</a>: "—"}    |
| Japan Tokyo   | {props.jp ? <a href={props.jp}>{props.jp}</a>: "—"}    |
:::danger[Deprecated]

This feature is deprecated. It won't receive any further updates and is likely to be completely removed in the future. Please consider using other available alternatives or contacting support@rossum.ai for further assistance.

:::
---
title: 'JSON Templating'
sidebar_position: 1
---

JSON Templating engine is commonly used to configure many of our extensions.

## JSON Templating Syntax

The basic and most commonly used syntax is for getting values from the annotation object:

```json
{
  "some_field": "@{schema_id_of_datapoint}"
}
```

See also [`$DATAPOINT_VALUE$`](#datapoint_value) for more verbose syntax.

## JSON Templating Operators

Operators are a special syntax that can be used to perform various complex operations when working with JSON.

### `$DATAPOINT_MAPPING$`

Returns rendered template specified in `mapping` for certain value of `schema_id`.

#### Example

```json
{
  "_ns_type": {
    "$DATAPOINT_MAPPING$": {
      "schema_id": "document_type",
      "mapping": {
        "tax_credit": "VendorCredit",
        "tax_invoice": "VendorBill"
      },
      "fallback_mapping": "Vendor" // optional
    }
  }
}
```

This template will return the following for `document_type` with value `tax_credit`:

```json
{ "_ns_type": "VendorCredit" }
```

Similarly, this template will return the following for `tax_invoice` value:

```json
{ "_ns_type": "VendorBill" }
```

Finally, the template will return the `_ns_type` of `Vendor` if no other value is found. This value can be omitted which will default to `null`.

#### Available configuration options

| Configuration option | Description                                                                                              | Required | Default |
| :------------------- | :------------------------------------------------------------------------------------------------------- | :------- | :------ |
| `schema_id`          | Schema ID of the datapoint.                                                                              | YES      |         |
| `mapping`            | Mapping of the datapoint value (`schema_id`).                                                            | YES      |         |
| `fallback_mapping`   | Default template that will be used if template for found datapoint value is not provided in the mapping. | no       | `null`  |

### `$DATAPOINT_VALUE$`

`$DATAPOINT_VALUE$` is a more verbose syntax for `@{schema_id}`. In other words, the following examples are identical:

```json
{
  "externalId": "@{ns_external_id_generated}"
}
```

Identical to:

```json
{
  "externalId": {
    "$DATAPOINT_VALUE$": {
      "schema_id": "ns_external_id_generated",
      "value_type": "string"
    }
  }
}
```

While the latter might seem unnecessary, it is needed when we want to cast the value to a certain type. For example:

```json
{
  "tranDate": {
    "$DATAPOINT_VALUE$": {
      "schema_id": "date_issue",
      "value_type": "iso_datetime"
    }
  }
}
```

#### Available configuration options

| Configuration option | Description                                                                                                                    | Required | Default  |
| :------------------- | :----------------------------------------------------------------------------------------------------------------------------- | :------- | :------- |
| `schema_id`          | Schema ID of the datapoint.                                                                                                    | YES      |          |
| `value_type`         | Type to which the value should be converted. Supported types are: `string`, `integer`, `float`, `boolean`, and `iso_datetime`. | no       | `string` |

### `$FOR_EACH_SCHEMA_ID$`

Iterates over multiline schema IDs, typically line items.

#### Example

```json
{
  "items": {
    "$FOR_EACH_SCHEMA_ID$": {
      "schema_id": "line_item",
      "mapping": {
        "_ns_type": "VendorBillItem",
        "quantity": "@{item_quantity}",
        "description": "@{item_description}"
      }
    }
  }
}
```

Will render:

```json
{
  "items": [
    {
      "_ns_type": "VendorBillItem",
      "quantity": "1",
      "description": "Some description 1"
    },
    {
      "_ns_type": "VendorBillItem",
      "quantity": "2",
      "description": "Some description 2"
    }
    // …
  ]
}
```

Inside the for-loop block, you can access a special variables (`schema_loop`):

- `schema_loop.index` for current iteration of the loop indexed from 1
- `schema_loop.index0` for current iteration of the loop indexed from 0

#### Available configuration options

| Configuration option | Description                                                                               | Required | Default |
| :------------------- | :---------------------------------------------------------------------------------------- | :------- | :------ |
| `schema_id`          | Schema ID of the datapoint.                                                               | YES      |         |
| `mapping`            | Mapping template that will be rendered for each element found with the given `schema_id`. | YES      |         |
| `fallback_mapping`   | Mapping template that will be rendered if no element with the given `schema_id` is found. | no       | `[]`    |

### `$IF_DATAPOINT_VALUE$`

Renders the provided `mapping` template if the value of the given `schema_id` is equal to `value`. Otherwise, the whole parent key is skipped.

Note that if element with given `schema_id` is not found or multiple with the same `schema_id` is found an exception is raised.

#### Example

```json
{
  "Other_Field": 123,
  "Company_Reference": {
    "$IF_DATAPOINT_VALUE": {
      "schema_id": "company_match",
      "value": "my_company",
      "mapping": {
        "field_1": 456,
        "field_2": 789
      }
    }
  }
}
```

If `company_match` has value `my_company` the output will be:

```json
{
  "Other_Field": 123,
  "Company_Reference": {
    "field_1": 456,
    "field_2": 789
  }
}
```

If `company_match` has value that is different from `my_company` the output will be:

```json
{
  "Other_Field": 123
}
```

#### Available configuration options

| Configuration option | Description                                                             | Required | Default |
| :------------------- | :---------------------------------------------------------------------- | :------- | :------ |
| `schema_id`          | Schema ID of the relevant datapoint.                                    | YES      |         |
| `value`              | Conditional (expected) value in `schema_id`.                            | YES      |         |
| `fallback_mapping`   | Mapping template that will be rendered if `schema_id` contains `value`. | YES      |         |

### `$IF_SCHEMA_ID$`

Similar to [`$FOR_EACH_SCHEMA_ID$`](#for_each_schema_id) operation: it checks the existence of `schema_id`, and it either outputs `mapping` if the `schema_id` points to a non-empty value or `fallback_mapping` otherwise.

Note that if you don't specify the `fallback_mapping`, it will skip the whole parent key!

#### Example

```json
{
  "dueDate": {
    "$IF_SCHEMA_ID$": {
      "schema_id": "date_due",
      "mapping": {
        "$DATAPOINT_VALUE$": {
          "schema_id": "date_due",
          "value_type": "iso_datetime"
        }
      }
    }
  }
}
```

#### Available configuration options

| Configuration option | Description                                                                                                                                                                         | Required | Default |
| :------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------- | :------ |
| `schema_id`          | Schema ID of the datapoint.                                                                                                                                                         | YES      |         |
| `mapping`            | Mapping template that will be rendered if `schema_id` exists.                                                                                                                       | YES      |         |
| `fallback_mapping`   | Mapping template that will be rendered if no element with the given `schema_id` is found. Note that if you don't specify the `fallback_mapping`, it will skip the whole parent key! | no       |         |
---
title: 'Full page search'
sidebar_position: 1
---

import RossumInternalOnly from '../\_rossum_internal_only.md';
import WebhookEndpoints from '../\_webhook_endpoints.md';

# Full page search

This extension searches defined keywords in the full page content and sets a target field value if a match is found.

<RossumInternalOnly url="https://rossumai.atlassian.net/wiki/x/IgCW6g" />

## Installation

**Full page search** extension is provided and maintained by Rossum.ai in the form of webhook. To start using it, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `Full page search`
   1. Trigger events: `Initialize, Started, Updated`
   1. Extension type: `Webhook`
   1. URL (see below)
1. In "Advanced settings" select **Token owner** (should have Admin access)
1. Click **Create the webhook**.

<WebhookEndpoints
  eu1="https://elis.full-page-search.rossum-ext.app/"
  eu2="https://shared-eu2.full-page-search.rossum-ext.app/"
  us="https://us.full-page-search.rossum-ext.app/"
  jp="https://shared-jp.full-page-search.rossum-ext.app/"
/>

## Configuration examples

```json
{
  "configurations": [
    {
      // See: https://elis.rossum.ai/api/docs/#get-page-spatial-data
      "granularity": "lines",
      "target_field": "is_credit_note",
      "target_value": "Y",

      // Note that the possible values are case-insensitive.
      "possible_values": ["credit invoice", "credit note", "credit nota"]
    },
    {
      "granularity": "words",
      "target_field": "is_kredietbeperking",
      "target_value": "Y",
      "possible_values": ["kredietbeperking"]
    }
  ]
}
```

## Known limitations

Note that the [`page_numbers` parameter](https://elis.rossum.ai/api/docs/#get-page-spatial-data) cannot be specified. Only the first 20 pages of a document are used. In other words, if the text is on page 21 or higher, it won't be found.
---
sidebar_position: 1
sidebar_label: 'Alternative Python solution'
title: 'Full page search: Alternative Python solution'
---

# Alternative Python solution

This function retrieves textual data from Rossum's [`page_data`](https://elis.rossum.ai/api/docs/#get-page-spatial-data) API for an annotation and processes it to:

1. Fetch OCR document content Data: Make an HTTP GET request to the `page_data` endpoint of a specific annotation using the provided `rossum_authorization_token`.
1. Retry Mechanism: Handle transient network or server issues by retrying up to 3 times in case of a non-200 HTTP response or exceptions.
1. Process Text Content: Iterate through the fetched text content for custom manipulations or pattern analysis.

```py
import requests


def get_ocr_document_content(payload):
    """
    Fetch page_data from annotation.
    :param payload: Dictionary containing the payload with annotation information.
    """
    token = payload.get("rossum_authorization_token")
    annotation_url = payload.get("annotation", {}).get("url")

    retries = 3
    for attempt in range(retries):
        try:
            # Request to fetch text content from annotation
            page_req = requests.get(
                url=f"{annotation_url}/page_data?granularity=texts",
                headers={"Authorization": f"Bearer {token}"}
            )

            if page_req.status_code == 200:
                results = page_req.json().get("results", [])
                # This part is optional iteration through all the text nodes
                for page in results:
                    for item in page.get("items", []):
                        ocr_text = item.get("text", "")
                        if ocr_text:
                            # Here will be any kind of manipulation with the text you need to do.
                            print(ocr_text)

                break  # Exit retry loop if request is successful
            else:
                print(f"Attempt {attempt + 1} failed with status code {page_req.status_code}. Retrying...")

        except requests.RequestException as e:
            print(f"Attempt {attempt + 1} encountered an exception: {e}. Retrying...")
```
---
sidebar_position: 2
sidebar_label: 'Serverless functions'
title: 'Rossum Formulas: Serverless functions'
---

import WIP from '../\_wip.md';

# Serverless functions

Examples of common or interesting serverless functions (using `TxScript` flavor).

## Accounts Payable Checks

Historically, there was an "Accounts Payable Checks" extension in the Rossum store which was used to validate basic calculations on invoices, for example. Nowadays, it is not necessary since the same can be easily achieved using custom serverless function. The following is a (loosely) migrated example of the extension configuration:

<WIP />

```py
import math
from rossum_python import RossumPython, is_set, is_empty, default_to
from datetime import datetime

def rossum_hook_request_handler(payload):
    x = RossumPython.from_payload(payload)
    rounding = 2

    ###################
    ## HEADER FIELDS ##
    ###################

    # 1: "Total Amount" (amount_total) == "Total Without Tax" (amount_total_base) + "Tax Amount" (amount_total_tax)
    if is_set(x.field.amount_total) and is_set(x.field.amount_total_base) and is_set(x.field.amount_total_tax):
        amount_total_ocr = round(x.field.amount_total, rounding)
        amount_total_calculated = round(x.field.amount_total_base + x.field.amount_total_tax, rounding)
        if not math.isclose(amount_total_ocr, amount_total_calculated):
            message = f"Total Amount is not calculated correctly (expected: ~{amount_total_calculated})."
            x.show_warning(message, x.field.amount_total)
            x.automation_blocker(message, x.field.amount_total)

    # 2: Check if "Due Date" (date_due) is within 0 to 120 days after "Invoice Date" (date_issue)
    if is_set(x.field.date_issue) and is_set(x.field.date_due):
        days_difference = (x.field.date_due - x.field.date_issue).days
        if not (0 <= days_difference <= 120):
            message = f"Due Date ({x.field.date_due}) is not within 120 days after Invoice Date ({x.field.date_issue})."
            x.show_warning(message, x.field.date_due)
            x.automation_blocker(message, x.field.date_due)

    #######################
    ## TAX DETAILS table ##
    #######################

    for row in x.field.tax_details:
        # 3: "VAT Amount" (tax_detail_tax) == "VAT Base" (tax_detail_base) * "VAT Rate" (tax_detail_rate)
        if is_set(row.tax_detail_tax) and is_set(row.tax_detail_base) and is_set(row.tax_detail_rate):
            tax_detail_tax_ocr = round(row.tax_detail_tax, rounding)
            tax_detail_tax_calculated = round(row.tax_detail_base * (row.tax_detail_rate / 100), rounding)
            if not math.isclose(tax_detail_tax_ocr, tax_detail_tax_calculated):
                message = f"VAT amount is not calculated correctly (expected: ~{tax_detail_tax_calculated})."
                x.show_warning(message, row.tax_detail_tax)
                x.automation_blocker(message, row.tax_detail_tax)
```

<details>
  <summary>Original default configuration of the "Accounts Payable Checks" extension (for reference).</summary>

The config examples are numbered for easier orientation:

```json
{
  "checks": [
    {
      // 1
      "left": ["amount_total"],
      "right": ["amount_total_base", "amount_total_tax"],
      "epsilon": 0.5,
      "operation": "check_left_sum_equals_right_sum",
      "message_type": "warning",
      "message_content": "{amount_total} is not equal to {amount_total_base} + {amount_total_tax}."
    },
    {
      // 2
      "left": "date_issue",
      "right": "date_due",
      "data_type": "date",
      "operation": "check_right_minus_left_within_range",
      "lower_bound": 0,
      "upper_bound": 120,
      "message_type": "warning",
      "message_content": "{date_due} is not within 120 days after {date_issue}."
    },
    {
      // 3
      "left": ["tax_detail_tax"],
      "right": ["tax_detail_base", "tax_detail_rate"],
      "epsilon": 0.5,
      "operation": "check_left_sum_equals_right_multiplication",
      "message_type": "warning",
      "message_content": "{tax_detail_tax} is not equal to {tax_detail_base} x {tax_detail_rate}."
    },
    {
      // 4
      "left": ["tax_detail_total"],
      "right": ["tax_detail_base", "tax_detail_tax"],
      "epsilon": 0.5,
      "operation": "check_left_sum_equals_right_sum",
      "message_type": "warning",
      "message_content": "{tax_detail_total} is not equal to {tax_detail_base} + {tax_detail_tax}."
    },
    {
      // 5
      "left": ["item_amount_total"],
      "right": ["item_total_base", "item_tax"],
      "epsilon": 0.5,
      "operation": "check_left_sum_equals_right_sum",
      "message_type": "warning",
      "message_content": "{item_amount_total} is not equal to {item_total_base} + {item_tax}."
    },
    {
      // 6
      "left": ["item_total_base"],
      "right": ["item_amount_base", "item_quantity"],
      "epsilon": 0.5,
      "operation": "check_left_sum_equals_right_multiplication",
      "message_type": "warning",
      "message_content": "{item_total_base} is not equal to {item_amount_base} x {item_quantity}."
    },
    {
      // 7
      "left": ["item_amount_total"],
      "right": ["item_amount", "item_quantity"],
      "epsilon": 0.5,
      "operation": "check_left_sum_equals_right_multiplication",
      "message_type": "warning",
      "message_content": "{item_amount_total} is not equal to {item_amount} x {item_quantity}."
    },
    {
      // 8
      "left": ["item_tax"],
      "right": ["item_total_base", "item_rate"],
      "epsilon": 0.5,
      "operation": "check_left_sum_equals_right_multiplication",
      "message_type": "warning",
      "message_content": "{item_tax} is not equal to {item_total_base} x {item_rate}."
    },
    {
      // 9
      "epsilon": 0.5,
      "operation": "check_header_field_equals_table_field_sum",
      "table_field": "tax_detail_total",
      "header_field": "amount_total",
      "message_type": "warning",
      "message_content": "{amount_total} is not equal to the {tax_detail_total} in the Tax table."
    },
    {
      // 10
      "epsilon": 0.5,
      "operation": "check_header_field_equals_table_field_sum",
      "table_field": "tax_detail_base",
      "header_field": "amount_total_base",
      "message_type": "warning",
      "message_content": "{amount_total_base} is not equal to the {tax_detail_base} in the Tax table."
    },
    {
      // 11
      "epsilon": 0.5,
      "operation": "check_header_field_equals_table_field_sum",
      "table_field": "tax_detail_tax",
      "header_field": "amount_total_tax",
      "message_type": "warning",
      "message_content": "{amount_total_tax} is not equal to {tax_detail_tax} in the Tax table."
    },
    {
      // 12
      "epsilon": 0.5,
      "operation": "check_header_field_equals_table_field_sum",
      "table_field": "item_amount_total",
      "header_field": "amount_total",
      "message_type": "warning",
      "message_content": "{amount_total} is not equal to the sum of the line items’ total amounts."
    },
    {
      // 13
      "epsilon": 0.5,
      "operation": "check_header_field_equals_table_field_sum",
      "table_field": "item_total_base",
      "header_field": "amount_total_base",
      "message_type": "warning",
      "message_content": "{amount_total_base} is not equal to the sum of the line items’ total bases."
    },
    {
      // 14
      "epsilon": 0.5,
      "operation": "check_header_field_equals_table_field_sum",
      "table_field": "item_tax",
      "header_field": "amount_total_tax",
      "message_type": "warning",
      "message_content": "{amount_total_tax} is not equal to the sum of the line items’ taxes."
    }
  ]
}
```

</details>

## Block automation when there are multiple data-matching results

The easiest solution might be to show an empty default value and make the field required. However, in case this is not possible, you can create an automation blocker instead:

```py
from rossum_python import RossumPython

def rossum_hook_request_handler(payload):
    x = RossumPython.from_payload(payload)

    if len(x.field.data_matching_enum.attr.options) > 1:
        message = "Please double-check the selection."
        x.automation_blocker(message)
        x.show_warning(message, x.field.data_matching_enum)

    return x.hook_response()
```

## Copy fields conditionally

Copies either `order_id_manual` or `order_id` into `order_id_normalized` depending on whether the manual field is filled or not:

```py
from txscript import TxScript, is_set

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    if is_set(t.field.order_id_manual):
        t.field.order_id_normalized = t.field.order_id_manual
    else:
        t.field.order_id_normalized = t.field.order_id

    return t.hook_response()
```

Note that this is just for illustrative purposes. For this particular use-case, always prefer making `order_id_normalized` a [formula field](./formula-fields.md).

## Date format correction

It can sometimes happen that invoices have dates in format `M/D/YYYY` format. But because the queue is in the UK region (for example), Rossum sometimes understands the dates incorrectly (5/1/2024 on the invoice is incorrectly read as "Jan 5th" instead of the correct "May 1st"). There is simply no way for Rossum AI to know what the correct date should be (especially when the queue region suggests something else).

This can be additionally corrected using the following simple code (if we know for what specific vendor this should be done):

```py
from datetime import datetime
from txscript import TxScript, is_empty

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    relevant_entities = [
        "123456"  # Vendor ABC
    ]

    t.field.date_issue_normalized = flip_day_month(t.field.date_issue) if t.field.ns_entity_match in relevant_entities else t.field.date_issue

    return t.hook_response()


def flip_day_month(date_value):
    if is_empty(date_value):
        return date_value

    day, month = date_value.day, date_value.month
    raw_text = date_value.attr.ocr_raw_text or date_value.attr.rir_raw_text

    try:
        raw_month, raw_day, raw_year = map(int, raw_text.split('/'))
    except ValueError:
        # Handle case where raw text isn't in the expected format
        return date_value

    # Check if the date might be misinterpreted (e.g., 5/1/2024 as January 5th instead of May 1st)
    if day == raw_day and month == raw_month:
        # No flip needed if day/month match the expected positions
        return date_value

    # Check if flipping makes logical sense (both day and month must be 12 or below)
    if day <= 12 and month <= 12:
        return datetime(date_value.year, day=month, month=day)
    else:
        return date_value
```

Alternative (and more generic implementation) can be found here: https://gist.github.com/mrtnzlml/1c7470e48acb840b4d6fdb6af2fcb2c9

## Get annotation information

```py
from txscript import TxScript

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    # Annotation ID:
    t.field.annotation_id = payload.get("annotation").get("id")

    # Document page count:
    t.field.page_count = len(payload.get("annotation").get("pages"))

    return t.hook_response()
```

## Get document information

```py
from txscript import TxScript

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    # Arrival date:
    t.field.document_arrived_at = payload.get("document").get("arrived_at")

    # Original file name:
    t.field.document_original_file_name = payload.get("document").get("original_file_name")

    return t.hook_response()
```

## Get queue name

To store the queue name in a schema data point `queue_name` please use the following code. Note that it is necessary to sideload both **Schema** and **Queue** in the extension setup.

```py
from txscript import TxScript

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    t.field.queue_name = payload.get("queues")[0].get("name")

    return t.hook_response()
```

## Validate header fields

```py
from txscript import TxScript, is_empty

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    # Header total = subtotal + taxes:
    if is_set(t.field.amount_due) and is_set(t.field.amount_total_base) and is_set(t.field.amount_total_tax):
        amount_total_base = default_to(t.field.amount_total_base, 0)
        amount_total_tax = default_to(t.field.amount_total_tax, 0)
        amount_due = default_to(t.field.amount_due, 0)
        if amount_due != (amount_total_base + amount_total_tax):
            message = "Total invoice amount is not equal to the sum of amount base and the tax."
            t.show_error(message, t.field.amount_due)

    return t.hook_response()
```

## Validate line items

In serverless functions, it is necessary to iterate the individual line items and perform the validations on row level:

```py
from txscript import TxScript, is_empty

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    for row in t.field.line_items:
        if is_empty(row.item_code):
            t.show_error("Item code is required on line items.", row.item_code)

    return t.hook_response()
```

## Working with "updated datapoints"

When users update any datapoints on the annotation screen, the serverless function can get triggered and the resulting values can get recalculated (assuming the extension is listening to "updated" trigger event). You can work with this information to either prevent the updates or optimize the execution (skip recalculations if not needed). To do so, use the following code:

```py
from txscript import TxScript

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    if t.field.my_custom_field.id in payload.get("updated_datapoints", []):
        # Perform any logic when "my_custom_field" field is updated
        # …

    return t.hook_response()
```
---
sidebar_position: 1
sidebar_label: 'Formula fields'
title: 'Rossum Formulas: Formula fields'
toc_max_heading_level: 4
---

# Formula fields

TODO (https://gitlab.rossum.cloud/solution-engineering/customers/deliveroo/deliveroo-solution/-/blob/12d2ad40682c2463dbce187531ed3f22f11167aa/production-org/default/hooks/Coupa%20validator%20(base)_%5B1066662%5D.py):

.attr.hidden = True/False

## Intro

Formula fields in Rossum enable you to easily transform your data directly within the app. Whether you need to normalize data, perform calculations, or handle text operations, formula fields provide the flexibility to set values based on your custom logic.

For most tasks, Rossum’s Copilot handles everything seamlessly without needing code, and we recommend using it for the best results. However, if you want to dive deeper into custom logic, formula fields support Python-based coding with simple examples to get you started.

:::info

This powerful feature is available on the Business plan and above. Existing customers interested in using formula fields can reach out to our support team at support@rossum.ai for assistance.

:::

## Basic information

- Formula Fields can run any Python code including its [Standard Library modules](https://docs.python.org/3/library/index.html).
- Additionally, the runtime is enriched with Rossum-specific functions and variables
- They are executed in an AWS lambda
- Formula Fields are automatically executed before and after each extension.
- Extensions cannot overwrite Formula Fields value (create a separate “data” field instead).

## Best practices

### Field naming and management

When you need to create a normalized version of a field like `document_id`, it's recommended to create a new field, such as `document_id_normalized`. Use the formula field to apply the normalization logic. This approach allows you to preserve the original value while keeping the normalized result in a separate field, making it easier to manage both.

For better organization, it's also a good practice to group these related fields together, ensuring they are logically aligned and easy to find.

### Wide variety of functions

As mentioned earlier, formula fields allow you to work with Python, enabling operations similar to those in serverless functions. However, formula fields are much simpler to maintain and manage, offering a streamlined approach for straightforward tasks.

### Access document object information in formula field

Unfortunately right now there is no better way than create a simple serverless function to store desired information to custom field and continue with that one in formula field.

### When to Use Formula Fields vs. Serverless Functions

Formula fields are ideal for simple tasks such as data normalization or creating new fields based on existing ones. For more complex operations, serverless functions may be more appropriate. Situations where you should prefer serverless functions include:

- There is a limit of **2000** characters per formula fields which declares the highest complexity of the formula fields.
- You cannot access the document object from within the formula function.
- Formula Fields cannot and should not make HTTP requests (to Rossum API or elsewhere).
- Formula Fields are executed only within the scope the specific field; for many rows (200+), the execution time may be too long.
- You need to set annotation status (e.g., “rejected”, “postponed”, etc.), you have to use Serverless functions.
- You want to edit multiple fields at the same time
- Manipulate enums

An additional advantage of formula fields is that they are stored at the schema level, so when you copy a queue, all associated formula fields are copied automatically. In contrast, serverless functions must be configured manually and re-linked to new queues after being copied.

## Examples of common formula fields

### Access other table and its first row values

```py
field.table_name[0].column_name
```

### Copy fields conditionally

Copy `order_id` into another field but prioritize `order_id_manual` datapoint if it exists:

New formula field `order_id_normalized`:

```py
field.order_id_manual if not is_empty(field.order_id_manual) else field.order_id
```

:::danger

Note that copying numbers like this does not copy the original format (specifically trailing zeros). So copying number `123.40` will result in `123.4` (notice the missing zero). In most of the cases, this is not an issue, however. To get the original format (as a string), you must use the following code:

```py
field.amount.attr.value  # "123.40"
```

:::

### Get all enum options

You can access all enum options via special `attr` attribute. The following code would return the number of enum options (regardless of what option is selected):

```py
len(field.language.attr.options)
```

### Find first non-empty line item value

```py
next((item for item in field.item_code.all_values if item), "")
```

### Generate NetSuite external IDs

Create external ID needed by NetSuite for _VendorBill_ and _VendorCredit_ records:

```py
# Create an external ID by combining document ID and entity (vendor) match. This is to make sure
# that different vendors with identical document numbering are not matched to the same NetSuite
# record (same NetSuite external ID).
external_id = f"{field.document_id}__{field.order_match__entity_internalId}"

# Construct the final result by concatenating (and normalizing) Rossum prefix, document type, and external ID:
substitute(r"[^a-zA-Z\d\-_]", "", f"__rossum__{field.document_type}__{external_id}".lower())
```

This is typically necessary when [exporting records into NetSuite](../netsuite/export-configuration#vendor-bills-invoices).

### Get line item index

Returns line item number (indexed from 0):

```py
field._index
```

### Normalize field value

Remove non-alphanumeric characters (except "-" and "\_"):

```py
substitute(r"[^a-zA-Z\d\-_]", "", field.order_id)
```

### Sum line item values

Sum the values of `item_amount_total`. Use `0` if the field is empty.

```py
sum(default_to(field.item_amount_total.all_values, 0))
```

### Validations

:::warning

Consider using a [Serverless function](./serverless-functions.md) as a better place to perform such validations. Internally, we consider this technique deprecated, albeit still valid. Formula fields should not affect behavior of other fields indirectly as it makes the overall solution less readable and predictable.

:::

To validate line items, create `item_validator` formula field with the following code:

```py
import math

item_total_base_calculated = field.item_quantity * field.item_amount_base

if not math.isclose(item_total_base_calculated, field.item_total_base, rel_tol=0.004):
    item_total_base_diff = abs(item_total_base_calculated - field.item_total_base)
    message = (f"The totals do not match. Expected total: {field.item_total_base}, "
               f"Calculated total: {item_total_base_calculated}, "
               f"Difference: {item_total_base_diff}")

    show_error(message, field.item_quantity)
    show_error(message, field.item_amount_base)
    show_error(message, field.item_total_base)
```

### Get year/month from a date field

Returns year/month integer of a date field:

```py
field.date_issue.year
field.date_issue.month
```

### Date validation

This function, `check_invoice_date`, checks if the invoice date (passed as `document_date`) is more than 90 days old compared to the current date. It calculates the difference in days between the two dates and, if the document is older than 90 days, triggers a warning message. This message notifies the user of the outdated date and blocks further automation until the issue is resolved.

```py
# import fo the datetime module is not necessary as it is already imported by default
import datetime
def check_invoice_date(document_date):
    # Get the current date
    current_date = datetime.datetime.now().date()

    # Calculate the difference in days between the current date and the document date
    days_difference = (current_date - document_date).days

    # Check if the document date is older than 90 days
    if days_difference > 90:
        # Raise a warning and set an automation blocker
        warning_message = f"Warning: Invoice date is older than 90 days ({days_difference} days). Please confirm the date."
        automation_blocker(warning_message, field.date_issue)
        show_warning(warning_message)

check_invoice_date(field.date_issue)
```

### HTML formatting

Basic HTML formatting is available inside show_warning() and similar functions.
You can even paste links (e.g, to the ERP system).

Example:

```python
show_warning("""
<ul>
    <li>I am in a list!</li>
    <li>Me too!</li>
</ul>
""")
```

Will render as:

![Warning example](img/warning_message.png)

### VAT Rates table fallbacks

It is often necessary to calculate missing values from the captured VAT information. For example, if we know "VAT Base" and "VAT Amount", we can calulate the missing "VAT Rate". The following section shows all the necessary combinations to calculate the missing values.

All the following examples are rounding the values to two decimal places.

#### `tax_detail_rate_calculated`

```py
if is_set(field.tax_detail_rate):
    round(field.tax_detail_rate, 2)
elif is_set(field.tax_detail_base) and is_set(field.tax_detail_tax) and field.tax_detail_base != 0:
    round((field.tax_detail_tax / field.tax_detail_base) * 100, 2)
else:
    0
```

#### `tax_detail_base_calculated`

```py
if is_set(field.tax_detail_base):
    round(field.tax_detail_base, 2)
elif is_set(field.tax_detail_tax) and is_set(field.tax_detail_rate) and field.tax_detail_rate != 0:
    round(field.tax_detail_tax * 100 / field.tax_detail_rate, 2)
```

#### `tax_detail_tax_calculated`

```py
if is_set(field.tax_detail_tax):
    round(field.tax_detail_tax, 2)
elif is_set(field.tax_detail_base) and is_set(field.tax_detail_rate):
    round(field.tax_detail_base * field.tax_detail_rate / 100, 2)
```

#### `tax_detail_total_calculated`

```py
if is_set(field.tax_detail_total):
    round(field.tax_detail_total, 2)
elif is_set(field.tax_detail_base) and is_set(field.tax_detail_tax):
    round(field.tax_detail_base + field.tax_detail_tax, 2)
elif is_set(field.tax_detail_base) and is_set(field.tax_detail_rate):
    round(field.tax_detail_base * (1 + field.tax_detail_rate / 100), 2)
elif is_set(field.tax_detail_rate) and is_set(field.tax_detail_tax) and field.tax_detail_rate != 0:
    round(field.tax_detail_tax / (field.tax_detail_rate / 100) + field.tax_detail_tax, 2)
```

### Check existence of a field in the schema

Formula fields expect that all fields are present in the schema. If a field is missing, an error will be raised. For example:

```txt
AttributeError: Field 'doesntexist' is not defined

Traceback (most recent call last):
  at line 7, in <formula:test_ff>:
    if is_set(field.doesntexist):
```

If you'd like to check the field existence, use one of the following formulas (all of them should work):

```py
if 'doesntexist' in vars(field) and is_set(value := field.doesntexist):
    show_info(value)
```

Or:

```py
if hasattr(field, 'doesntexist') and is_set(value := field.doesntexist):
    show_info(value)
```

Or:

```py
if is_set(value := getattr(field, 'doesntexist', None)):
    show_info(value)
```

Or:

```py
try:
    if is_set(value := field.doesntexist):
        show_info(value)
except AttributeError:
    pass
```

### Remove whitespaces

```py
''.join(field.iban.split())
```
---
sidebar_position: 3
sidebar_label: 'Custom code handbook'
title: 'Rossum Formulas: Custom code handbook'
---

# Custom code handbook

Custom code refers to user-defined logic written as [Serverless Functions](https://rossum.ai/help/article/customize-rossum-logic-with-serverless-function/) in Rossum \- it’s an implementation of AWS Lambda using Python 3.12. This means writing code that runs in a stateless, event-driven compute environment. These functions execute custom operations in response to triggers without the need to manage server infrastructure.

## 1\. Determine if any code is necessary

Before writing custom code, consider whether it's truly needed. Rossum offers functionalities in the [Rossum Store](https://elis.rossum.ai/settings/store), [Formula Fields](https://rossum.ai/help/article/formula-fields-an-advanced-guide-to-manual-formula-creation/) or [Export pipelines](../export-pipeline) that might already fulfill your requirements. Utilizing existing solutions can save time and reduce complexity.

### When to avoid writing custom code:

- **Rossum Store Solutions**: Check the [Rossum Store](https://elis.rossum.ai/settings/store) for pre-built extensions and integrations that might meet your needs.
- **Formula Fields**: Use [Formula Fields](https://rossum.ai/help/article/formula-fields-an-advanced-guide-to-manual-formula-creation/) for simple data transformations or calculations within Rossum's platform without the need for custom functions.

## 2\. Write once, read many times

When writing the code, the key principle is to make your code as **readable** as possible. You'll revisit your code multiple times after writing it, especially during debugging or future updates. Aim for code that's easy to scan rather than requiring deep interpretation.

**Meaningful naming**: Use clear, descriptive names for functions, variables and serverless functions themselves to convey their purpose without needing to delve into the details. Avoid one-character variable names. Always use English\!

Example:

```py
def extract_invoice_data(invoice) -> None:
    # Logic to extract data from an invoice
    pass
```

**Thoughtful comments**: Add comments where necessary, especially around complex logic. Avoid over-commenting simple lines but provide insights into the **why** behind decisions or non-obvious steps.

Example:

```py
# This ensures that tasks with closer deadlines are processed first
sorted_tasks = sort_tasks_by_deadline(tasks)
```

**Expose common attributes**: Extract variables that reference external attributes to the hook settings and reference them by fetching their value from the payload’s settings attribute.

Example:

Bad practice - hook code

```py
API_URL = "https://api.example.com/v1"
WRITE_RESPONSE_TARGET_FIELD = "response_datapoint"
```

Good practice \- hook.settings

```json
{
  "API_URL": "https://api.example.com/v1",
  "WRITE_RESPONSE_TARGET_FIELD": "response_datapoint"
}
```

**Define secrets schema**: If a function requires any secrets (password, client ID, …), make sure the secrets keys are visible by defining JSON schema in hook.secrets_schema. They must be applied via API hook PATCH call.

```json
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "username": {
      "type": "string"
    },
    "password": {
      "type": "string"
    }
  },
  "required": ["username", "password"]
}
```

**Modular code**: Break down your code into small, reusable functions with single responsibilities. This makes your code cleaner and easier to debug and test.

Example:

```py
def process_data(data):
   # …

def validate_data(data):
   # …

def transform_data(data):
   # …
```

**Leverage modern python features**: Utilize features from Python 3.12 version to simplify and enhance your code. The walrus operator (`:=`) allows you to assign values within expressions, making code more concise. The \`**match**\` statement can simplify complex conditional logic.

Example with walrus operator:

```py
def process_data(data: dict[str, Any]) -> dict[str, Any]:
    if parsed_data := parse_data(data):
        return transform_data(parsed_data)
    raise ValueError("Invalid data")
```

Example with a `match` statement:

```py
def handle_error(error: Exception) -> None:
    match error:
        case ValueError():
            print(f"ValueError occurred: {error}")
        case KeyError():
            print(f"KeyError occurred: {error}")
        case _:
            print(f"Unexpected error: {error}")
```

### Use typing annotations wisely

Typing annotations enhance code readability and help catch type-related errors. However, it's important to use them judiciously to avoid clutter and ensure they add value.

**Use where they add value**: Apply typing annotations to function signatures, variables, and return types to make the code more self-documenting.

Example:

```py
def calculate_total(amounts: dict[str, float]) -> float:
    return sum(amounts.values())
```

**Avoid overusing typing**: Don't annotate every single variable or function if it doesn't add meaningful clarity. Focus on areas where type information aids understanding and maintenance.

## 3\. Use Rossum Python (TxScripts)

Whenever you access or modify the payload, or need to display a message to the user, use **Rossum Python** (soon to be renamed **TxScripts**). You can learn more about it at [Rossum Python documentation](../rossum-formulas/serverless-functions.md) and [TxScripts documentation](https://elis.rossum.ai/api/docs/#rossum-transaction-scripts).

Example:

```py
from txscript import TxScript

def rossum_hook_request_handler(payload: dict) -> dict:
    t = TxScript.from_payload(payload)
    print(t)
    return t.hook_response()
```

## 4\. Effective debugging with `print()`

While using a logger is considered the best practice for structured and maintainable logging, the simplest approach is to use `print()`. Logs are crucial for troubleshooting, especially when you don’t have access to a traditional UI for monitoring. The Rossum platform captures output from standard streams, so `print()` can be an effective and easy method for logging. To view this output: _Extensions \-\> Logs \-\> Detail_ of log record, “output” key.

**Log at key points**:

- Log critical entry points along with their input parameters.
- Log exceptions and errors.
- Focus on logging information that provides meaningful insights and isn't readily apparent.
- Accumulate loop outputs and log them as a single record.

**Avoid sensitive data**: Do not log sensitive information such as personally identifiable information (PII) or credentials.

**Be concise**: Too many logs can make debugging harder. Focus on logging important checkpoints and errors.

## 5\. Exception handling: be specific, avoid catching `Exception`

Proper error handling ensures that failures are visible and correctable. Using broad exceptions like `Exception` can mask critical issues, making debugging more difficult.

**Catch specific exceptions**: to prevent hiding important bugs. Any unhandled exception should eventually be caught in the entry point function (e.g., `rossum_hook_request_handler()`), ensuring they are appropriately logged and managed by the Rossum platform.

Bad Practice:

```py
try:
    process_data(data)
except Exception as e:
    print(f"An error occurred: {e}")
```

Good Practice:

```py
try:
    process_data(data)
except ValueError as e:
    print(f"ValueError: {e}")
    raise
except KeyError as e:
    print(f"KeyError: Missing expected key: {e}")
    raise
```

**Re-raise exceptions**: After catching and logging, always re-raise the exception unless there's a valid fallback strategy. This ensures the failure is handled appropriately.

**Graceful failure handling**: Catch predictable errors (like invalid input) and return meaningful messages or default values without crashing the function entirely.

**Fail fast**: If something is wrong with the data or the flow, handle it immediately. Do not let errors propagate unnoticed.

Example:

```py
def rossum_hook_request_handler(payload: dict) -> dict:
   messages, operations = [], []
   try:
       messages, operations = main(payload)
   except Exception as e:
       print(f"Raised exception: {e}")
       messages = [create_message("error", f"MyFunction raised an exception: {e}")]
   return {"messages": messages, "operations": operations}


def main(payload: dict) -> tuple[list, list]:
  # …
  try:
      process_data(data)
  except ValueError as e:
      print(f"ValueError: {e}")
      raise
  # …
  return messages, operations
```

## 6\. Understand payload structure

Understanding Rossum's payload structure is essential for effective data parsing and manipulation.

**Data format**: Rossum typically uses JSON for request and response payloads.

**Variable keys based on hook configuration**: The keys present in the payload depend on your hook configuration. This means the structure and content of the payload can vary depending on how you have set up your function within Rossum. For example:

- **Authentication Token**: If your hook configuration includes an authentication token, the payload will contain keys like `rossum_authorization_token`. Use this token for authenticated API calls without hardcoding credentials.
- **Sideloaded Data**: If you have configured your hook to sideload schemas or other data, the payload will include additional keys such as `schema`, `document`, or `annotations`.
  - **URLs vs. Sideloaded Data**: Often, keys in the payload have values that are URLs pointing to resources like documents or schemas. However, when you enable sideloading for certain data (e.g., schemas, annotations), the value of those keys changes from a URL to a dictionary containing the sideloaded data.  
    See [https://elis.rossum.ai/api/docs/\#sideloading](https://elis.rossum.ai/api/docs/#sideloading)
- **Custom Parameters**: Any custom parameters you define in your hook configuration will also appear as keys in the payload (`settings`, `secrets`).

**Data extraction**:

Use appropriate keys to extract the required data.

Example:

```py
base_url = payload.get("base_url", "https://api.elis.rossum.ai")
auth_token = payload.get("rossum_authorization_token")
document_id = payload["document"]["id"]
```

**Event and action validation**: As a good practice and part of defensive programming, you should verify that the `event` and `action` fields in the payload match those defined in your hook configuration. This ensures that your function responds appropriately to the [correct triggers](https://elis.develop.r8.lol/api/docs/#webhook-events).

```py
def rossum_hook_request_handler(payload: Dict) -> Dict:
   if payload["event"] != "annotation_content" or (
       payload["action"] not in ["started", "user_update", "updated", "initialize"]
   ):
       return
   try:
       operations, messages = main(payload)
   except Exception as e:
       messages = [
           create_message(
               "error",
               "Unexpected Error in MyCustomFunction: " + str(e) + "<br>" + format_exc().replace("\n", "<br>"),
           )
       ]

       return {"operations": [], "messages": messages}

   return {"operations": operations, "messages": messages}
```

**Error handling**: Always validate the extracted data before processing to ensure it's in the expected format. Check for Missing Keys: Implement checks or use methods like dict.get() to handle cases where expected data might be missing.

Example:

```py
try:
    total_amount = document["annotations"][0].get("value", "0.00")
except IndexError:
    total_amount = 0.0
```

## 7\. Use Rossum API

Working with the Rossum API is covered in the [documentation](https://elis.develop.r8.lol/api/docs/#getting-started).

**Using Payload Values for API Calls**:

With `base_url` and `auth_token` from the payload, you can construct authenticated API requests to Rossum without embedding sensitive information in your code.

Example:

```py
import requests

if token := payload.get('rossum_authorization_token'):
     headers = {"Authorization": f"Bearer {token}"}
     document_id = payload["document"]["id"]
     url = f"{payload.get('base_url')}/api/v1/documents/{document_id}"
     if (response := requests.get(url, headers=headers)).status_code == 200:
         document_data = response.json()
         print(document_data)
```

When working in serverless environments, understanding and utilizing the appropriate concurrency model can significantly impact performance. However, complexity can introduce risks, so it's crucial to choose the simplest model that meets your needs.

**Prefer single-threaded execution**: Single-threaded code is the simplest to implement and reduces the risk of concurrency-related errors. For many serverless functions, especially those that are not performance-critical, single-threaded execution is sufficient and more maintainable.

**Benefits**:

- Easier to write and understand.
- No concurrency issues like race conditions or deadlocks.
- Simpler debugging and testing.

**Use asynchronous programming ONLY when necessary**: If your function is I/O-bound and you need to improve performance to avoid timeouts, consider using asynchronous programming. It can help make your code faster without the complexities of threading.

**Use-cases for asynchronous calls**:

- **Datapoint Updates**: When you need to update several data points independently, async allows these operations to run in parallel.
- **Fetching Multiple Pages**: If you need to retrieve multiple pages of data from an API, asynchronous calls can fetch them simultaneously, reducing total execution time.

Example with `asyncio`:

```py
import sys
import asyncio
import httpx

def rossum_hook_request_handler(payload: dict) -> dict:
    messages = []
    operations = []
    try:
        urls = [payload["document"]["url"], payload["annotation"]["url"]]
        pages = asyncio.run(main_async(payload, urls))
        for page_content in pages:
            print(f"Fetched page content: {page_content}")

    except Exception as e:
        print(f"Raised exception: {e}")
        messages = [create_message("error", f"MyFunction ended with an exception: {e}")]


    return {"messages": messages, "operations": operations}



async def main_async(payload: dict, urls: list[str]):
    base_url = payload.get("base_url", "https://api.elis.rossum.ai")
    auth_token = payload.get("rossum_authorization_token")
    headers = {"Authorization": f"Bearer {auth_token}"}
    async with httpx.AsyncClient() as client:
        tasks = [fetch_page(client, url, headers) for url in urls]
        pages = await asyncio.gather(*tasks)
        return pages  # You can process pages here if needed


async def fetch_page(client: httpx.AsyncClient, url: str, headers: dict) -> dict:
    response = await client.get(url, headers=headers)
    response.raise_for_status()
    return response.json()
```

**Avoid threading and multiprocessing**: In most cases, threading and multiprocessing introduce unnecessary complexity and potential errors in a serverless environment. They can lead to issues with shared resources and are generally not recommended unless absolutely necessary.

**Reasons to avoid**:

- Increased complexity in code.
- Potential for concurrency bugs.
- Limited benefits in serverless architectures due to resource constraints.

**Note**: This handbook provides a set of advice and guidelines but does not cover every aspect of software development. Always consider the specific requirements of your project and consult official documentation or contact person when necessary.
---
title: 'Rossum Formulas'
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

This section covers both the Rossum **Formula Fields** and the **Serverless Functions** (Formula Fields flavor in custom extensions).

## Installation

Formula Fields or Serverless Functions do not require any installation. They are both available as a native Rossum functionality.

Formula Fields are available in the queue schema as a `formula` field type. In serverless functions, you can import `TxScript` library which is a wrapper introducing the same functionality into serverless functions. Both flavors are fundamentally similar and differ only in how they are used with minimal syntax differences.

In case you want to use the TxScript within pre-existing serverless functions, you need to enable following setting:

1. Go to the settings of the Webhook (Serverless function)
2. Scroll to `Additional notification metadata`
3. Enable the `Schemas` option
4. Save it, and now you can work with the `TxScript` in your serverless function

## Basic usage

To start with Formula Fields, follow these steps:

1. Create a new `order_id_normalized` field in the queue schema.
1. Select the `formula` field type of the field.
1. Write the formula in the `formula` field.

Your first formula can be as simple as (no returns, no imports):

```py
field.order_id
```

This formula copies the `order_id` field into your newly created Formula Field.

Alternatively, you can create a new serverless (Python) function with the following boilerplate code that does the same thing:

```py
from txscript import TxScript

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    t.field.order_id_normalized = t.field.order_id  # Copies `order_id` value into `order_id_normalized`

    return t.hook_response()
```

Notice that it is a little bit more verbose, but it is still very similar. The main differences are that we need to wrap the functionality into `rossum_hook_request_handler` function and that we need to explicitly write into the `order_id_normalized` field.

This is an illustrative example. In case you only need to modify an existing field value, always prefer making it a formula field.

For backward compatibility, you can also use the following import which works the same way as `TxScript`:

```py
from rossum_python import RossumPython

# …
```

## Available functions and features

Here is a list of available functions and features and their comparison between [Formula Fields](./formula-fields.md) and [Serverless Functions](./serverless-functions.md). Note that serverless functions examples always assume that the code is wrapped in `rossum_hook_request_handler` function like so:

```py
from txscript import TxScript

def rossum_hook_request_handler(payload):
    t = TxScript.from_payload(payload)

    # INSERT THE EXAMPLES HERE

    return t.hook_response()
```

Formula field examples do not require any further modification.

### Working with datapoints

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

Getting values (or meta information):

```py
field.amount                            # Get datapoint value
field.amount.id                         # Get datapoint system ID
field.amount.attr.rir_confidence        # Get confidence score
field.amount.attr.ocr_raw_text          # Get raw value extracted by OCR, if applicable
field.amount.attr.rir_raw_text          # Get raw extracted text by RIR
```

:::warning

Formula fields cannot write into any other fields. They simply return the value into the formula field itself.

:::

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

Getting values (or meta information):

```py
t.field.amount                          # Get datapoint value
t.field.amount.id                       # Get datapoint system ID
t.field.amount.rir_confidence           # Get confidence score
t.field.amount.attr.ocr_raw_text        # Get raw value extracted by OCR, if applicable
t.field.amount.attr.rir_raw_text        # Get raw extracted text by RIR
```

Writing values:

```py
t.field.amount = 10
```

  </TabItem>
</Tabs>

### Working with annotation data

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

```py
annotation.id                                       # Get annotation ID
annotation.metadata                                 # Get annotation metadata
annotation.document.original_file_name              # Get document original file name
annotation.email.subject                            # Get email subject
```

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
t.annotation.id                                     # Get annotation ID
t.annotation.metadata                               # Get annotation metadata
t.annotation.document.original_file_name            # Get document original file name
t.annotation.email.subject                          # Get email subject
```

  </TabItem>
</Tabs>

### Check whether datapoint is set or not

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

```py
is_set(field.amount)        # Returns `true` if datapoint is set (has value)
is_empty(field.amount)      # Opposite of `is_set`
```

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
from txscript import TxScript, is_set, is_empty

# …

is_set(t.field.amount)      # Returns `true` if datapoint is set (has value)
is_empty(t.field.amount)    # Opposite of `is_set`
```

  </TabItem>
</Tabs>

### Defaulting values

Use the default value if the field is empty.

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

```py
default_to(field.amount, 0)
```

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
from txscript import TxScript, default_to

# …

default_to(t.field.amount, 0)
```

  </TabItem>
</Tabs>

### Substitute

Substitute is an alias for [`re.sub`](https://docs.python.org/3/library/re.html#re.sub) function (for convenience).

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

```py
substitute(r"[^0-9]", r"", field.document_id)  # Remove non-digit characters
```

Could also be written as (`re` is imported automatically):

```py
re.sub(r"[^0-9]", r"", field.document_id)
```

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
from txscript import TxScript, substitute

# …

substitute(r"[^0-9]", r"", t.field.document_id)
```

  </TabItem>
</Tabs>

### Show info/warning/error messages

:::note

Messages do not affect the automation behavior and, therefore, automation blockers must be set explicitly (see how to set [automation blockers](#set-automation-blockers)). The only exception is `show_error` which always blocks the automation.

:::

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

```py
show_info("…")                          # Show global info message
show_info("…", field.amount)            # Show info message on the specified field

show_warning("…")                       # Show global warning message
show_warning("…", field.amount)         # Show warning message on the specified field

show_error("…")                         # Show global error message
show_error("…", field.amount)           # Show error message on the specified field
```

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
t.show_info("…")                        # Show global info message
t.show_info("…", t.field.amount)        # Show info message on the specified field

t.show_warning("…")                     # Show global warning message
t.show_warning("…", t.field.amount)     # Show warning message on the specified field

t.show_error("…")                       # Show global error message
t.show_error("…", t.field.amount)       # Show error message on the specified field
```

  </TabItem>
</Tabs>

### Set automation blockers

:::note

Automation blockers must be set independently of the info/warning messages. Error messages block the automation by default (cannot be disabled).

:::

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

```py
automation_blocker("message", field.amount)
```

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
t.automation_blocker("message", t.field.amount)
```

  </TabItem>
</Tabs>

### Create multivalue field values

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

:::warning

Multivalue formula fields are currently not supported (only serverless functions).

:::

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
t.field.multivalue_field.all_values = ["AAA", "BBB"]
```

  </TabItem>
</Tabs>

### Create enum field options

<Tabs groupId="rossum-formulas-flavor">
  <TabItem value="formula_field" label="Formula field" default>

:::warning

Changing enum field options is currently not supported in formula fields (only serverless functions).

:::

  </TabItem>
  <TabItem value="serverless_function" label="Serverless function">

```py
t.field.enum_field.attr.options = [{"label":"AAA", "value":"aaa"}, {"label":"BBB", "value":"bbb"}]
t.field.enum_field = "bbb"
```

  </TabItem>
</Tabs>
:::warning[Work in progress]

We're still working on this part and would love to hear your thoughts! Feel free to [share your feedback](https://github.com/rossumai/university/discussions) or [submit a pull request](https://github.com/rossumai/university/pulls). Thank you! 🙏

{props.issue ? <p>Tracking issue: <a href={props.issue}>{props.issue}</a></p> : ''}

:::
---
title: 'Automation'
sidebar_position: 1
---

import DocCardList from '@theme/DocCardList';

<DocCardList />
---
title: 'Automation: Automation unblocker'
sidebar_label: 'Automation unblocker'
sidebar_position: 1
---

# Automation unblocker

Unblocks specified datapoints when conditions are met.

## Source code

```py
from txscript import TxScript
from typing import Callable, Dict, Iterator, List
from pydantic import BaseModel


class FieldConfig(BaseModel):
    check_field: str
    fields_to_automate: List[str]


class Settings(BaseModel):
    value_existence: List[FieldConfig] = []
    single_option: List[FieldConfig] = []


def rossum_hook_request_handler(payload: dict) -> Dict[str, list]:
    settings = Settings.parse_obj(payload["settings"])
    content = payload["annotation"]["content"]

    operations = []

    for config in settings.value_existence:
        automation_ops = automate(
            config,
            content,
            lambda dp: dp["content"]["value"] != "",
        )
        operations.extend(automation_ops)

    for config in settings.single_option:
        automation_ops = automate(
            config,
            content,
            lambda dp: len(dp.get("options",[])) == 1 and dp["content"]["value"] != "",
        )
        operations.extend(automation_ops)

    return {"operations": operations}


def automate(
    config: FieldConfig,
    content: List[dict],
    condition: Callable[[dict], bool],
) -> List[dict]:
    operations = []
    for datapoint in find_all_by_schema_id(content, config.check_field):
        if condition(datapoint):
            for schema_id in config.fields_to_automate:
                for automated_dp in find_all_by_schema_id(content, schema_id):
                    operations.append(create_automation_operation(automated_dp))
    return operations



def create_automation_operation(
    datapoint: dict,
    automation_type: str = "connector",
) -> dict:
    """
    Enable automation of a specific field by updating its validation sources.
    :param datapoint: content of the datapoint
    :param automation_type: type of the automation validation source to be used
    :return: dict with replace operation definition (see https://api.elis.rossum.ai/docs/#annotation-content-event-response-format)
    """
    validation_sources = datapoint["validation_sources"]

    if automation_type not in validation_sources:
        validation_sources.append(automation_type)

    return {
        "op": "replace",
        "id": datapoint["id"],
        "value": {"validation_sources": validation_sources},
    }


def find_all_by_schema_id(content: List[dict], schema_id: str) -> Iterator[dict]:
    for node in content:
        if node["schema_id"] == schema_id:
            yield node
        elif children := node.get("children"):
            yield from find_all_by_schema_id(children, schema_id)
```

## Example configuration

```json
{
  "single_option": [
    {
      "check_field": "order_header_match",
      "fields_to_automate": ["order_id"]
    }
  ]
}
```
---
title: 'User management: Single Sign-On (SSO)'
sidebar_label: 'Single Sign-On (SSO)'
sidebar_position: 1
---

import PaidFeature from '../\_paid_feature.md';
import RossumInternalOnly from '../\_rossum_internal_only.md';

# Single Sign-On (SSO)

<PaidFeature />

## Creating users with SSO

When the SSO is configured, all newly created users are created with SSO enabled by default. Follow these steps:

1. Go to the user's profile and click on **Settings**.
1. Click **Add user**
1. Fill in all the required users and click **Add user** button

It might be necessary is to update the **External user ID** so that it matches the 3rd party system. However, by default, we use the same email address and the user might be able to log in without any changes needed.

## Changing existing users to SSO

When SSO is enabled on the organization, older users that were still using password-based login are not switched automatically. To change an existing user to SSO, you need to:

1. Contact Rossum.ai support to enable SSO for your account.
1. Go to the user's profile and click on **Settings**.
1. Find the relevant user and click on it.
1. In the user's profile, click on **Personal info**.
1. In the **Authentication** section, select **Log in with SSO**.

Note that if you do not see this section, it means that SSO is not enabled for your account.

![SSO settings](./_img/sso-settings.png)

The **External user ID** can be used to connect the user to your system (Azure, Google). It is a unique identifier that is used to match the user in your system with the user in Rossum.ai. In most cases, this ID is the same as user email.

For more technical information about SSO please visit our API reference: https://elis.rossum.ai/api/docs/#single-sign-on-sso

## Changing existing users back to password

Users with enabled SSO can be switched back to password based login following these steps:

1. Go to the user's profile and click on **Settings**.
1. Find the relevant user and click on it.
1. In the user's profile, click on **Personal info**.
1. In the **Authentication** section, select **Log in with Password**.

After saving the changes it is necessary to click on **Reset password** which will send an email with password reset link. This is necessary even when the user previously had a password but was switched to SSO! It is because when switching to SSO, we purge the passwords for security reasons.

## Using SSO on both sandbox and production

<RossumInternalOnly url="https://rossumai.atlassian.net/wiki/x/AQA6Uw" />

It is sometimes necessary to grant SSO access to the same user to both sandbox and production. Rossum, however, doesn't allow creating the same in multiple organizations (on the same cluster). Here is our recommended approach to work around this limitation:

1. Create unique users in both sandbox and production organizations. It is recommended to use their actual email addresses in production environment and add "+sandbox" suffix in sandbox environment (for example, test@example.com and test+sandbox@example.com).
1. In user settings, unify the "External user ID" values (claims). This value should be the same as the email used in the identity provider (so in sandbox remove the "+sandbox" suffix).
1. Ask your assigned account manager to create two "SSO providers" with the same frontend domain. This way, you'll be able to log in from one URL to both environments. Consider auto-provisioning in production.
1. Use this one domain to log in to sandbox or production via SSO. You can also log in via password for "break-glass" accounts.

Here is an example of how the login screen will look like:

![Multiple SSO providers](./_img/multiple-sso-providers.png)
---
title: 'User management: Service accounts'
sidebar_label: 'Service accounts'
sidebar_position: 2
---

# Service accounts

Learn how to create service account for integrations.

## 1. Login with an admin account using username/password

```bash
curl https://mycompany.rossum.app/api/v1/auth/login \
--header 'Content-Type: application/json' \
--data '{
    "username": "myusername@company.com",
    "password": "thisismypassword"
}'
```

After successful login you will receive a bearer token in the key attribute of the response:

```json
{
  "key": "ltcg2p2w7o9vxju313f04rq7lcc4xu2bwso423b3",
  "domain": null
}
```

The `key` value will be referred as `<token>` in the rest of the article.

## 2. Get your organization ID

```bash
curl -H 'Authorization: Bearer <token> -X GET 'https://mycompany.rossum.app/api/v1/organizations'
```

Note the `id` of an organization you want to create the user in:

```json
{
  "pagination": {
    "total": 1,
    "total_pages": 1,
    "next": null,
    "previous": null
  },
  "results": [
    {
      "id": 1234,
      // highlight-start
      "url": "https://mycompany.rossum.app/api/v1/organizations/1234"
      // highlight-end
      // …
    }
  ]
}
```

## 3. Use bearer token to create service account

Technical users must be created via API call because that’s the only way to set user's password explicitly:

```bash
curl 'https://mycompany.rossum.app/api/v1/users' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer <token>' \
--data '{
    "first_name": "SYSTEM USER",
    "last_name": "(DO NOT DELETE)",
    "username": "api_user@mycompany",
    "password": "myserviceaccoutpassword",
    "organization": "https://mycompany.rossum.app/api/v1/organizations/1234",
    "groups": [
        "https://mycompany.rossum.app/api/v1/groups/3"
    ]
}'
```

### 3.1 Exchange one time for long lasting reusable token

In the integration application login with the newly created service account and immediately exchange it for long-lasting reusable token. Use this long-lasting token until it expires. When that happens, login again with the first command and exchange for long-lasting token. Note that `/auth/login` endpoint is throttled and too many login attempts will cause the login endpoint to stop generating new tokens.

```bash
curl https://mycompany.rossum.app/api/v1/auth/login \
--header 'Content-Type: application/json' \
--data '{
    "username": "api_user@mycompany",
    "password": "myserviceaccoutpassword"
}'
```

Token exchange:

```bash
curl -X POST -H 'Authorization: Bearer <token> 'https://mycompany.rossum.app/api/v1/auth/token'
```
---
title: 'NetSuite: Postman collection'
sidebar_position: 6
sidebar_label: 'Postman collection'
---

import WIP from '../\_wip.md';

# Postman collection

It might be useful to call NetSuite using [Postman](https://www.postman.com/) for testing purposes. Follow the guide on this page to be able to send requests directly to NetSuite.

## SOAP requests

First, create a new Postman collection and use the following "[Pre-req](https://learning.postman.com/docs/tests-and-scripts/write-scripts/pre-request-scripts/)" script:

```js
const account = pm.environment.get('account');
const consumerKey = pm.environment.get('consumer_key');
const consumerSecret = pm.environment.get('consumer_secret');
const tokenKey = pm.environment.get('token_key');
const tokenSecret = pm.environment.get('token_secret');

const timestamp = Math.floor(Date.now() / 1000).toString();
const nonce = CryptoJS.lib.WordArray.random(10).toString();
const baseString = `${account}&${consumerKey}&${tokenKey}&${nonce}&${timestamp}`;
const key = `${consumerSecret}&${tokenSecret}`;
const signature = CryptoJS.HmacSHA256(baseString, key).toString(CryptoJS.enc.Base64);

pm.environment.set('signature', signature);
pm.environment.set('nonce', nonce);
pm.environment.set('timestamp', timestamp);
```

This script is necessary for signing all outgoing SOAP requests. It also implies that the following environment variables must be set:

- `account`
- `consumer_key`
- `consumer_secret`
- `token_key`
- `token_secret`

It will in return create the following environment variables (to be used later in our request):

- `signature`
- `nonce`
- `timestamp`

Next, create a new `POST` request in the created collection. Apart from the default headers, the request should have this NetSuite header (value depends on the request action):

```text
SOAPAction: upsert
```

Depending on your NetSuite version and account number, the `POST` request URL can look like this:

```text
https://123-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2023_2
```

And finally, the SOAP request body (raw XML). In this case to upsert a Vendor Bill record in NetSuite (notice the environment variables from above):

```xml
<soapenv:Envelope xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <soapenv:Header>
        <tokenPassport>
            <account>{{account}}</account>
            <consumerKey>{{consumer_key}}</consumerKey>
            <token>{{token_key}}</token>
            <nonce>{{nonce}}</nonce>
            <timestamp>{{timestamp}}</timestamp>
            <signature algorithm="HMAC-SHA256">{{signature}}</signature>
        </tokenPassport>
        <platformMsg:preferences soapenv:mustUnderstand="0" soapenv:actor="http://schemas.xmlsoap.org/soap/actor/next" xmlns:platformMsg="urn:messages_2023_2.platform.webservices.netsuite.com">
            <platformMsg:useConditionalDefaultsOnAdd>true</platformMsg:useConditionalDefaultsOnAdd>
            <platformMsg:ignoreReadOnlyFields>true</platformMsg:ignoreReadOnlyFields>
            <platformMsg:warningAsError>false</platformMsg:warningAsError>
            <platformMsg:runServerSuiteScriptAndTriggerWorkflows>true</platformMsg:runServerSuiteScriptAndTriggerWorkflows>
        </platformMsg:preferences>
    </soapenv:Header>
    <soapenv:Body>
        <ns0:upsert xmlns:ns0="urn:messages_2023_2.platform.webservices.netsuite.com">
            <ns0:record xsi:type="ns169:VendorBill" externalId="__CHANGE_ME__" xmlns:ns169="urn:purchases_2023_2.transactions.webservices.netsuite.com" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
                <ns1:customForm type="customRecord" internalId="__CHANGE_ME__" xmlns:ns1="urn:purchases_2023_2.transactions.webservices.netsuite.com"/>
                <ns2:entity type="vendor" internalId="__CHANGE_ME__" xmlns:ns2="urn:purchases_2023_2.transactions.webservices.netsuite.com"/>
                <ns3:subsidiary type="subsidiary" internalId="__CHANGE_ME__" xmlns:ns3="urn:purchases_2023_2.transactions.webservices.netsuite.com"/>
                <ns4:approvalStatus internalId="2" xmlns:ns4="urn:purchases_2023_2.transactions.webservices.netsuite.com"/>
                <ns5:tranDate xmlns:ns5="urn:purchases_2023_2.transactions.webservices.netsuite.com">2024-12-24T00:00:00</ns5:tranDate>
                <ns6:tranId xmlns:ns6="urn:purchases_2023_2.transactions.webservices.netsuite.com">__CHANGE_ME__</ns6:tranId>
                <ns7:currency type="currency" internalId="1" xmlns:ns7="urn:purchases_2023_2.transactions.webservices.netsuite.com"/>
                <ns8:class type="classification" internalId="__CHANGE_ME__" xmlns:ns8="urn:purchases_2023_2.transactions.webservices.netsuite.com"/>
                <ns9:department type="department" internalId="__CHANGE_ME__" xmlns:ns9="urn:purchases_2023_2.transactions.webservices.netsuite.com"/>
                <ns10:itemList replaceAll="true" xmlns:ns10="urn:purchases_2023_2.transactions.webservices.netsuite.com">
                    <ns10:item>
                        <ns10:item type="inventoryItem" internalId="__CHANGE_ME__"/>
                        <ns10:taxCode type="taxType" internalId="__CHANGE_ME__"/>
                        <ns10:quantity>1</ns10:quantity>
                        <ns10:rate>__CHANGE_ME__</ns10:rate>
                        <ns10:amount>__CHANGE_ME__</ns10:amount>
                        <ns10:grossAmt>__CHANGE_ME__</ns10:grossAmt>
                        <ns10:tax1Amt>__CHANGE_ME__</ns10:tax1Amt>
                        <ns10:description>__CHANGE_ME__</ns10:description>
                        <ns10:department type="department" internalId="__CHANGE_ME__"/>
                        <ns10:class type="classification" internalId="__CHANGE_ME__"/>
                        <ns10:location type="location" internalId="__CHANGE_ME__"/>
                    </ns10:item>
                </ns10:itemList>
            </ns0:record>
        </ns0:upsert>
    </soapenv:Body>
</soapenv:Envelope>
```

Similar SOAP payloads can be found in NetSuite [SOAP Web Services Usage Logs](https://system.netsuite.com/app/webservices/syncstatus.nl).

## REST API requests

<WIP />
---
title: 'NetSuite: Data matching'
sidebar_position: 4
sidebar_label: 'Data matching'
---

import WIP from '../\_wip.md';

TODO: NetSuite-specific data matching configurations

## PO-backed invoices

### Entity matching

Why: needed for VB creation

```json
{
  "entity": {
    "type": "vendor",
    "_ns_type": "RecordRef",
    "internalId": "@{entity_id}"
  }
}
```

---

```json
{
  "name": "PO-backed Entity ID",
  "source": {
    "dataset": "NS_SB2_PurchaseOrder_v1",
    "queries": [
      {
        "find": {
          "tranId": "{order_id_export}"
        }
      }
    ]
  },
  "default": {
    "label": "---",
    "value": ""
  },
  "mapping": {
    "dataset_key": "entity.internalId",
    "target_schema_id": "entity_id"
  },
  "queue_ids": [1418217, 1626091],
  "result_actions": {
    "no_match_found": {
      "message": {
        "type": "error",
        "content": "No match found"
      }
    },
    "one_match_found": {
      "select": "best_match"
    },
    "multiple_matches_found": {
      "select": "best_match",
      "message": {
        "type": "warning",
        "content": "Multiple matches found"
      }
    }
  }
}
```
---
title: 'NetSuite: Import configuration'
sidebar_position: 2
sidebar_label: 'Import configuration'
toc_max_heading_level: 4
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Import configuration

Notice that each configuration has `concurrency_limit` configured. The best way how to determine the right number is to visit **Setup → Integration → Integration Governance** where you can see (and configure) not only the concurrency limits but also peak concurrency of all integrations allowing you to choose the best number.

:::tip

Visit the following link when trying to figure out how should the import searches be configured (drill down to the required fields): https://system.netsuite.com/help/helpcenter/en_US/srbrowser/Browser2022_2/schema/search/transactionsearch.html?mode=package

:::

## Differential data imports (daily)

Recommended schedule: `0 22 * * *`

```json
{
  "run_async": true,
  "netsuite_settings": {
    "account": "XXX_SB1", // Case sensitive!
    "concurrency_limit": 4,
    "wsdl_url": "https://XXX-sb1.suitetalk.api.netsuite.com/wsdl/v2024_1_0/netsuite.wsdl",
    "service_url": "https://XXX-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2024_1",
    "service_binding_name": "{urn:platform_2024_1.webservices.netsuite.com}NetSuiteBinding"
  },
  "import_configs": [
    {
      // Currencies
      "master_data_name": "NS_Currency_v1",
      "async_settings": {
        "retries": 5,
        "max_run_time_s": 36000
      },
      "payload": {
        "method_name": "getAll",
        "method_args": [
          {
            "_ns_type": "GetAllRecord",
            "recordType": "currency"
          }
        ]
      }
    }

    // … see below for all import config examples
  ]
}
```

Consult the other configurations below for more real-world examples.

### Accounts

:::info

Requires 'Lists -> Accounts' permissions.

:::

```json
{
  "master_data_name": "NS_Account_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "AccountSearchBasic"
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Currencies

:::info

Requires 'Lists -> Currency' permissions.

:::

```json
{
  "master_data_name": "NS_Currency_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "getAll",
    "method_args": [
      {
        "_ns_type": "GetAllRecord",
        "recordType": "currency"
      }
    ]
  }
}
```

### Customers

:::info

Requires 'Lists -> Customers' permissions.

:::

```json
{
  "master_data_name": "NS_Customer_v1",
  "payload": {
    "method_args": [
      {
        "_ns_type": "CustomerSearchBasic",
        "isInactive": {
          "searchValue": false
        }
      }
    ],
    "method_name": "search",
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  },
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  }
}
```

### Departments

```json
{
  "master_data_name": "NS_Department_v1",
  "payload": {
    "method_args": [
      {
        "_ns_type": "DepartmentSearchBasic",
        "isInactive": {
          "searchValue": "false"
        }
      }
    ],
    "method_name": "search",
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  },
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  }
}
```

### Items

#### Inventory items

```json
{
  "master_data_name": "NS_InventoryItem_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "ItemSearchBasic",
        "type": {
          "searchValue": "_inventoryItem",
          "operator": "anyOf"
        },
        "isInactive": {
          "searchValue": false
        },
        "lastModifiedDate": {
          "operator": "onOrAfter",
          "searchValue": "{last_modified_date}"
        }
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

#### Non-inventory items

```json
{
  "master_data_name": "NS_NonInventoryItem_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "ItemSearchBasic",
        "type": {
          "searchValue": "_nonInventoryItem",
          "operator": "anyOf"
        },
        "isInactive": {
          "searchValue": false
        },
        "lastModifiedDate": {
          "operator": "onOrAfter",
          "searchValue": "{last_modified_date}"
        }
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

#### All items (inventory and non-inventory)

```json
{
  "master_data_name": "NS_Inventory_NonInventoryItem_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_args": [
      {
        "type": {
          "operator": "anyOf",
          "searchValue": ["_inventoryItem", "_nonInventoryItem"]
        },
        "_ns_type": "ItemSearchBasic",
        "isInactive": {
          "searchValue": false
        },
        "lastModifiedDate": {
          "operator": "onOrAfter",
          "searchValue": "{last_modified_date}"
        }
      }
    ],
    "method_name": "search",
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Item Receipts (GRNs)

```json
{
  "master_data_name": "NS_ItemReceipt_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "TransactionSearchBasic",
        "type": {
          "operator": "anyOf",
          "searchValue": "_itemReceipt"
        },
        "lastModifiedDate": {
          "operator": "onOrAfter",
          "searchValue": "{last_modified_date}"
        }
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Locations

```json
{
  "master_data_name": "NS_Location_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "LocationSearchBasic"
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Nexuses

```json
{
  "master_data_name": "NS_Nexus_v1",
  "payload": {
    "method_args": [
      {
        "_ns_type": "NexusSearchBasic"
      }
    ],
    "method_name": "search",
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  },
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  }
}
```

### Purchase Orders

:::info

Requires 'Transactions → Purchase Order (View)' permissions.

:::

```json
{
  "master_data_name": "NS_PurchaseOrder_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "TransactionSearchBasic",
        "type": {
          "searchValue": "_purchaseOrder",
          "operator": "anyOf"
        },
        "lastModifiedDate": {
          "operator": "onOrAfter",
          "searchValue": "{last_modified_date}"
        }
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Sales tax items

```json
{
  "master_data_name": "NS_SalesTaxItem_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "SalesTaxItemSearchBasic"
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Subsidiaries

:::info

Requires 'Lists -> Subsidiaries' permissions.

:::

```json
{
  "master_data_name": "NS_Subsidiary_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "SubsidiarySearchBasic",
        "isInactive": {
          "searchValue": "false"
        }
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Vendors

:::info

Requires 'Lists -> Vendors' permissions.

:::

```json
{
  "master_data_name": "NS_Vendor_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "VendorSearchBasic",
        "isInactive": {
          "searchValue": "false"
        },
        "lastModifiedDate": {
          "operator": "onOrAfter",
          "searchValue": "{last_modified_date}"
        }
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

### Vendor-Subsidiary Relationship

:::warning

Note that when working with this import, you might get the following error:

```text
UNEXPECTED_ERROR: An unexpected error occurred. Error ID: m3hhy7qy186i564uw6ob4
```

To solve it, try removing `method_headers.searchPreferences` from your request. This is a solution recommended by NetSuite support. There is however no good explanation of why this happens.

:::

```json
{
  "master_data_name": "NS_VendorSubsidiaryRelationship_v1",
  "payload": {
    "method_args": [
      {
        "_ns_type": "VendorSubsidiaryRelationshipSearchBasic"
      }
    ],
    "method_name": "search"
  },
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  }
}
```

### Vendor Bills (Invoices)

```json
{
  "master_data_name": "NS_VendorBill_v1",
  "async_settings": {
    "retries": 5,
    "max_run_time_s": 36000
  },
  "payload": {
    "method_name": "search",
    "method_args": [
      {
        "_ns_type": "TransactionSearchBasic",
        "type": {
          "searchValue": "_vendorBill",
          "operator": "anyOf"
        },
        "lastModifiedDate": {
          "operator": "onOrAfter",
          "searchValue": "{last_modified_date}"
        }
      }
    ],
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": false
      }
    }
  }
}
```

## Async settings

Usually, all imports (as well as exports) will run in asynchronous mode, see:

```json
{
  "run_async": true
}
```

If you'd like to modify the async settings, you can do so using the following `async_settings` configuration:

```json
{
  "run_async": true,
  "import_configs": [
    {
      "master_data_name": "NS_PurchaseOrder_v1",
      // highlight-start
      "async_settings": {
        "retries": 2, // max: 10
        "max_run_time_s": 7200 // 2 hours default, min: 60 (1 minute), max: 36000 (10 hours)
        "valid_for_s": 43200 // 12 hours default, min: 300 (5 minutes), max: 172800 (2 days)
      },
      // highlight-end
      "payload": {
        "method_name": "search",
        "method_args": [
          {
            "_ns_type": "TransactionSearchBasic",
            "type": {
              "searchValue": "_purchaseOrder",
              "operator": "anyOf"
            }
          }
        ]
      }
    }
  ]
}
```

Note that this configuration must be applied to all relevant import configs. Each config can even have a different timeouts and retries.

## Importing individual records

Sometimes, it can be handy to import just one specific record:

```json
{
  "run_async": true,
  "import_configs": [
    {
      "payload": {
        "method_name": "search",
        "method_args": [
          {
            "type": {
              "operator": "anyOf",
              "searchValue": "_purchaseOrder"
            },
            "tranId": {
              "operator": "is",
              "searchValue": "PO-45512"
            },
            "_ns_type": "TransactionSearchBasic"
          }
        ],
        "method_headers": {
          "searchPreferences": {
            "bodyFieldsOnly": false,
            "returnSearchColumns": false
          }
        }
      },
      "async_settings": {
        "retries": 5
      },
      "master_data_name": "NS_PurchaseOrder_v1"
    }
  ],
  "netsuite_settings": {
    // …
  }
}
```

## Using advanced transaction search

Advanced search can be beneficial if we want to select which fields should be fetched from NetSuite (to lower the payload size as well as data storage requirements). Additionally, it can be useful to fetch additional columns such as `createdFromJoin` or `applyingTransactionJoin` and similar.

### `TransactionSearchAdvanced`

:::info

Advanced transaction search requires 'Transactions → Find Transaction' permission.

:::

`TransactionSearchAdvanced` requires two main fields: `criteria` (to specify the actual search) and `columns` (to specify what columns should be returned). It is also important to set `returnSearchColumns` to `true` and finally `advanced_search_internal_id_jmespath` to define unique identifier for the basic record:

```json
{
  "run_async": false,
  "import_configs": [
    {
      "payload": {
        "method_name": "search",
        "method_args": [
          {
            "_ns_type": "TransactionSearchAdvanced",
            // highlight-start
            "criteria": {
              // highlight-end
              "_ns_type": "TransactionSearch",
              "basic": {
                "_ns_type": "TransactionSearchBasic",
                "type": {
                  "operator": "anyOf",
                  "searchValue": "_purchaseOrder"
                },
                "dateCreated": {
                  "operator": "onOrAfter",
                  "searchValue": "2024-01-01T00:00:00Z"
                }
              }
            },
            // highlight-start
            "columns": {
              // highlight-end
              "_ns_type": "TransactionSearchRow",
              "basic": {
                "_ns_type": "TransactionSearchRowBasic",
                "tranId": {},
                "tranDate": {},
                "internalId": {},
                "externalId": {},
                "recordType": {},
                "dateCreated": {},
                "lastModifiedDate": {}
              },
              "applyingTransactionJoin": {
                "_ns_type": "TransactionSearchRowBasic",
                "tranId": {},
                "tranDate": {},
                "internalId": {},
                "externalId": {},
                "recordType": {},
                "dateCreated": {},
                "lastModifiedDate": {}
              }
            }
          }
        ],
        "method_headers": {
          "searchPreferences": {
            "pageSize": 100,
            "bodyFieldsOnly": false,
            // highlight-start
            "returnSearchColumns": true
            // highlight-end
          }
        }
      },
      "master_data_name": "NS_PurchaseOrder_v1",
      // highlight-start
      "advanced_search_internal_id_jmespath": "basic.internalId[0].searchValue.internalId"
      // highlight-end
    }
  ],
  "netsuite_settings": {
    // …
  }
}
```

### `CustomRecordSearchAdvanced`

```json
{
  "payload": {
    "method_args": [
      {
        "columns": {
          "basic": {
            "created": {},
            "recType": {},
            "externalId": {},
            "internalId": {},
            "isInactive": {},
            "lastModified": {},
            "customFieldList": {
              "_ns_type": "SearchColumnCustomFieldList",
              "customField": [
                {
                  "_ns_type": "SearchColumnStringCustomField",
                  "scriptId": "custrecord_2663_entity_bic"
                },
                {
                  "_ns_type": "SearchColumnStringCustomField",
                  "scriptId": "custrecord_2663_entity_iban"
                },
                {
                  "_ns_type": "SearchColumnStringCustomField",
                  "scriptId": "custrecord_2663_entity_branch_no"
                },
                {
                  "_ns_type": "SearchColumnStringCustomField",
                  "scriptId": "custrecord_2663_entity_acct_no"
                },
                {
                  "_ns_type": "SearchColumnStringCustomField",
                  "scriptId": "custrecord_2663_entity_acct_suffix"
                }
              ]
            }
          },
          "_ns_type": "CustomRecordSearchRow"
        },
        "_ns_type": "CustomRecordSearchAdvanced",
        "criteria": {
          "basic": {
            "recType": {
              "internalId": 854
            },
            "lastModified": {
              "operator": "onOrAfter",
              "searchValue": "{last_modified_date}"
            },
            "_ns_type": "CustomRecordSearchBasic"
          },
          "_ns_type": "CustomRecordSearch"
        }
      }
    ],
    "method_name": "search",
    "method_headers": {
      "searchPreferences": {
        "pageSize": 100,
        "bodyFieldsOnly": false,
        "returnSearchColumns": true
      }
    }
  },
  "master_data_name": "NS_BankDetail_v1",
  "advanced_search_internal_id_jmespath": "basic.internalId[0].searchValue.internalId"
}
```

### Main line advanced search

By default, the advanced search returns one record for each line item. In case we'd not care about the line items, we can change the search behavior to return one line per main line record by configuring `criteria.basic.mainLine` (see [Main Line in NetSuite](https://docs.oracle.com/en/cloud/saas/netsuite/ns-online-help/section_4459563851.html)):

```json
{
  "_ns_type": "TransactionSearchAdvanced",
  "columns": {
    "_ns_type": "TransactionSearchRow",
    "basic": {
      // …
    }
  },
  "criteria": {
    "_ns_type": "TransactionSearch",
    "basic": {
      "_ns_type": "TransactionSearchBasic",
      // highlight-start
      "mainLine": {
        "searchValue": true
      }
      // highlight-end
      // …
    }
  }
}
```

## Initial full data imports

:::warning

Rossum's team of Solution Architects is typically needed for large NetSuite imports and recoveries. Consider contacting [Rossum Sales](https://rossum.ai/form/contact/) or Rossum Support team if you need help.

:::

When creating a new organization, the Master Data Hub collections are empty and need to be imported from NetSuite. The most naive approach is to simply run the [differential import from above](#differential-data-imports-daily) which will on the first run import everything. This is because when the collections are empty, the `last_modified_date` will default to January 1st, 1970 (effectively resulting in a full import).

However, initial imports are typically very large and can take **several days** when ran sequentially. It's expected that the initial imports can fail during this period. Moreover, the maximum runtime of import jobs is currently **10 hours**. The following section describes how to deal with such failures and how to approach initial imports in a less naive and more controlled way.

:::tip

Consider whether full dataset import is necessary. It might be enough to pull the last year only, for example.

:::

All imported records typically have sorting specified. For example, all transactions are typically sorted by "Date Created", see: https://docs.oracle.com/en/cloud/saas/netsuite/ns-online-help/section_N3518731.html

We can leverage this default sorting and partition the initial imports to years (so that we can download several years in parallel):

```json
{
  "import_configs": [
    {
      // Download Purchase Orders for year 2022:
      "payload": {
        "method_name": "search",
        "method_args": [
          {
            "_ns_type": "TransactionSearchBasic",
            "type": {
              "operator": "anyOf",
              "searchValue": "_purchaseOrder"
            },
            "dateCreated": {
              // highlight-start
              "operator": "within",
              "searchValue": "2022-01-01T00:00:00Z",
              "searchValue2": "2023-01-01T00:00:00Z"
              // highlight-end
            }
          }
        ],
        "method_headers": {
          "searchPreferences": {
            "pageSize": 100,
            "bodyFieldsOnly": false,
            "returnSearchColumns": false
          }
        }
      },
      "master_data_name": "NS_PurchaseOrder_v1"
    },
    {
      // Download Purchase Orders for year 2023:
      "payload": {
        "method_name": "search",
        "method_args": [
          {
            "_ns_type": "TransactionSearchBasic",
            "type": {
              "operator": "anyOf",
              "searchValue": "_purchaseOrder"
            },
            "dateCreated": {
              // highlight-start
              "operator": "within",
              "searchValue": "2023-01-01T00:00:00Z",
              "searchValue2": "2024-01-01T00:00:00Z"
              // highlight-end
            }
          }
        ],
        "method_headers": {
          "searchPreferences": {
            "pageSize": 100,
            "bodyFieldsOnly": false,
            "returnSearchColumns": false
          }
        }
      },
      "master_data_name": "NS_PurchaseOrder_v1"
    },
    {
      // Download Purchase Orders for the rest of the years:
      "payload": {
        "method_name": "search",
        "method_args": [
          {
            "type": {
              "operator": "anyOf",
              "searchValue": "_purchaseOrder"
            },
            "_ns_type": "TransactionSearchBasic",
            "dateCreated": {
              // highlight-start
              "operator": "onOrAfter",
              "searchValue": "2024-01-01T00:00:00Z"
              // highlight-end
            }
          }
        ],
        "method_headers": {
          "searchPreferences": {
            "pageSize": 100,
            "bodyFieldsOnly": false,
            "returnSearchColumns": false
          }
        }
      },
      "master_data_name": "NS_PurchaseOrder_v1"
    }
  ]
}
```

It is necessary to observe whether all the partitions were imported successfully. In case they were not (for example some network issue cause the import jobs to fail), we can adjust the `within` window to ignore the already imported dates and restart the import. To check the latest available date in the partition, you can use the following MongoDB query:

<Tabs>
  <TabItem value="basic" label="Basic search" default>

:::warning

Confusingly, NetSuite returns `createdDate` field but the SOAP API exposes `dateCreated` search argument instead!

:::

```json
{
  "aggregate": [
    {
      "$match": {
        "createdDate": {
          "$gte": { "$date": "2024-01-01T00:00:00Z" },
          "$lte": { "$date": "2025-01-01T00:00:00Z" }
        }
      }
    },
    { "$sort": { "createdDate": -1 } },
    { "$limit": 3 },
    { "$project": { "createdDate": 1, "itemList": 1 } }
  ]
}
```

  </TabItem>
  <TabItem value="advanced" label="Advanced search" default>

```json
{
  "aggregate": [
    {
      "$match": {
        "basic.dateCreated.searchValue": {
          "$gte": { "$date": "2024-01-01T00:00:00Z" },
          "$lte": { "$date": "2025-01-01T00:00:00Z" }
        }
      }
    },
    { "$sort": { "basic.dateCreated.searchValue": -1 } },
    { "$limit": 3 },
    { "$project": { "__basic_dateCreated": { "$first": "$basic.dateCreated.searchValue" } } }
  ]
}
```

  </TabItem>
</Tabs>

After the successful import, it is a good idea to run differential import (using `lastModifiedDate`) for the period during which we were performing the initial migration (to synchronize records that were updated in the meantime):

```json
{
  "lastModifiedDate": {
    "operator": "onOrAfter",
    "searchValue": "__date of the full import start__" // replace with ISO date format
  }
}
```

And finally, it is possible to switch to differential imports only:

```json
{
  "lastModifiedDate": {
    "operator": "onOrAfter",
    "searchValue": "{last_modified_date}"
  }
}
```
---
title: 'NetSuite: Export configuration'
sidebar_position: 3
sidebar_label: 'Export configuration'
---

import WIP from '../\_wip.md';

# Export configuration

This page showcases the most common configurations. The final configuration depends heavily on the NetSuite instance configuration and might need to be adjusted as needed.

:::tip

When building the configuration, consult the [methods documentation](https://docs.oracle.com/en/cloud/saas/netsuite/ns-online-help/section_N3478008.html#Web-Services-Standard-Operations) and [schema browser](https://system.netsuite.com/help/helpcenter/en_US/srbrowser/Browser2022_2/script/record/vendor.html).

:::

## Customer Payment

```json
{
  "run_async": false,
  "export_configs": [
    {
      "payload": [
        {
          "method_args": [
            {
              "_ns_type": "CustomerPayment",
              "arAcct": {
                "type": "account",
                "_ns_type": "RecordRef",
                "internalId": 123
              },
              "account": {
                "type": "account",
                "_ns_type": "RecordRef",
                "internalId": "@{ns_account}"
              },
              "payment": "@{amount_due}",
              "customer": {
                "type": "customer",
                "_ns_type": "RecordRef",
                "internalId": "@{sender_match}"
              },
              "tranDate": {
                "$IF_SCHEMA_ID$": {
                  "mapping": {
                    "$DATAPOINT_VALUE$": {
                      "schema_id": "date_issue",
                      "value_type": "iso_datetime"
                    }
                  },
                  "schema_id": "date_issue"
                }
              },
              "applyList": {
                "apply": {
                  "$FOR_EACH_SCHEMA_ID$": {
                    "mapping": {
                      "doc": "@{ns_item_internal_id}",
                      "due": "@{item_amount_to_apply}",
                      "apply": true,
                      "total": "@{item_amount_to_apply}",
                      "amount": "@{item_amount_to_apply}",
                      "refNum": "@{item_invoice_number}",
                      "_ns_type": "CustomerPaymentApply"
                    },
                    "schema_id": "line_item"
                  }
                },
                "_ns_type": "CustomerPaymentApplyList"
              },
              "undepFunds": false,
              "paymentOption": {
                "type": "paymentMethod",
                "_ns_type": "RecordRef",
                "internalId": "@{ns_payment_option}"
              }
            }
          ],
          "method_name": "add"
        }
      ]
    }
  ],
  "netsuite_settings": {
    "account": "XXX_SB1",
    "wsdl_url": "https://XXX-sb1.suitetalk.api.netsuite.com/wsdl/v2024_1_0/netsuite.wsdl",
    "service_url": "https://XXX-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2024_1",
    "concurrency_limit": 4,
    "service_binding_name": "{urn:platform_2024_1.webservices.netsuite.com}NetSuiteBinding"
  }
}
```

## Sales Order

From Agile Sourcing:

```json
{
  "run_async": false,
  "netsuite_settings": {
    "account": "4597347_SB1",
    "concurrency_limit": 4,
    "wsdl_url": "https://4597347-sb1.suitetalk.api.netsuite.com/wsdl/v2025_1_0/netsuite.wsdl",
    "service_url": "https://4597347-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2025_1",
    "service_binding_name": "{urn:platform_2025_1.webservices.netsuite.com}NetSuiteBinding"
  },
  "export_configs": [
    {
      "payload": {
        "method_name": "upsert",
        "method_args": [
          {
            "_ns_type": "SalesOrder",
            "customForm": {
              "type": "customRecord",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_customForm}"
            },
            "entity": {
              "type": "customer",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_customer_match}"
            },
            "subsidiary": {
              "type": "subsidiary",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_subsidiary_match}"
            },
            "currency": {
              "type": "currency",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_currency_match}"
            },
            "location": {
              "type": "location",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_location_match}"
            },
            "tranDate": {
              "$IF_SCHEMA_ID$": {
                "schema_id": "date_issue",
                "mapping": {
                  "$DATAPOINT_VALUE$": {
                    "schema_id": "date_issue",
                    "value_type": "iso_datetime"
                  }
                }
              }
            },
            "itemList": {
              "_ns_type": "SalesOrderItemList",
              "item": {
                "$FOR_EACH_SCHEMA_ID$": {
                  "schema_id": "line_item",
                  "mapping": {
                    "_ns_type": "SalesOrderItem",
                    "item": {
                      "type": "inventoryItem",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_item_match}"
                    },
                    "quantity": "@{item_quantity}"
                  }
                }
              }
            }
          }
        ]
      }
    }
  ]
}
```

More comprehensive (but untested) example:

```json
{
  "run_async": false,
  "netsuite_settings": {
    "account": "4597347_SB1",
    "concurrency_limit": 4,
    "wsdl_url": "https://4597347-sb1.suitetalk.api.netsuite.com/wsdl/v2025_1_0/netsuite.wsdl",
    "service_url": "https://4597347-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2025_1",
    "service_binding_name": "{urn:platform_2025_1.webservices.netsuite.com}NetSuiteBinding"
  },
  "export_configs": [
    {
      "payload": {
        "method_name": "upsert",
        "method_args": [
          {
            "_ns_type": "SalesOrder",
            "customForm": {
              "type": "customRecord",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_customForm}"
            },
            "entity": {
              "type": "customer",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_customer_match}"
            },
            "currency": {
              "type": "currency",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_currency_match}"
            },
            "subsidiary": {
              "type": "subsidiary",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_subsidiary_match}"
            },
            "class": {
              "type": "classification",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_class_match}"
            },
            "department": {
              "type": "department",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_department_match}"
            },
            "location": {
              "type": "location",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_location_match}"
            },
            "tranId": "@{document_id_normalized}",
            "tranDate": {
              "$IF_SCHEMA_ID$": {
                "schema_id": "date_issue",
                "mapping": {
                  "$DATAPOINT_VALUE$": {
                    "schema_id": "date_issue",
                    "value_type": "iso_datetime"
                  }
                }
              }
            },
            "orderStatus": {
              "_ns_type": "SalesOrderOrderStatus",
              "value": "_pendingFulfillment"
            },
            "salesRep": {
              "type": "employee",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_salesrep_match}"
            },
            "terms": {
              "type": "term",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_terms_match}"
            },
            "memo": "@{memo}",
            "otherRefNum": "@{reference_number}",
            "externalId": "@{ns_externalId}",
            "billingAddress": {
              "$IF_SCHEMA_ID$": {
                "schema_id": "billing_address",
                "mapping": {
                  "_ns_type": "Address",
                  "addr1": "@{billing_address_line1}",
                  "addr2": "@{billing_address_line2}",
                  "city": "@{billing_address_city}",
                  "state": "@{billing_address_state}",
                  "zip": "@{billing_address_zip}",
                  "country": "@{billing_address_country}"
                }
              }
            },
            "shippingAddress": {
              "$IF_SCHEMA_ID$": {
                "schema_id": "shipping_address",
                "mapping": {
                  "_ns_type": "Address",
                  "addr1": "@{shipping_address_line1}",
                  "addr2": "@{shipping_address_line2}",
                  "city": "@{shipping_address_city}",
                  "state": "@{shipping_address_state}",
                  "zip": "@{shipping_address_zip}",
                  "country": "@{shipping_address_country}"
                }
              }
            },
            "shipDate": {
              "$IF_SCHEMA_ID$": {
                "schema_id": "ship_date",
                "mapping": {
                  "$DATAPOINT_VALUE$": {
                    "schema_id": "ship_date",
                    "value_type": "iso_datetime"
                  }
                }
              }
            },
            "shipMethod": {
              "type": "shipItem",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_shipmethod_match}"
            },
            "itemList": {
              "_ns_type": "SalesOrderItemList",
              "item": {
                "$FOR_EACH_SCHEMA_ID$": {
                  "schema_id": "line_item",
                  "mapping": {
                    "_ns_type": "SalesOrderItem",
                    "item": {
                      "type": "inventoryItem",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_item_match}"
                    },
                    "description": "@{item_description}",
                    "quantity": "@{item_quantity}",
                    "rate": "@{item_amount_base}",
                    "amount": "@{item_amount_total}",
                    "location": {
                      "type": "location",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_location_match}"
                    },
                    "taxCode": {
                      "type": "taxType",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_taxcode_match}"
                    },
                    "class": {
                      "type": "classification",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_class_match}"
                    },
                    "department": {
                      "type": "department",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_department_match}"
                    }
                  }
                }
              }
            }
          }
        ]
      }
    }
  ]
}
```

## Vendor Bills (Invoices)

The following shows a Vendor Bill export that (perhaps with some small tweaks) should work for most of the cases.

Visit [Rossum Formulas](../rossum-formulas/formula-fields#generate-netsuite-external-ids) page to learn how to create external NetSuite IDs.

```json
{
  "run_async": false,
  "netsuite_settings": {
    "account": "XXX_SB1", // Case sensitive!
    "concurrency_limit": 4,
    "wsdl_url": "https://XXX-sb1.suitetalk.api.netsuite.com/wsdl/v2024_1_0/netsuite.wsdl",
    "service_url": "https://XXX-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2024_1",
    "service_binding_name": "{urn:platform_2024_1.webservices.netsuite.com}NetSuiteBinding"
  },
  "export_configs": [
    {
      "payload": {
        "method_name": "upsert",
        "method_args": [
          {
            "_ns_type": "VendorBill",
            "approvalStatus": {
              "_ns_type": "RecordRef",
              "internalId": "2" // 1 = Pending, 2 = Approved
            },
            "class": {
              "type": "classification",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_class_match}"
            },
            "customForm": {
              "type": "customRecord",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_customForm}"
            },
            "currency": {
              "type": "currency",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_currency_match}"
            },
            "department": {
              "type": "department",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_department_match}"
            },
            "dueDate": {
              "$IF_SCHEMA_ID$": {
                "schema_id": "date_due",
                "mapping": {
                  "$DATAPOINT_VALUE$": {
                    "schema_id": "date_due",
                    "value_type": "iso_datetime"
                  }
                }
              }
            },
            "entity": {
              "type": "vendor",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_vendor_match}"
            },
            "externalId": "@{ns_externalId}",
            "subsidiary": {
              "type": "subsidiary",
              "_ns_type": "RecordRef",
              "internalId": "@{ns_subsidiary_match}"
            },
            "tranId": "@{document_id_normalized}",
            "tranDate": {
              "$IF_SCHEMA_ID$": {
                "schema_id": "date_issue",
                "mapping": {
                  "$DATAPOINT_VALUE$": {
                    "schema_id": "date_issue",
                    "value_type": "iso_datetime"
                  }
                }
              }
            },
            "itemList": {
              "_ns_type": "VendorBillItemList",
              "item": {
                "$FOR_EACH_SCHEMA_ID$": {
                  "schema_id": "line_item",
                  "mapping": {
                    "_ns_type": "VendorBillItem",
                    "description": "@{item_description}",
                    "item": {
                      "type": "inventoryItem",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_item_match}"
                    },
                    "rate": "@{item_amount_base}",
                    "location": {
                      "type": "location",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_ns_location_match}"
                    },
                    "quantity": "@{item_quantity}",
                    "taxCode": {
                      "type": "taxType",
                      "_ns_type": "RecordRef",
                      "internalId": "@{item_po_item_taxCode_match}"
                    }
                  }
                }
              }
            }
          }
        ]
      }
    }
  ]
}
```

### Linking Vendor Bills with Purchase Orders

To connect Vendor Bills with Purchase Orders, it is necessary to set both `orderDoc` and `orderLine` on line-items level (showing only relevant parts of the export config):

```json
{
  "export_configs": [
    {
      "payload": {
        "method_name": "upsert",
        "method_args": [
          {
            "tranId": "@{document_id}",
            "itemList": {
              "_ns_type": "VendorBillItemList",
              "item": {
                "$FOR_EACH_SCHEMA_ID$": {
                  "schema_id": "line_item",
                  "mapping": {
                    // …
                    "_ns_type": "VendorBillItem",
                    // highlight-start
                    "orderDoc": "@{item_po_match}", // PO internal ID
                    "orderLine": "@{item_po_item_line_match}" // Relevant line-item number from PO (itemList.item.line)
                    // highlight-end
                    // …
                  }
                }
              }
            }
            // …
          }
        ]
      }
    }
  ]
}
```

Note that the combination of Purchase Order and line item no. can appear only once in the payload. In case it appears twice on the invoice then it's necessary to group the line items before exporting.

### Conditional configuration using `$DATAPOINT_MAPPING$`

You can leverage JSON Templating to introduce conditions into the configuration. For example, in this example, we are changing document (NS) type based on the detected document type:

```json
{
  "_ns_type": {
    // highlight-start
    "$DATAPOINT_MAPPING$": {
      // highlight-end
      "schema_id": "document_type",
      "mapping": {
        "tax_credit": "VendorCredit",
        "tax_invoice": "VendorBill"
      }
    }
  }
}
```

Similarly, for line items and so on:

```json
{
  "_ns_type": {
    // highlight-start
    "$DATAPOINT_MAPPING$": {
      // highlight-end
      "schema_id": "document_type",
      "mapping": {
        "tax_credit": "VendorCreditItemList",
        "tax_invoice": "VendorBillItemList"
      }
    }
  }
}
```

Consider implementing this `$DATAPOINT_MAPPING$` condition higher in the configuration tree and duplicating the whole sections to avoid too complex conditional configurations.

### Working with custom fields

Custom fields on header level are usually prefixed by `custbody_`:

```json
{
  "customFieldList": {
    "_ns_type": "CustomFieldList",
    "customField": [
      {
        "value": "@{amount_total}",
        "_ns_type": "StringCustomFieldRef",
        "scriptId": "custbody_captured_total_amount"
      }
      // …
    ]
  }
}
```

Custom fields can also be added conditionally using special `$IF_SCHEMA_ID$` syntax:

```json
{
  "customFieldList": {
    "_ns_type": "CustomFieldList",
    "customField": [
      {
        "$IF_SCHEMA_ID$": {
          "mapping": {
            "value": "@{amount_total}",
            "_ns_type": "StringCustomFieldRef",
            "scriptId": "custbody_captured_total_amount"
          },
          "schema_id": "amount_total"
        }
      }
      // …
    ]
  }
}
```

Line item custom fields are usually prefixed by `custcol_`. They also must be nested in the item list:

```json
{
  "itemList": {
    "item": {
      "customFieldList": {
        "_ns_type": "CustomFieldList",
        "customField": [
          {
            "value": "@{ns_custcol_some_field}",
            "_ns_type": "StringCustomFieldRef",
            "scriptId": "custcol_some_field"
          }
          // …
        ]
      }
    }
  }
}
```

Custom fields are represented by the type `CustomFieldRef`, which is an abstract type. The table below contains a list of concrete custom field types that extend the `CustomFieldRef` type:

| JSON Mapping Type           | Custom Field Type in UI                                                       |
| :-------------------------- | :---------------------------------------------------------------------------- |
| `LongCustomFieldRef`        | Integer                                                                       |
| `DoubleCustomFieldRef`      | Decimal Number                                                                |
| `BooleanCustomFieldRef`     | Check Box                                                                     |
| `StringCustomFieldRef`      | Free-Form Text, Text Area, Phone Number, E-mail Address, Hyperlink, Rich Text |
| `DateCustomFieldRef`        | Date, Time of Day, or Date/Time (both in one field)                           |
| `SelectCustomFieldRef`      | List/Record, Document                                                         |
| `MultiSelectCustomFieldRef` | Multiple Select                                                               |

For more information, please visit: https://docs.oracle.com/en/cloud/saas/netsuite/ns-online-help/section_N3458179.html

### Using NetSuite File Cabinet (`pipeline_context`)

You can reference earlier export stages by accessing `pipeline_context` variable. In the following example, we use `pipeline_context` for attaching file uploaded to the NetSuite File Cabinet. Note that the configuration uses `original_file_name` variable from the [`Get document information` serverless function](../rossum-formulas/serverless-functions.md#get-document-information).

<details>
  <summary>`original_file_name` serverless function ensuring two files with the same name can be uploaded to NetSuite file cabinet</summary>

```py
import os
from datetime import datetime
from rossum_python import RossumPython


def add_iso_timestamp(filename):
    # Get the file name and extension separately
    name, ext = os.path.splitext(filename)

    # Get the current ISO timestamp
    timestamp = datetime.now().isoformat(timespec='seconds')

    # Create the new filename with the timestamp
    new_filename = f"{name}_{timestamp}{ext}"

    return new_filename


def rossum_hook_request_handler(payload):
    r = RossumPython.from_payload(payload)

    # Add ISO timestamp to the file name to ensure two files with the same name can be uploaded to NetSuite file cabinet:
    r.field.meta_original_file_name = add_iso_timestamp(payload.get("document").get("original_file_name"))

    return r.hook_response()
```

</details>

```json
{
  "export_configs": [
    {
      "payload": [
        {
          // … upsert VendorBill here as usual
        },
        {
          "method_name": "add",
          "method_args": [
            {
              "name": "@{original_file_name}",
              "folder": {
                "type": "folder",
                "_ns_type": "RecordRef",
                "internalId": "123456"
              },
              "content": {
                // highlight-start
                "$GET_DOCUMENT_CONTENT$": {}
                // highlight-end
              },
              "_ns_type": "File",
              "attachFrom": "_web",
              "description": {
                "$DATAPOINT_MAPPING$": {
                  "mapping": {
                    "credit_note": "VendorCredit processed by Rossum",
                    "tax_invoice": "VendorBill processed by Rossum"
                  },
                  "schema_id": "document_type"
                }
              }
            }
          ]
        },
        {
          "method_name": "attach",
          "method_args": [
            {
              "_ns_type": "AttachBasicReference",
              "attachTo": {
                "type": {
                  "$DATAPOINT_MAPPING$": {
                    "mapping": {
                      "credit_note": "vendorCredit",
                      "tax_invoice": "vendorBill"
                    },
                    "schema_id": "document_type"
                  }
                },
                "_ns_type": "RecordRef",
                // highlight-start
                "internalId": "{pipeline_context[0].internal_id}"
                // highlight-end
              },
              "attachedRecord": {
                "type": "file",
                "_ns_type": "RecordRef",
                // highlight-start
                "internalId": "{pipeline_context[1].internal_id}"
                // highlight-end
              }
            }
          ]
        }
      ]
    }
  ]
}
```

Notice also the highlighted `$GET_DOCUMENT_CONTENT$` which returns the content of the original file.

## Vendor Credits (credit notes)

<WIP />

TODO: copy one from Deliveroo

### Linking Vendor Credits with Vendor Bills

Vendor credits can be linked to vendor bills via `applyList` configuration. Note that the credits can be applied only "Open" vendor bills and always to VB line `0`. In this example, we apply the whole credit note value to the specified vendor bill:

```json
{
  "applyList": {
    "$IF_DATAPOINT_VALUE$": {
      "schema_id": "ns_status_match",
      "value": "Open",
      "mapping": {
        "apply": {
          "doc": "@{ns_internalId_match}",
          "line": "0",
          "apply": true,
          "amount": "@{amount_total_normalized}",
          "_ns_type": "VendorCreditApply"
        },
        "_ns_type": "VendorCreditApplyList"
      }
    }
  }
}
```
---
title: 'NetSuite'
sidebar_position: 1
---

import WebhookEndpoints from '../\_webhook_endpoints.md';

:::info[API documentation]

👉 https://elis.rossum.ai/svc/netsuite-v3/api/redoc

:::

## Installation

NetSuite service (integration) is provided by Rossum.ai in the form of webhook. To start using NetSuite (either imports or exports), follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `NetSuite SB1: Import/Export`
   1. Trigger events: `Manual` (later also `Scheduled`)
   1. Extension type: `Webhook`
   1. URL (see below)
   1. In "Advanced settings" select Token owner
1. Click **Create the webhook**.
1. Fill `Configuration` and `Secrets` fields (see [Integration Configuration](./integration-configuration.md) and [Import configuration](./import-configuration.md) or [Export configuration](./export-configuration.md).
1. (Optional) Disable retries for export webhooks (see: [Considerations & Limitations](./considerations.md#webhook-retries-5-on-failed-requests))
1. (Optional) Set hook `secrets_schema` value (see [below](#setting-hook-secrets_schema-value))

### Import endpoints

<WebhookEndpoints
  eu1="https://elis.rossum.ai/svc/netsuite-v3/api/v1/import"
  eu2="https://shared-eu2.rossum.app/svc/netsuite-v3/api/v1/import"
  us="https://us.app.rossum.ai/svc/netsuite-v3/api/v1/import"
/>

### Export endpoints

<WebhookEndpoints
  eu1="https://elis.rossum.ai/svc/netsuite-v3/api/v1/export"
  eu2="https://shared-eu2.rossum.app/svc/netsuite-v3/api/v1/export"
  us="https://us.app.rossum.ai/svc/netsuite-v3/api/v1/export"
/>

### Setting hook `secrets_schema` value

By default, all hooks have the following JSON schema of their secrets:

```json
{
  "type": "object",
  "additionalProperties": { "type": "string" }
}
```

Consider changing it to the following value to clearly outline what values are supported:

```json
{
  "type": "object",
  "properties": {
    "consumer_key": { "type": "string", "minLength": 1 },
    "consumer_secret": { "type": "string", "minLength": 1 },
    "token_key": { "type": "string", "minLength": 1 },
    "token_secret": { "type": "string", "minLength": 1 },
    "rossum_username": { "type": "string", "minLength": 1 },
    "rossum_password": { "type": "string", "minLength": 1 }
  },
  "additionalProperties": false
}
```

Probably the easiest way to achieve this is updating the hook configuration using [`prd` tool](../sandboxes/index.md)

## System context diagram

![NetSuite system context diagram](./img/rossum-netsuite-system-context-diagram.png)

## Useful links

NetSuite main navigation can sometimes be very confusing as it is very customizable. Use the following paths to quickly access NetSuite resources:

- Accounts: [`/app/accounting/account/accounts.nl`](https://system.netsuite.com/app/accounting/account/accounts.nl)
- Currencies: [`/app/common/multicurrency/currencylist.nl`](https://system.netsuite.com/app/common/multicurrency/currencylist.nl)
- Departments: [`/app/common/otherlists/departmentlist.nl?searchtype=Department`](https://system.netsuite.com/app/common/otherlists/departmentlist.nl?searchtype=Department)
- File Cabinet [`/app/common/media/mediaitemfolders.nl`](https://system.netsuite.com/app/common/media/mediaitemfolders.nl)
- Items: [`/app/common/item/itemlist.nl`](https://system.netsuite.com/app/common/item/itemlist.nl)
- Nexuses: [`/app/setup/nexuses.nl`](https://system.netsuite.com/app/setup/nexuses.nl)
- Purchase Orders: [`/app/accounting/transactions/transactionlist.nl?Transaction_TYPE=PurchOrd`](https://system.netsuite.com/app/accounting/transactions/transactionlist.nl?Transaction_TYPE=PurchOrd)
- Roles: [`/app/setup/rolelist.nl`](https://system.netsuite.com/app/setup/rolelist.nl)
- Subsidiaries: [`/app/common/otherlists/subsidiarylist.nl`](https://system.netsuite.com/app/common/otherlists/subsidiarylist.nl)
- Vendor Bills: [`/app/accounting/transactions/transactionlist.nl?Transaction_TYPE=VendBill`](https://system.netsuite.com/app/accounting/transactions/transactionlist.nl?Transaction_TYPE=VendBill)
- Vendor Credits: [`/app/accounting/transactions/transactionlist.nl?Transaction_TYPE=VendCred`](https://system.netsuite.com/app/accounting/transactions/transactionlist.nl?Transaction_TYPE=VendCred)
- Vendors: [`/app/common/entity/vendorlist.nl`](https://system.netsuite.com/app/common/entity/vendorlist.nl)

## Available configuration options

The following configuration options are available:

```json
{
  // Determines whether or not NetSuite should run the configuration asynchronously. Typically,
  // imports are asynchronous (since they can take hours) and exports are synchronous (they should
  // take minutes).
  "run_async": true,

  "netsuite_settings": {
    // Case sensitive NetSuite account:
    "account": "XXX_SB1",

    // How many concurrent operations through API can run at the same time:
    "concurrency_limit": 4,

    "wsdl_url": "https://XXX-sb1.suitetalk.api.netsuite.com/wsdl/v2024_1_0/netsuite.wsdl",
    "service_url": "https://XXX-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2024_1",
    "service_binding_name": "{urn:platform_2024_1.webservices.netsuite.com}NetSuiteBinding"
  },

  // Configures imports (cannot be used together with `export_configs`):
  "import_configs": [
    {
      // Name of the dataset in Master Data Hub:
      "master_data_name": "NS_SB1_Currency_v1",

      // Optional configurations of the asynchronous behavior (makes sense only when
      // `run_async` is true):
      "async_settings": {
        "retries": 5,
        "max_run_time_s": 36000
      },

      // The actual payload of NetSuite request (closely follows NetSuite API docs):
      "payload": {
        "method_name": "getAll",
        "method_args": [
          {
            "_ns_type": "GetAllRecord",
            "recordType": "currency"
          }
        ],

        // Optional headers for NetSuite API request:
        "method_headers": {
          // NetSuite request-level search preferences (https://docs.oracle.com/en/cloud/saas/netsuite/ns-online-help/section_4170181850.html):
          "searchPreferences": {
            "pageSize": 100,
            "bodyFieldsOnly": false,
            "returnSearchColumns": false
          },

          // Other NetSuite request-level preferences (https://docs.oracle.com/en/cloud/saas/netsuite/ns-online-help/section_4170181850.html):
          "preferences": {
            "runServerSuiteScriptAndTriggerWorkflows": false
            // …
          }
        }
      }
    }
    // …
  ],

  // Configures exports (cannot be used together with `import_configs`):
  "export_configs": [
    // Same with `import_configs`
  ]
}
```
---
title: 'NetSuite: Integration configuration'
sidebar_position: 1
sidebar_label: 'Integration configuration'
---

# Integration configuration

To configure an access token in NetSuite for SOAP communication, especially when there is no integration user yet, involves several steps that include creating an integration record in NetSuite, setting up a role with the necessary permissions, creating an integration user, and then generating the access token. Below is a step-by-step guide:

## 1. Create an Integration Record in NetSuite

1. **Log in to your NetSuite account** as an administrator (https://system.netsuite.com/).
1. Navigate to **Setup → Integrations → Manage Integrations → New**.
1. Fill in the **Name** of the integration and ensure that **State** is enabled.
1. Check the **Token-Based Authentication** to use token-based auth along with SOAP.
1. **Save** the integration. Note the **Consumer Key** and **Consumer Secret** presented upon saving; these are important for authentication.

:::warning

Remember to save the **Consumer Key** and **Consumer Secret** for later.

:::

![NetSuite Integration configuration](./img/netsuite-integration.png)

## 2. Create a Role with necessary permissions

1. Navigate to **Setup → Users/Roles → Manage Roles → New**.
1. Provide a **Name** for the role and assign it permissions necessary for the operations the integration will perform. At a minimum, for SOAP communication, you might need permissions like **SOAP Web Services, Log in using Access Tokens**, and any specific permissions related to the data you wish to access or modify (Lists).
1. **Save** the role.

![NetSuite Role configuration](./img/netsuite-role.png)

## 3. Create an Integration User

1. Go to **Lists → Employees → Employees → New**.
1. Fill in the necessary information for the user. Under the **Access** tab, ensure you **Check** the **Give Access** option, set a **Password**, and **Assign the Role** you created earlier.
1. **Save** the employee record.

:::tip

While it is technically possible to reuse already existing employee account, we recommend creating a new one for the integration. This way, the integration won't get broken when the employee account gets deactivated.

:::

## 4. Generate the Access Token

1. Navigate to **Setup → Users/Roles → Access Tokens → New**.
1. Select the **Application Name** (the integration you created in Step 1), the **User**, and the **Role** you've assigned to this integration.
1. **Save** to generate the Token ID and Token Secret.

:::warning

Remember to save the **Token ID** and **Token Secret** for later.

:::

## 5. Use the Access Token in SOAP communication

With the Consumer Key, Consumer Secret, Token ID, and Token Secret, you can now configure Rossum.ai SOAP client for communication with NetSuite:

Secrets:

```json
{
  "consumer_key": "…", // change
  "consumer_secret": "…", // change
  "token_key": "…", // change
  "token_secret": "…", // change
  "rossum_username": "system.user@rossum.example", // change
  "rossum_password": "…" // change
}
```

Import/Export configuration:

```json
{
  "netsuite_settings": {
    "account": "XXX_SB1", // Case sensitive!
    "concurrency_limit": 4,
    "wsdl_url": "https://XXX-sb1.suitetalk.api.netsuite.com/wsdl/v2024_1_0/netsuite.wsdl",
    "service_url": "https://XXX-sb1.suitetalk.api.netsuite.com/services/NetSuitePort_2024_1",
    "service_binding_name": "{urn:platform_2024_1.webservices.netsuite.com}NetSuiteBinding"
  }
}
```

Account ID (`account`) can be found under **Setup → Company → Company Information**.

## Important notes

- Ensure your NetSuite account has the **Token-Based Authentication** feature enabled. This can be checked and enabled under **Setup → Company → Enable Features → SuiteCloud**.
- The permissions assigned to the role will dictate what operations can be performed through the SOAP API. Make sure to adjust these according to the least privilege principle, granting only the permissions necessary for the tasks the integration will perform.

This process sets up a secure method for your application or integration to communicate with NetSuite using SOAP. If you encounter any specific issues during setup or need more detailed instructions, NetSuite's official documentation and support resources can provide additional guidance tailored to the latest platform updates and best practices.
---
title: 'NetSuite: Considerations & Limitations'
sidebar_position: 5
sidebar_label: 'Considerations & Limitations'
---

# Considerations & Limitations

## In general

Building NetSuite integration is much more than just reading the values from documents and forwarding them to the NetSuite API. The following considerations should be taken into account when designing a new NetSuite implementation:

1. What document types are going to be processed? `VendorBill`, `VendorCredit`, or some other documents? Should they be in one queue or each document type or vendor in its own queue? What are the document regions? All of these questions affect the final queue structure, schemas as well as NetSuite export configuration.
1. What data needs to be data matched? PO-backed workflows might require just POs whereas non-PO-backed workflows might require many other NetSuite records to match (required by the NetSuite export). This greatly affects what data needs to be synchronized from NetSuite. Also consider how large are the collections and how many records should be synchronized during the initial import.
1. What system fields will be necessary? For example, `VendorCredits` must have all amounts and quantities positive. Therefore, several hidden [Formula Fields](../rossum-formulas/formula-fields) performing this conversion might be necessary for the export.
1. Consider preparing [Line items grouping](../line-items-grouping) extension. Apart from potential business requirements, NetSuite doesn't allow line items with repeating items.
1. Consider what all business rules and validations must be implemented.
1. Consider whether duplicate detection should be implemented or not. Customers often find it unexpected when we allow exporting the same document multiple times (even though NetSuite allows it by default). Note that this is an additional mechanism how to detect duplicates. In general, duplicates in Rossum can be detected by:
   1. Comparing files on a binary level (looking for identical files)
   2. Comparing extracted fields (looking for exactly matching extracted content)
   3. Searching whether such invoices already exist in NetSuite or not (complements the previous point but takes historic documents into account as well)
1. Consider whether original files should be attached and created in the NetSuite File Cabinet. If so, make sure that the original filenames are unique (filenames in Rossum can be duplicated but not in NetSuite). Example [serverless function](../rossum-formulas/serverless-functions.md) implementation:

```py
import os
from datetime import datetime
from rossum_python import RossumPython


def add_iso_timestamp(filename):
    # Get the file name and extension separately
    name, ext = os.path.splitext(filename)

    # Get the current ISO timestamp
    timestamp = datetime.now().isoformat(timespec='seconds')

    # Create the new filename with the timestamp
    new_filename = f"{name}_{timestamp}{ext}"

    return new_filename


def rossum_hook_request_handler(payload):
    r = RossumPython.from_payload(payload)

    # Add ISO timestamp to the file name to ensure two files with the same name can be uploaded to NetSuite file cabinet:
    r.field.meta_original_file_name = add_iso_timestamp(payload.get("document").get("original_file_name"))

    return r.hook_response()
```

## Default webhook timeout is 30 seconds

By default, all webhooks in Rossum timeout after 30 seconds. Usually, this time is enough for most webhooks. However, some more complex documents (longer ones with more line items) can take longer than that to export.

To fix this issue, it is possible to set custom timeout by calling the following API endpoint:

```text
PATCH /v1/hooks/{id}
```

Request payload example:

```json
{
  "config": {
    // highlight-start
    "timeout_s": 60
    // highlight-end
  }
}
```

Example [`curl`](https://github.com/curl/curl) request:

```bash
curl --location --request PATCH 'https://[EXAMPLE].rossum.app/api/v1/hooks/[HOOK_ID]' \
--header 'Authorization: Bearer [API_TOKEN]' \
--header 'Content-Type: application/json' \
--data '{"timeout_s": 60}'
```

See API reference for more details: https://elis.rossum.ai/api/docs/#update-part-of-a-hook

:::warning

The maximum allowed timeout is 60 seconds. Consider contacting [Rossum Sales](https://rossum.ai/form/contact/) or Rossum Support team if you need help finding alternative solutions.

:::

## Webhook retries 5× on failed requests

By default, webhooks are retried 5× on failed requests. This behavior can be inconvenient if it's not possible to guarantee idempotency of the requests (for example, NetSuite exports). This can be changed or completely disabled using the following API endpoint:

```text
PATCH /v1/hooks/{id}
```

Request payload example:

```json
{
  "config": {
    // highlight-start
    "retry_count": 0
    // highlight-end
  }
}
```

Use number `0` to disable retries or any other number to change the default number of retries.

See API reference for more details: https://elis.rossum.ai/api/docs/#update-part-of-a-hook
---
title: 'Supported Integration Patterns'
sidebar_position: 1
---

# Supported Integration Patterns

Integrating systems efficiently is critical for seamless data exchange and workflow automation. When working with the Rossum API, various integration patterns can be employed to suit different technical requirements and business needs. In this article, we explore five key integration methods—each with its own pros and cons—to help you determine the best approach for your system.

## 1. Scheduled polling integration

In this model, the target system schedules regular queries to the Rossum API to retrieve annotations that are ready for export.

<b>Pros:</b>

- Full control over query frequency, error handling, and resource management.

<b>Cons:</b>

- No real-time updates.
- Requires tracking loaded annotations (Rossum’s `Confirmed` to `Exported` statuses help with this).
- Hosting computational resources is necessary to run these scheduled queries.

![Scheduled-Polling-Integration](img/Scheduled-Polling-Integration.png)

## 2. Webhook-driven integration

Here, the target system listens for real-time notifications via Rossum’s Webhook. When a notification is received, the system queries the Rossum API to retrieve the annotation data. This eliminates the need for a scheduled job and for tracking which annotations are ready, compared to approach #1 above. Notifications Webhook is provided by Rossum out of the box.

<b>Pros:</b>

- Real-time updates.
- Ability to respond dynamically to status changes, customizing actions based on business logic.
- Webhook setup is straightforward, requiring just a target URL and specified triggers.

<b>Cons:</b>

- The system must be able to handle incoming requests.
- Even after receiving the notification, an API query is still necessary to retrieve the full annotation (except for certain event types that include annotation data. Please see [documentation](https://elis.rossum.ai/api/docs/#webhook-events) and option #3).

![Webhook-Driven-Integration](img/Webhook-Driven-Integration.png)

## 3. Direct push integration using webhook with annotation data

In this variation, Rossum’s Webhook delivers a direct push of data to the target system.
It will be possible to do for the range of actions like: confirmation or export. For more see [documentation](https://elis.rossum.ai/api/docs/#webhook-events). But if you want to react only on status change the attached payload will have no content of the annotation.

<b>Pros:</b>

- Real-time updates.
- Rossum provides the Webhook integration out of the box.

<b>Cons:</b>

- The system must be able to handle incoming requests.
- Not all events deliver annotation data.
- Limited customization of the Webhook logic, as it is offered "as-is."

![Direct-Push-Integration-(Option-#2)](<img/Direct-Push-Integration-(Option-2).png>)

## 4. Direct push integration using serverless function hosted in Rossum

Rossum’s serverless function pushes data directly to the target system’s public endpoint, eliminating the need for the system to pull data.

<b>Pros:</b>

- Real-time updates.
- Flexibility to push data to any public endpoint.
- No external hosting required, as Rossum provides the serverless function for integration development.

<b>Cons:</b>

- The system must be able to handle incoming requests.
- Development is needed within Rossum, which requires knowledge of Python or Node.js.

![Direct-Push-Integration-(Option-#1)](<img/Direct-Push-Integration-(Option-1).png>)

## 5. File-based integration

With this method, Rossum exports documents to an SFTP server, and the target system retrieves the files based on its internal logic.

<b>Pros:</b>

- Simple setup process in Rossum.

<b>Cons:</b>

- Must track new vs. old documents.
- Potential latency due to the polling interval by the target system.

![File-based-Integration](img/File-based-Integration.png)

## Conclusion

Selecting the right integration pattern depends on your business’s real-time data needs, technical capabilities, and the resources available. Webhook-driven and direct push integrations offer the benefit of immediate updates, while scheduled polling and file-based integrations provide flexibility and simplicity at the cost of real-time responsiveness. By weighing the pros and cons of each approach, you can build a more efficient and tailored integration strategy with Rossum.
---
title: 'Distributive webhook'
sidebar_position: 1
---

import WebhookEndpoints from '../\_webhook_endpoints.md';
import WIP from '../\_wip.md';

TODO: update according to: https://rossumai.atlassian.net/wiki/spaces/SA/pages/2250572130/How+to+Enable+the+Distributive+Webhook

**Distributive webhook** extension allows for distribution of header-level or multiple-lines-level fields to respective line items. This is handy when some values appear only once per multiple lines.

## Install distributive webhook extension

Distribution of values to line items is provided as a service by Rossum.ai in the form of a webhook. To start using the extension, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Fill the following fields:
   1. Name: `Distributive Webhook`
   1. Trigger events: `Document content: Initialize, Updated`
   1. Queues where the extension should be executed
   1. Extension type: `Webhook`
   1. URL (see [Available endpoints](#available-endpoints) below)
   1. Under Advanced settings find Additional notification metadata and enable `Schemas` and `Queues`
1. Click **Create the webhook**.

### Available endpoints

<WebhookEndpoints
  eu1="https://elis.rossum.ai/svc/distributive/validate"
  eu2="https://{your domain}.rossum.app/svc/distributive/validate"
  us="https://us.app.rossum.ai/svc/distributive/validate"
  jp="https://shared-jp.app.rossum.ai/svc/distributive/validate"
/>

## Add the fields to schema

The distributive webhook needs the following fields:

1. Multi-value field for extracting the header-level or multi-line-level fields - source of the distribution
1. Line item field - target of the distribution

![Example with 2 multi-value fields](./img/schema.png)

## Configure the queue

The source and target fields of the distribution are configured in each queue metadata:

1. Make sure the queue is selected in the distributive webhook extension
1. Save the configuration into the queue metadata - you need to use the [API](https://elis.rossum.ai/api/docs/#queue)

### Configuration example

The configuration follows this format:

```json
{
  "distributive_fields": {
    "{line items table schema ID}": {
      "{line item field (target) schema ID}": {
        "schema_ids": ["{multivalue field's child (source) schema ID}"]
      }
    }
  }
}
```

For the sample schema above, the configuration would look like this:

```json
{
  "distributive_fields": {
    "line_items": {
      "item_delivery_note": {
        "schema_ids": ["delivery_note"]
      },
      "item_delivery_date": {
        "schema_ids": ["delivery_date"]
      }
    }
  }
}
```

### Change the Direction of Distribution

The default direction of the distribution is downwards, to the rows below the value.

For cases when the value should be distributed to the rows above it (instead of the ones below), it is possible to specify the preferred direction.

```json
{
  "distributive_fields": {
    "line_items": {
      "item_delivery_note": {
        "schema_ids": ["delivery_note"],
        // highlight-start
        "preferred_direction": "upwards",
        "preferred_direction_only": true
        // highlight-end
      }
    }
  }
}
```
---
title: 'Coupa: Postman collection'
sidebar_position: 5
sidebar_label: 'Postman collection'
---

# Postman collection

## Source

:::info

Download the Postman collection from here: https://compass.coupa.com/en-us/products/core-platform/integration-playbooks-and-resources/integration-knowledge-articles/postman-collection-for-coupa-apis

:::

## Usage

Although it is possible to setup it differently, using the provided collection has some benefits built-in:

- You can copy and paste the OAuth scopes "as is" (including commas)
- The secrets handling and rotation is also included in the "Pre-req" script

Best practices

- Do not store credentials directly in the variables, use Environment instead
- The field it is required you to fill are:
  - `URL`
  - `client_id`
  - `client_secret`
  - `scope`

(Do not fill the `token_life_time` or `access_token`. It is not necessary to create these variables at all, the "Pre-req" script will create it and handle automatically.)

## Working example

### Variables

![alt text](img/coupa-postman-vars.png)

### Environment

![alt text](img/coupa-postman-env.png)
---
title: 'Coupa: Import configuration'
sidebar_position: 2
sidebar_label: 'Import configuration'
---

import WebhookEndpoints from '../\_webhook_endpoints.md';

# Import configuration

Allows importing Coupa data to your Master Data Hub via the [Coupa Code API](https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api).

## Setup

Create webhook as described in [Integration Setup](./integration-setup.md#configuring-rossum) and use the right link from the table below (according the Rossum environment of configured account)

### Import endpoints

<WebhookEndpoints
  eu1="https://elis.rossum.ai/svc/scheduled-imports/api/coupa/v1/import"
  eu2="https://shared-eu2.rossum.app/svc/scheduled-imports/api/coupa/v1/import"
  us="https://us.app.rossum.ai/svc/scheduled-imports/api/coupa/v1/import"
/>

## Available configuration options

```json
{
  "credentials": {
    // Example: "b1946ac92492d2347c6235b4d2611184"
    "client_id": "…",
    // Example: "https://mycompany-dev.coupahost.com/"
    "base_api_url": "…",
    // Example: "core.accounting.read"
    "client_scope": "…"
  },
  "import_config": {
    // Query parameters to be passed to the Coupa API.
    "query": {
      // Explicit list of attributes to be imported.
      "fields": []
    },
    // What method to use when inserting data to MDH. Available options: "update", "insert"
    "method": "update",
    // List of attributes that are used to identify a record (used for method "update").
    "id_keys": ["id"],
    // Coupa API endpoint.
    "endpoint": "api/currencies",
    // Name of the dataset in MDH.
    "dataset_name": "COUPA_DEV_currencies_v1",
    // Number of records that will be imported per request.
    "records_per_request": 50
  }
}
```

### Adding new fields to `query.fields`

In case you want to add a new attribute from the Coupa API to be imported to your Master Data Hub, you need to simply add the attribute as a string to the list. Only keep in mind to replace "-" with "\_". For example:

- Original field name: `attribute-example`
- New field name: `attribute_example`

If you need to add a new object to the import you need to specify the name of the object and then list all attributes that are nested within the given object. The following example demonstrates import of a nested object which is named `payment-term` and from this object, we need attributes `id` and `name`.

Keep in mind to also replace "-" with "\_" in the name of the object:

```json
{
  "fields": [
    "id",
    "name",
    "code",
    "allowable_precession",
    "active",
    "updated_at",
    "created_at",
    // highlight-start
    {
      "payment_term": ["id", "name"]
    }
    // highlight-end
  ]
}
```

## Configuration examples

### Accounts

See: [Accounts API (/accounts)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/accounts-api-(accounts)>)

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.accounting.read"
  },
  "import_config": {
    "query": {
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/accounts",
    "dataset_name": "COUPA_DEV_accounts_v1",
    "records_per_request": 50
  }
}
```

### Account types

See: [Account Types API (/account_types)](https://compass.coupa.com/_dita_/en-us/documentation/plat/integ/coupa_core_api/topics/account_types_api_account_types.dita)

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.accounting.read"
  },
  "import_config": {
    "query": {
      "fields": [
        "id",
        "created_at",
        "updated_at",
        "name",
        "active",
        "legal_entity_name",
        "dynamic_flag",
        {
          "currency": ["id", "code", "decimals"]
        },
        {
          "primary_contact": [
            "id",
            "created_at",
            "updated_at",
            "email",
            "name_prefix",
            "name_suffix",
            "name_additional",
            "name_given",
            "name_family",
            "name_fullname",
            "notes",
            "active",
            "purposes"
          ]
        },
        {
          "primary_address": [
            "id",
            "created_at",
            "updated_at",
            "name",
            "location_code",
            "street1",
            "street2",
            "street3",
            "street4",
            "city",
            "state",
            "postal_code",
            "attention",
            "active",
            "business_group_name",
            "vat_number",
            "local_tax_number",
            "type",
            {
              "country": ["id", "code", "name"]
            },
            {
              "vat_country": ["id", "code", "name"]
            }
          ]
        }
      ],
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/account_types",
    "dataset_name": "COUPA_DEV_account_types_v1",
    "records_per_request": 50
  }
}
```

### Currencies

See [Currencies API (/currencies)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/currencies-api-(currencies)>)

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.common.read"
  },
  "import_config": {
    "query": {
      "fields": ["id", "code", "decimals"]
    },
    "method": "replace",
    "endpoint": "api/currencies",
    "dataset_name": "COUPA_DEV_currencies_v1",
    "records_per_request": 50
  }
}
```

### Exchange rates

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.common.read"
  },
  "import_config": {
    "query": {
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
    },
    "method": "update",
    "id_keys": ["id"],
    "endpoint": "api/exchange_rates",
    "dataset_name": "COUPA_DEV_exchange_rates_v1",
    "records_per_request": 50
  }
}
```

### Invoices

See: [Invoices API (/invoices)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/transactional-resources/invoices-api-(invoices)>)

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "….coupacloud.com/",
    "client_scope": "core.invoice.read"
  },
  "import_config": {
    "query": {
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/invoices",
    "dataset_name": "COUPA_DEV_invoices_v1",
    "records_per_request": 50
  }
}
```

### Legal entities

See: [Legal Entity API](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/transactional-resources/expenses-api-(expense_reports)/legal-entity-api>)

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.legal_entity.read"
  },
  "import_config": {
    "query": {
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/legal_entities",
    "dataset_name": "COUPA_DEV_legal_entities_v1",
    "records_per_request": 50
  }
}
```

### Lookup values

See: [Lookup Values API (/lookup_values)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/lookup-values-api-(lookup_values)>)

:::tip

Query attributes necessary for differential update are highlighted.

:::

For invoices that are not tied to a purchase order (non-PO-backed), Coupa requires default lookup values to ensure the correct categorization and processing of the invoice. These values help populate essential fields that would otherwise be filled by the purchase order, ensuring the invoice is successfully imported and can proceed through the approval and payment workflows without issues.

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "….coupacloud.com/",
    "client_scope": "core.common.read"
  },
  "import_config": {
    "query": {
      "fields": [
        "id",
        "name",
        "external-ref-num",
        "external-ref-code",
        "active",
        {
          "parent": [
            "id",
            "name",
            "active",
            {
              "custom_fields": {}
            },
            {
              "parent": [
                "id",
                "name",
                "active",
                {
                  "custom_fields": {}
                }
              ]
            }
          ]
        },
        {
          "lookup": ["id", "name", "active"]
        },
        {
          "custom_fields": {}
        }
      ],
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/lookup_values",
    "dataset_name": "COUPA_DEV_lookup_values_v1",
    "records_per_request": 50
  }
}
```

### Payment Terms

See: [Payment Terms API (/payment_terms)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/payment-terms-api-(payment_terms)>)

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.common.read"
  },
  "import_config": {
    "query": {
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/payment_terms",
    "dataset_name": "COUPA_DEV_payment_terms_v1",
    "records_per_request": 50
  }
}
```

### Purchase orders

With this import configuration, you will receive a final collection that includes all Purchase Orders (POs), along with an array of their corresponding PO line items nested within each order. This structure allows for easy access to both the POs and their specific line item details.

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.purchase_order.read"
  },
  "import_config": {
    "query": {
      "dir": "desc",
      "fields": [
        "id",
        "created_at",
        "updated_at",
        "po_number",
        "status",
        "version",
        "payment_method",
        "ship_to_attention",
        {
          "ship_to_address": [
            "id",
            "created_at",
            "updated_at",
            "name",
            "location_code",
            "street1",
            "street2",
            "street3",
            "street4",
            "city",
            "state",
            "postal_code",
            "attention",
            "active",
            "business_group_name",
            "vat_number",
            "local_tax_number",
            "type",
            {
              "country": ["id", "code", "name"]
            }
          ]
        },
        {
          "supplier": ["id", "name", "display_name", "number"]
        },
        {
          "order_lines": [
            "id",
            "created_at",
            "updated_at",
            "accounting_total",
            {
              "accounting_total_currency": ["id", "code", "decimals"]
            },
            {
              "custom_fields": ["start_date", "end_date", "payment_method"]
            },
            "description",
            "line_num",
            "order_header_id",
            "order_header_number",
            "price",
            "quantity",
            "source_part_num",
            "status",
            "sub_line_num",
            "total",
            "type",
            "version",
            "supplier_order_number",
            {
              "account": [
                "id",
                "created_at",
                "updated_at",
                "name",
                "code",
                "active",
                "account_type_id",
                "segment_1",
                "segment_2",
                "segment_3",
                "segment_4",
                "segment_5",
                "segment_6",
                "segment_7",
                "segment_8",
                "segment_9",
                "segment_10",
                "segment_11",
                "segment_12",
                "segment_13",
                "segment_14",
                "segment_15",
                "segment_16",
                "segment_17",
                "segment_18",
                "segment_19",
                "segment_20"
              ]
            },
            {
              "currency": ["id", "code", "decimals"]
            },
            {
              "supplier": ["id", "name", "display_name", "number"]
            },
            {
              "uom": [
                "id",
                "created_at",
                "updated_at",
                "code",
                "name",
                "allowable_precision",
                "active"
              ]
            }
          ]
        }
      ],
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
    },
    "method": "update",
    "id_keys": ["id"],
    "endpoint": "api/purchase_orders",
    "dataset_name": "COUPA_DEV_purchase_orders_v1",
    "records_per_request": 50
  }
}
```

### Purchase order - Line items

With this import configuration, you will receive a final collection that includes all PO line items, along with basic information about the associated Purchase Order for each line item.

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.purchase_order.read"
  },
  "import_config": {
    "query": {
      "dir": "desc",
      "fields": [
        "id",
        "created_at",
        "updated_at",
        "accounting_total",
        {
          "accounting_total_currency": ["id", "code", "decimals"]
        },
        {
          "custom_fields": ["start_date", "end_date", "payment_method"]
        },
        "description",
        "line_num",
        "order_header_id",
        "order_header_number",
        "price",
        "quantity",
        "source_part_num",
        "status",
        "sub_line_num",
        "total",
        "type",
        "version",
        "supplier_order_number",
        {
          "account": [
            "id",
            "created_at",
            "updated_at",
            "name",
            "code",
            "active",
            "account_type_id",
            "segment_1",
            "segment_2",
            "segment_3",
            "segment_4",
            "segment_5",
            "segment_6",
            "segment_7",
            "segment_8",
            "segment_9",
            "segment_10",
            "segment_11",
            "segment_12",
            "segment_13",
            "segment_14",
            "segment_15",
            "segment_16",
            "segment_17",
            "segment_18",
            "segment_19",
            "segment_20"
          ]
        },
        {
          "currency": ["id", "code", "decimals"]
        },
        {
          "supplier": ["id", "name", "display_name", "number"]
        },
        {
          "uom": ["id", "created_at", "updated_at", "code", "name", "allowable_precision", "active"]
        }
      ],
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
    },
    "method": "update",
    "id_keys": ["id"],
    "endpoint": "api/purchase_order_lines",
    "dataset_name": "COUPA_DEV_purchase_order_lines_v1",
    "records_per_request": 50
  }
}
```

### Suppliers

See: [Suppliers API (/suppliers)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/suppliers-api-(suppliers)>)

:::warning

Do not fetch `remit_to_addresses` directly on the supplier object. Update to the remit-to addresses in Coupa would not re-trigger the differential import and your supplier data would be inconsistent. Instead, use [Supplier remit-to addresses](#supplier-remit-to-addresses).

:::

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.supplier.read"
  },
  "import_config": {
    "query": {
      "fields": [
        "id",
        "created_at",
        "updated_at",
        "name",
        "display_name",
        "number",
        "status",
        "po_email",
        "account_number",
        "tax_id",
        {
          "primary_contact": [
            "id",
            "created_at",
            "updated_at",
            "email",
            "name_prefix",
            "name_suffix",
            "name_additional",
            "name_given",
            "name_family",
            "name_fullname",
            "notes",
            "active",
            "purposes"
          ]
        },
        {
          "primary_address": [
            "id",
            "created_at",
            "updated_at",
            "name",
            "location_code",
            "street1",
            "street2",
            "street3",
            "street4",
            "city",
            "state",
            "postal_code",
            "attention",
            "active",
            "business_group_name",
            "vat_number",
            "local_tax_number",
            "type",
            {
              "country": ["id", "code", "name"]
            },
            {
              "vat_country": ["id", "code", "name"]
            }
          ]
        },
        {
          "remit_to_addresses": [
            "id",
            "created_at",
            "updated_at",
            "remit_to_code",
            "name",
            "street1",
            "street2",
            "street3",
            "street4",
            "city",
            "state",
            "postal_code",
            "active",
            "vat_number",
            "local_tax_number",
            "external_src_ref",
            "external_src_name",
            {
              "country": ["id", "code", "name"]
            }
          ]
        }
      ],
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/suppliers",
    "dataset_name": "COUPA_DEV_suppliers_v1",
    "records_per_request": 50
  }
}
```

### Supplier remit-to addresses

See: [Suppliers API (/suppliers)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/suppliers-api-(suppliers)>)

Use this configuration to import all supplier remit-to addresses. Note that Coupa (currently) doesn't have any API endpoint to fetch all the remit-to addresses directly. Therefore, we fetch all suppliers and get the remit to addresses from there. It is important to do it in a separate import job (not together with the supplier data) to make sure that `last_modified_date` works correctly.

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.supplier.read"
  },
  "import_config": {
    "query": {
      "fields": [
        "id",
        "created_at",
        "updated_at",
        "name",
        "display_name",
        {
          "remit_to_addresses": [
            "id",
            "created_at",
            "updated_at",
            "remit_to_code",
            "name",
            "street1",
            "street2",
            "street3",
            "street4",
            "city",
            "state",
            "postal_code",
            "active",
            "vat_number",
            "local_tax_number",
            "external_src_ref",
            "external_src_name",
            {
              "country": ["id", "code", "name"]
            }
          ]
        }
      ],
      // highlight-start
      "order_by": "created_at",
      "remit-to-addresses[updated-at][gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/suppliers",
    "dataset_name": "COUPA_TEST_suppliers_remit_to_addresses_v1",
    "records_per_request": 50
  }
}
```

### Supplier information

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.supplier.read"
  },
  "import_config": {
    "query": {
      "fields": [
        "id",
        "created_at",
        "updated_at",
        "supplier_id",
        "name",
        "display_name",
        "supplier_number",
        "status",
        "industry",
        "inco_terms",
        "minority_indicator",
        "minority_type_id",
        "tax_region",
        "tax_classification",
        "federal_tax_num",
        "social_security_number",
        "duns_number",
        "tax_exempt_other_explanation",
        "income_type",
        "fed_reportable",
        "intl_tax_num",
        "intl_tax_classification",
        "intl_other_explanation",
        "backend_system_invoicing",
        "backend_system_catalog",
        "supplier_region",
        "payment_terms_id",
        "payment_term_id",
        "govt_agency_interaction_indicator",
        "govt_agency_interaction",
        "organization_type",
        "policy_for_bribery_corruption_indicator",
        "policy_for_bribery_corruption",
        "govt_allegation_fraud_bribery_indicator",
        "govt_allegation_fraud_bribery",
        "third_party_interaction_indicator",
        "third_party_interaction",
        "goods_services_provided",
        "pay_group",
        "invoice_amount_limit",
        "hold_payment_indicator",
        "hold_payment",
        "separate_remit_to",
        "comment_source",
        "comment",
        "last_exported_at",
        "logo_file_name",
        "logo_content_type",
        "logo_file_size",
        "logo_updated_at",
        "website",
        "allow_cxml_invoicing",
        "hold_invoices_for_ap_review",
        "send_invoices_to_approvals",
        "allow_inv_no_backing_doc_from_connect",
        "allow_inv_unbacked_lines_from_connect",
        "commodity_id",
        "invoice_matching_level",
        "shipping_term_id",
        "tax_code_id",
        "savings_pct",
        "allow_inv_from_connect",
        "allow_inv_choose_billing_account",
        "invoice_inbound_emails",
        "default_invoice_email",
        "inbound_invoice_domain",
        "duplicate_exists",
        "estimated_spend_amount",
        "currency_id",
        "user_id",
        "one_time_supplier",
        "scope_three_emissions",
        "po_email",
        "po_method",
        "po_change_method",
        "buyer_hold",
        "cxml_url",
        "cxml_domain",
        "cxml_identity",
        "cxml_supplier_domain",
        "cxml_supplier_identity",
        "cxml_secret",
        "cxml_protocol",
        "cxml_ssl_version",
        "disable_cert_verify",
        {
          "custom_fields": [
            "primary_subsidiary",
            "subs_secondary",
            "vendor_region",
            "payment_file_format",
            "company_registration_number",
            "eligibility_1099",
            "critical_vs_not_critical_vendor",
            "permanent_account_number_pan",
            "msmeudyam_registration_number",
            "credit_card_vendor",
            "credit_card",
            "vendor_require_risk_questionnaire",
            "risk_review",
            "expedited_vendor",
            "msme_registration_certificate",
            "additional_currency",
            "payment_remittance_email",
            "wil_be_sued_for_po__backed_purchases"
          ]
        },
        {
          "supplier_information_addresses": [
            "intermediary_bank_name",
            "bank_address",
            "bank_city",
            "bank_state_region",
            "bank_postal_code",
            "name_on_bank_account",
            "bank_name",
            "bank_account_number",
            "bank_routing_number",
            "wire_routing_number",
            "international_bank_account_number",
            "iban_number",
            "sort_code",
            "swift_code",
            "bsb_number",
            "bic",
            "bank_code",
            "ifsc",
            "transit_number_and_institution_number",
            "bic_routing_code",
            "payment_method_item",
            "active",
            "csp_rta_id",
            "email",
            "virtual_card_email"
          ]
        }
      ],
      "updated-at[gt_or_eq]": "${last_modified_date}"
    },
    "method": "update",
    "id_keys": ["id"],
    "endpoint": "api/supplier_information",
    "dataset_name": "COUPA_DEV_supplier_information_v1",
    "records_per_request": 50
  }
}
```

### Tax Codes

See: [Tax Code API](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/tax-registrations-api-(tax_registrations)/tax-code-api>)

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.common.read"
  },
  "import_config": {
    "query": {
      // highlight-start
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/tax_codes",
    "dataset_name": "tax_codes",
    "records_per_request": 50
  }
}
```

### Tax Registrations

See: [Tax Registrations API (/tax_registrations)](<https://compass.coupa.com/en-us/products/product-documentation/integration-technical-documentation/the-coupa-core-api/resources/reference-data-resources/tax-registrations-api-(tax_registrations)>)

:::tip

Query attributes necessary for differential update are highlighted.

:::

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "…",
    "client_scope": "core.invoice.read"
  },
  "import_config": {
    "query": {
      "fields": [
        "id",
        "created_at",
        "updated_at",
        "number",
        "owner-id",
        "owner-type",
        "active",
        "local",
        {
          "country": ["id", "code", "name"]
        }
      ],
      // highlight-start
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
      // highlight-end
    },
    // highlight-start
    "method": "update",
    "id_keys": ["id"],
    // highlight-end
    "endpoint": "api/tax_registrations",
    "dataset_name": "COUPA_DEV_tax_registrations_v1",
    "records_per_request": 50
  }
}
```

### Units of measurement

```json
{
  "credentials": {
    "client_id": "…",
    "base_api_url": "….coupacloud.com/",
    "client_scope": "core.common.read"
  },
  "import_config": {
    "query": {
      "fields": [
        "id",
        "name",
        "code",
        "allowable-precession",
        "active",
        "updated-at",
        "created-at"
      ],
      "order_by": "created_at",
      "updated-at[gt_or_eq]": "${last_modified_date}"
    },
    "method": "update",
    "id_keys": ["id"],
    "endpoint": "api/uoms",
    "dataset_name": "COUPA_DEV_uoms_v1",
    "records_per_request": 50
  }
}
```
---
title: 'Coupa: Export configuration'
sidebar_position: 3
sidebar_label: 'Export configuration'
---

import WebhookEndpoints from '../\_webhook_endpoints.md';

# Export configuration

## Setup

Create webhook as described in [Integration Setup](./integration-setup.md#configuring-rossum) and use the right link from the table below (according the Rossum environment of configured account)

### Export endpoints

Coupa exports use [Custom format templating](../export-pipeline/custom-format-templating.md) from [Export pipelines](../export-pipeline/index.md) and therefore doesn't have any Coupa-specific URL. Instead, use the [Custom format templating](../export-pipeline/custom-format-templating.md) URLs from the table below:

<WebhookEndpoints
  eu1="https://elis.custom-format-templating.rossum-ext.app/"
  eu2="https://shared-eu2.custom-format-templating.rossum-ext.app/"
  us="https://us.custom-format-templating.rossum-ext.app/"
  jp="https://shared-jp.custom-format-templating.rossum-ext.app/"
/>

This template then must be sent to the Coupa REST API which can be achieved using the [REST API export](../export-pipeline/rest-api-export.md) extension (also part of the generic "Export pipelines" mechanism):

<WebhookEndpoints
  eu1="https://elis.rest-api-export.rossum-ext.app/"
  eu2="https://shared-eu2.rest-api-export.rossum-ext.app/"
  us="https://us.rest-api-export.rossum-ext.app/"
  jp="https://shared-jp.rest-api-export.rossum-ext.app/"
/>

Optional secrets schema configuration:

```json
{
  "type": "object",
  "properties": {
    "client_secret": {
      "type": "string",
      "minLength": 1,
      "description": "API OAuth Client secret"
    }
  },
  "additionalProperties": false
}
```

## Configuration examples

### Invoice & Credit Note

```json
{
  "export_configs": [
    {
      "content_encoding": "utf-8",
      "export_reference_key": "coupa_invoice_draft",
      "file_content_template_multiline": [
        "{",
        "  \"currency\": {",
        "    \"code\": \"{{ field.currency }}\"",
        "  },",
        "  \"supplier\": {",
        "    \"number\": \"{{ field.sender_match }}\"",
        "  },",
        "  {% if field.document_type == \"credit_note\" %}",
        "  \"document-type\": \"Credit Note\",",
        "  {% else %}",
        "  \"document-type\": \"Invoice\",",
        "  {% endif %}",
        "  \"taggings\": [",
        "    {",
        "      \"tag\": {",
        "        \"name\": \"{{ field.rossum_tag }}\"",
        "      }",
        "    }",
        "  ],",
        "  \"gross-total\": \"{{ field.amount_total | default(0,true) }}\",",
        "  \"account-type\": {",
        "    \"id\": \"{{ field.recipient_export }}\"",
        "  },",
        "  \"invoice-date\": \"{{ field.date_issue }}\",",
        "  \"invoice-number\": \"{{ field.document_id_manual }}\",",
        "  {% if field.payment_terms_match != \"\" %}",
        "  \"payment-term\": {",
        "    \"id\": \"{{ field.payment_terms_match }}\"",
        "  },",
        "  {% endif %}",
        "  \"total-with-taxes\": \"{{ field.amount_total | default(0,true) }}\",",
        "  \"line-level-taxation\": true,",
        "  \"original-invoice-date\": \"{{ field.original_date_issue }}\",",
        "  \"original-invoice-number\": \"{{ field.original_document_id }}\",",
        "  {% if field.document_type == \"credit_note\" %}",
        "  \"is-credit-note\": \"true\",",
        "  {% else %}",
        "  \"is-credit-note\": \"false\",",
        "  {% endif %}",
        "  \"invoice-lines\": [",
        "  {% for item in field.line_items %}{",
        "   \"uom\": {",
        "     \"code\": \"{{ item.item_uom_export }}\"",
        "   },",
        "   \"price\": {{ item.item_price_export | default(0,true) }},",
        "   \"currency\": {",
        "     \"code\": \"{{ field.currency }}\"",
        "   },",
        "   \"type\": \"InvoiceQuantityLine\",",
        "   \"quantity\": \"{{ item.item_quantity_export }}\",",
        "   \"po-number\": \"{{ item.item_order_id_calculated }}\",",
        "   \"description\": \"{{ item.item_description|e | replace('\n',' ') }}\",",
        "   \"order-line-num\": \"{{ item.item_po_line_number_match }}\",",
        "   \"order-header-num\": \"{{ item.item_order_id_calculated }}\",",
        "   \"tax-lines\": {",
        "     \"tax-line\": {",
        "       \"type\": \"TaxLine\",",
        "       {% if item.item_tax_code_match != \"\" %}",
        "         \"tax-code\": {",
        "           \"id\": \"{{ item.item_tax_code_match }}\"",
        "         },",
        "       {% endif %}",
        "       \"amount\": \"{{ item.item_tax_calculated }}\",",
        "       \"rate\": \"{{ item.item_rate_calculated }}\"",
        "     }",
        "   }",
        "  }{% if not loop.last %},{% endif %}",
        "  {% else %}",
        "  {",
        "   \"uom\": {",
        "     \"code\": \"{{ field.uom_export }}\"",
        "   },",
        "   \"price\": {{ field.price_export | default(0,true) }},",
        "   \"currency\": {",
        "     \"code\": \"{{ field.currency }}\"",
        "   },",
        "   \"type\": \"InvoiceQuantityLine\",",
        "   \"quantity\": \"{{ field.quantity_export }}\",",
        "   \"po-number\": \"{{ field.order_id_calculated }}\",",
        "   \"description\": \"{{ field.description_export|e | replace('\n',' ') }}\",",
        "   \"order-line-num\": \"{{ field.po_line_number_match }}\",",
        "   \"order-header-num\": \"{{ field.order_id_calculated }}\",",
        "   \"tax-lines\": {",
        "     \"tax-line\": {",
        "       \"type\": \"TaxLine\",",
        "       {% if field.tax_code_match != \"\" %}",
        "         \"tax-code\": {",
        "           \"id\": \"{{ field.item_tax_code_match }}\"",
        "         },",
        "       {% endif %}",
        "       \"amount\": \"{{ field.amount_total_tax_calculated }}\",",
        "       \"rate\": \"{{ field.tax_rate_calculated }}\"",
        "     }",
        "   }",
        "   }",
        "  {% endfor %}]",
        "}"
      ]
    }
  ]
}
```
---
title: 'Coupa'
sidebar_position: 1
---

:::warning

This Coupa section is about a new **Coupa API Integration**, not the older SFTP based one **Coupa Integration Service**.

:::

## Coupa API Integration architecture

![Coupa API Integration architecture](img/coupa-api-integration-architecture.png)

## Usage

1. First of all, you need to [setup the integration](./integration-setup.md) with Coupa
2. Then you probably want to create some [imports](./import-configuration.md)
3. Setup **Data Matching** (TBD)
4. Setup **Business Rules** (TBD)
5. After that there is a right time to also setup [exports](./export-configuration.md)
6. It is always good to know, which [OAuth 2.0](./oauth-scopes.md) is needed for corresponding API call
7. For initial testing or debugging the Coupa API, the [Postman Colection](./postman-collection.md) will be usefull
8. We have also a nice example of [Workflow with Coupa](./workflow-example.md)
---
title: 'Coupa: Workflow example'
sidebar_position: 4
sidebar_label: 'Workflow example'
---

# Workflow example

![Coupa Workflow Example](./img/coupa-workflow-example.png)
---
title: 'Coupa: OAuth 2.0 scopes'
sidebar_position: 6
sidebar_label: 'OAuth 2.0 scopes'
---

# OAuth 2.0 scopes

_OAuth Scopes and Associated Permissions_

## 1. core.accounting.read
### 1.1 API/Account Groups
- Index – Query account groups
- Show – Show account groups

### 1.2 API/Account Types
- Index – Query COAs
- Show – Show COAs

### 1.3 API/Account Validation Rules
- Index – Query account validations
- Show – Show account validations

### 1.4 API/Accounts
- Favorites – Account favorites
- Index – Query accounts
- List – List accounts
- Recent – Recent accounts
- Show – Show accounts
- User Accounts – Accounts for the user

## 2. core.accounting.write
### 2.1 API/Account Groups
- Create – Create account group
- Update – Update account group

### 2.2 API/Account Types
- Copy – Copy/Clone existing COA
- Create – Create COA

### 2.3 API/Account Validation Rules
- Create – Create account validation
- Update – Update account validation

### 2.4 API/Accounts
- Create – Create account
- Update – Update account
- Validate account – Validate account

## 3. core.approval.read
### 3.1 API/Approvals
- Approver Search – Allows autocomplete when searching for approvers or watchers
- Index – Query approvals
- Show – Show approvals

### 3.2 API/Average Approval Times
- Index – Average approval times

## 4. core.approval.write
### 4.1 API/Approvals
- Approve – Perform approve action on an approval
- Create – Create approval
- Hold – Perform hold action on an approval
- Reject – Perform reject action on an approval
- Update – Update approval

## 5. core.budget.read
### 5.1 API/Budget Line Adjustments
- Index – Query budget line adjustments
- Show – Show budget line adjustments

### 5.2 API/Budget Lines
- Index – Query budget lines
- Show – Show budget lines

## 6. core.budget.write
### 6.1 API/Budget Line Adjustments
- Create – Create budget line adjustments

### 6.2 API/Budget Lines
- Adjust – Adjust budget lines
- Create – Create budget lines
- Update – Update budget lines

## 7. core.business_entity.read
### 7.1 Business Entities
- Index – List business entities
- Show – Show a business entity

## 8. core.business_entity.write
### 8.1 Business Entities
- Create – Create a new business entity
- Update – Update a business entity

## 9. core.catalog.read
### 9.1 API/Items
- Image – Send item image via API
- Index – Query items
- Show – Show items

### 9.2 API/Supplier Items
- Catalogue Item Info – Show supplier ID and item count for the supplier
- Catalogue Items – Show supplier items with fields from the item loader
- Index – Query supplier items
- Search – Advanced supplier item search
- Show – Show supplier items

## 10. core.catalog.write
### 10.1 API/Supplier
- Create – Create supplier item
- Destroy – Delete supplier item
- Update – Update supplier item

## 11. core.comment.read
### 11.1 API/Comments
- Index – Query comments
- Show – Show comments

## 12. core.comment.write
### 12.1 API/Comments
- Create – Create comment
- Destroy – Ability to delete my own comments
- Update - Ability to edit my own comments

## 13. core.common.read
### 13.1 API/Addresses
- Index – Query addresses
- Show – Show addresses

### 13.2 API/Announcements
- Index – Query the announcements page
- Show – Show the announcements page

### 13.3 API/API Docs
- Index – List open API docs
- Show – Retrieve open API docs

### 13.4 API/Business Groups
- Index – Query business groups
- Show – Show business groups

### 13.5 API/Commodities
- Index – Query commodities
- Show – Show commodities

### 13.6 API/Commodity Translations
- Index – Query commodity translations
- Show – Show commodity translations

### 13.7 API/Currencies
- Index – Query currencies
- Show – Show currency

### 13.8 API/Custom Field Attributes
- Index – Query custom field attributes
- Show – Show custom field attributes

### 13.9 API/Departments
- Index – Query departments
- Show – Show department

### 13.10 Diversity Categories
- Index – List diversity categories
- Show – Show diversity categories

### 13.11 API/Exchange Rates
- Index – Query exchange rates
- Show – Show exchange rates

### 13.12 API/Lookup values
- Index – Query lookup values
- Show – Show lookup values
- User Lookup Values – Lookup values for a user

### 13.13 API/Lookups
- Index – Query lookups
- Show – Show lookups

### 13.14 API/Notifications
- Index – Query notifications
- Show – Show notifications

### 13.15 Custom Object Instances
- Index – List of object instances
- Show – Show the object instances

### 13.16 API/Payment Terms
- Index – Query payment terms
- Show – Show payment terms

### 13.17 API/Roles
- Index – Query roles
- Show – Show roles

### 13.18 API/Setup
- Index – Query the company setup information
- Show – Show setup keys

### 13.19 API/Shipping Terms
- Index – Query shipping terms
- Show – Show shipping terms

### 13.20 API/Tax Codes
- Index – Query tax codes
- Show – Show tax codes

### 13.21 API/UOMs
- Index – List of active UOMs
- Show – Show details of a UOM

## 14. core.common.write
### 14.1 API/Addresses
- Create – Create address
- Update – Update addresses

### 14.2 API/Business Groups
- Create – Create a business group
- Update – Update business groups

### 14.3 API/Commodities
- Create – Create a commodity
- Update – Update a commodity

### 14.4 API/ Commodity Translations
- Create – Create commodity translations
- Destroy – Destroy commodity translations
- Update – Update commodity translations

### 14.5 API/Departments
- Create – Create a department
- Update – Update a department

### 14.6 API/Exchange Rates
- Create – Create an exchange rate
- Update – Update an exchange rate

### 14.7 API/Lookup Values
- Create – Create lookup values
- Update – Update lookup values

### 14.8 API/Lookups
- Create – Create lookups
- Update – Update lookups

### 14.9 Custom Object Instances
- Create – Create a new object instance
- Update – Update the object instance

### 14.10 API/Payment Terms
- Create – Create payment terms
- Update – Update payment terms

### 14.11 API/Periods
- Create – Create periods

### 14.12 API/Roles
- Create – Create a new custom role
- Update – Update an existing custom role

### 14.13 API/Shipping Terms
- Create – Create shipping terms
- Update – Update shipping terms

### 14.14 API/Tax Codes
- Create – Create tax codes
- Update – Update tax codes

## 15. core.contract.read
### 15.1 API/Contract Business Groups
- Index – Query a contract’s association to a business group
- Show – Show a contract’s association to a business group

### 15.2 API/Contract Terms
- Index – Query contract terms
- Show – Show contract terms

### 15.3 API/Contract Types
- Index – Search contract types

### 15.4 API/Contracts
- Index – Query contracts
- My Draft Fields – INTERNAL TO COUPA – consumed by Coupa’s MS Word add-in to compare edited document field values with draft field values
- Retrieve Legal Agreement – retrieve legal agreement
- Show – Show contract

### 15.5 API/Contract Templates
- Index – Query contract templates
- Show – Show contract templates

### 15.6 API/Legal Documents
- Index – List legal documents
- Show – show legal documents

### 15.7 CLMA Risk Widget
- Permissions – Check if user has access to risk widget

## 16. core.contract.write
### 16.1 API/Contract Business Groups
- Add- Add a business group to a contract
- Remove – Remove a business group from a contract
- Remove All - Remove all business groups from a contract

### 16.2 API/Contract Terms
- Create – Create a contract term
- Update – Update a contract term

### 16.3 API/Contracts
- Add Approver – Allows the manual addition of approvers to the list of approvals of a contract
- Notify that a signature has been added in CCC – Notifies that a signature has been added in CCC
- Check-in – Check-in a legal document
- Complete – Moves the contract from pending signatures to completed
- Create – Create a contract
- Create Publisher – Create a published contract
- Legal Agreement – Update a legal agreement
- Remove Approval – Allows the removal of a manual approval from the list of approvals of a contract
- Submit for Approval – Submit the contract for approval
- Update – Update a contract
- Update Legal Agreement – Update the legal agreement for completed or published contracts
- Update Completed or Published- Update completed or published contracts
- Moves contract to corresponding status after CCC withdraws signatures - Moves contracts to corresponding status after CCC withdraws signatures

### 16.4 API/Legal Documents
- Add Approver – Add an approver to the approval chain of the legal document
- Attach – Attach a file to a legal document
- Complete – Moves the legal document to the completed status
- Create – Create a legal document
- Remove Approval – Allows the removal of a manual approval from the list of approvals of a legal document
- Submit for Approval – Submits a legal document for approval
- Update – Update a legal document

## 17. core.easy_form_response.approval.write
### 17.1 API/Easy Form Responses
- Add Approver - Allows the manual addition of approvers to the list of approvals of an easy form response
- Remove Approval – Removes a manual approval from the list of approvals of an easy form response

## 18. core.easy_form_response.read
### 18.1 API/Easy Form Responses
- Index – Query form response
- Show – Show form response

## 19. core.easy_form_response.write
### 19.1 API/Easy Form Responses
- Approval – Submit form response for approval
- Review – Review form responses
- Update – Update form responses

## 20. core.expense.read
### 20.1 API/Expense Artifacts
- Image – Upload expense artifact image
- Index – Query expense artifacts
- Show – Show expense artifacts

### 20.2 API/Expense Attendees
- Auto-Complete – Allows autocomplete when searching for attendees
- Recent – Recent attendees
- Show – Show attendee

### 20.3 API/Expense Categories
- Index – Query expense categories
- Milage Metadata – Returns milage related metadata
- Show – Show expense category
- Tax Metadata – Get metadata of tax configurations

### 20.4 API/Expense Category Translations
- Index – Query expense category translations
- Show – Show expense category translations

### 20.5 API/Expense Lines
- Expense Trip Segments Recent Milage – Recent locations for expense mileage
- Index – Query expense lines
- Show – Show expense lines

### 20.6 Expense Per Diem Configs
- Index – View per diem configurations

### 20.7 Expense Per Diem Data
- Index – View per diem rates

### 20.8 API/Expense Polices
- Index – Query expense polices
- Show – Show expense policy

### 20.9 API/Expense Preapprovals
- Index – List expense preapprovals
- Show – Show specified expense preapproval

### 20.10 API/Expense Reports
- Download – Download expense reports
- Index – Query expense reports
- Show – Show expense reports

### 20.11 API/Expense Wallet Lines
- Index – Query to get a user’s expense wallet lines
- Show – Show the expense wallet lines

### 20.12 API/Travel/Expense Artifacts
- Index – List travel artifacts
- Show – Show travel artifacts

## 21. core.expense.secure.read
### 21.1 API/Expense Account Number Lookups
- Index – Translate internal account numbers to real account numbers
- PCI – Handle translations from internal account numbers to real account numbers
- Show – Translate internal account numbers to real account numbers

## 22. core.expense.secure.write
- Nothing as of R34.3.0

## 23. core.expense.write
### 23.1 API/Expense Artifacts
- Create – Create expense artifacts
- Destroy – Destroy expense artifacts
- Update – Update expense artifacts

### 23.2 API/Expense Attendees
- Create – Attach attendee to an expense line
- Deactivate – Deactivate attendee
- Self-Attendee – Returns self-attendee
- Update – Update attendee

### 23.3 API/Expense Categories
- Create – Create expense category
- Destroy – Delete expense category
- Update – Update expense category

### 23.4 API/Expense Category Translation
- Create – Create expense category translation
- Destroy – Delete expense category translation
- Update – Update expense category translation

### 23.5 API/Expense Lines
- Assign Attendee – Assign new attendee to an expense line
- Calculate Mileage Amount – Calculate line amount based on mileage info
- Calculate expense line total based on trip data – Calculate line amount based on trip info
- Calculate Taxes – Allow associating tax lines to expense lines
- Classify – Returns a matching expense category from the expense classification engine
- Create – Create an expense line
- Destroy – Delete an expense line
- Estimate Taxes – Allow estimating taxes for an expense line
- Unassign attendees – Unassign attendee from expense lines
- Update – Update expense lines

### 23.6 API/Expense Policies
- Create – Create an expense policy
- Destroy – Delete an expense policy
- Update – Update an expense policy

### 23.7 API/Expense Reports
- Add Approver – Allows the manual addition of approvers to the list of approvals of an expense report
- Create – Create an expense report in draft status
- Destroy – Ability to delete an expense report
- Export – Mark the expense report as exported
- Remove Approval – Allows the removal of a manual approval from the list of approvals of an expense report
- Sending expense line back to submitter – Sending expense line back to submitter
- Submit – Submit expense report
- Update – Update expense report

### 23.8 API/Expense Wallet Lines
- Create – Ability to add receipts to a wallet
- Destroy – Delete expense wallet lines
- Merge with expense line – Merge wallet lines with expense lines
- Move to expense report – Move wallet lines to an expense report
- Update – Ability to update wallet lines

### 23.9 API/OCR/Expense Artifacts
- Update with OCR response – Update expense artifact with OCR response

### 23.10 API/Travel/Expense Artifacts
- Create Placeholder – Create travel artifact placeholder
- Update – Update travel artifact
- Update with OCR response – Update travel artifact with OCR response

### 23.11 API/Travel/Travel Expense Lines
- Create – Create expense lines from a travel itinerary
- Receipts – Attach receipts to expense lines
- Update – Update travel expense line

## 24. core.financial_counterparty.read
### 24.1 API/Financial Counterparties
- Index – Query financial counterparties
- Show – Show financial counterparties

## 25. core.financial_counterparty.write
### 25.1 API/Financial Counterparties
- Create – Create financial counterparties
- Update – Update financial counterparties

## 26. core.global_navigation.read
- Nothing as of R34.3.0

## 27. core.integration.read
### 27.1 API/Data File Sources
- Index – Query data file sources
- Show – Show data file source

### 27.2 API/Data Sources
- Index – Query data sources
- Show – Show data sources

### 27.3 API/Integration Contacts
- Index – Query integration contacts
- Show – Show integration contacts

### 27.4 API/Integration Errors
- Index – Query integration errors
- Show – Show integration errors

### 27.5 API/Integration History Records
- Index – Query integration history records
- Show – Show integration history records

### 27.6 API/Integration Runs
- Index – Query integration runs
- Show – Show integration runs

### 27.7 API/Integrations
- Index – Query integrations
- Show – Show integrations

## 28. core.integration.write
### 28.1 API/Data File Sources
- Load File – Loads a single file into the request body
- Load from SFTP – Upload flat file and process via data file source
- Load from storage – Allow posting of files

### 28.2 API Integration Contacts
- Create – Create an integration contact
- Update – Update an integration contact

### 28.3 API/Integration Errors
- Create – Create an integration error
- Create alert – Create an integration error alert
- Resolve – Resolve an integration error
- Unresolve – Unresolve an integration error
- Update – Update an integration error

### 28.4 API/Integration History Records
- Acknowledge – The acknowledge action is not documented ... yet
- Create – Create integration history records
- Create Alert – The create_alert action is not documented ... yet
- Create alert and mark export – The create_alert_and_mark_exported action is not documented ... yet
- Mark export – The mark_exported action is not documented ... yet
- Resolve – Resolve integration history records
- Unresolve – Unresolve integration history record
- Update – Update integration history record

### 28.5 API/Integration Runs
- Create – Create an integration run with the status of pending
- Fail – Set an integration to fail
- Finish – Set an integration run status as successful if no errors exist
- Pause – Set an integration run to pause
- Pending – Set an integration run to pending
- Run – Set an integration run to run
- Success – Set an integration run to success
- Update – Update an integration run

### 28.6 API/Integrations
- Create – Create an integration
- Update – Update an integration

## 29. core.inventory.adjustment.read
### 29.1 API Inventory Adjustment
- Index – List inventory adjustments
- Show – Show inventory adjustments

## 30. core.inventory.adjustment.write
### 30.1 API/Attachments
- Create – Create an attachment

### 30.2 API Inventory Adjustment
- Create – Create inventory adjustments
- Index – List inventory adjustments
- Show – Show inventory adjustments
- Update – Update inventory adjustments

## 31. core.inventory.asn.read
### 31.1 API/Advance Ship Notice Lines
- Index – Query advance ship notice lines
- Show – Show advance ship notice lines

### 31.2 API/ASN/Headers
- Export – Export ASN
- Index – List ASN headers
- Show – Show ASN headers

## 32. core.inventory.asn.write
### 32.1 API/ASN/Headers
- Create – Create ASN header
- Receive – Receive ASN header
- Update – Update ASN header

### 32.2 API/ASN/Lines
- Receive ASN lines
- Void ASN lines

## 33. core.inventory.balance.read
### 33.1 API/Asset tags
- Index – Query asset tags
- Show – Show asset tags

### 33.2 API/Inventory
- Index – Query inventory

### 33.3 API/Inventory Balance Lots
- Index – Query inventory balance lot

## 34. core.inventory.common.read
### 34.1 API Default receiving location
- Index – Query the default receiving locations list
- Show – Show a default receiving location

### 34.2 API inspection codes
- Index – Permission to access inspection codes

### 34.3 API/Inventory
- Configurations – Read inventory and receiving configurations
- Show – Show inventory balance

### 34.4 API/Inventory codes
- Index – Allows user to query inventory codes

### 34.5 API warehouse locations
- Index – List warehouse locations
- Show – Show warehouse locations

### 34.6 API/Warehouses
- Index – Query warehouses
- Show – Show warehouses

## 35. core.inventory.common.write
### 35.1 API/Asset tags
- Bulk Update – Bulk updates the asset tags
- Create – Create asset tag
- Update – Update asset tag

### 35.2 API Default Receiving Location
- Create – Create a default receiving location

### 35.3 API Warehouse Locations
- Create – Create warehouse location
- Destroy – Delete warehouse location
- Update – Update warehouse location

### 35.4 API Warehouses
- Create – Create warehouse
- Update – Update warehouse

## 36. core.inventory.consumption.read
### 36.1 API Inventory Consumptions
- Index – List inventory consumptions
- Show – Show inventory consumptions

## 37. core.inventory.consumption.write
### 37.1 API/Attachments
- Create – Create an attachment

### 37.2 API Inventory Consumptions
- Create – Create inventory consumptions
- Index – List inventory consumptions
- Show – Show inventory consumptions
- Update – Update inventory consumptions
- Void – Void inventory consumptions

## 38. core.inventory.cycle_counts.read
### 38.1 Cycle Count Lines
- Index – Query cycle count lines
- Show – Show cycle count lines

### 38.2 API/Cycle Counts
- Index – Query cycle counts
- Show – Show cycle counts

## 39. core.inventory.cycle_counts.write
### 39.1 Cycle Count Lines
- Update – Update cycle count lines via API

### 39.2 API/Cycle Counts
- Create – Create cycle counts
- Submit – Submit cycle counts
- Update – Update cycle counts

## 40. core.inventory.pick_list.read
### 40.1 API/ASN/Lines
- Void – Void ASN lines

### 40.2 API/Pick Lists
- Index – Query pick lists

## 41. core.inventory.pick_list.write
### 41.1 API/Pick Lists
- Update Fulfillments – Update fulfilment pick lists

## 42. core.inventory.receiveing.read
### 42.1 API Receipt Requests
- Index – List receipt requests
- Show – Show receipt requests

### 42.2 API Receiving Purchase Order Lines
- Index – Query the receivable purchase order lines
- Show – Show the receivable purchase order lines

### 42.3 API Inventory Receiving
- Index – List the receiving transactions
- Show – Show the receiving transactions

## 43. core.inventory.receiving.write
### 43.1 API/Attachments
- Create – Create an attachment

### 43.2 API Purchase Order Line Receipt Request
- Create – Create receipt or receipt request for PO lines

### 43.3 API Receipt Requests
- Destroy – Delete the receipt request
- Submit – Submit the receipt request
- Withdraw – Withdraw the receipt request

### 43.4 API Receivable Purchase Order Lines
- Index – Query the receivable purchase order lines
- Show – Show the receivable purchase order lines

### 43.5 API Inventory Receiving
- Create – Create receiving transactions
- Index – Query receiving transactions
- Show – Show receiving transactions
- Update – Update receiving transactions
- Void – Void receiving transactions

## 44. core.inventory.return_to_supplier.read
### 44.1 API Return to Supplier Transactions
- Index – Query return to supplier transactions
- Show – Show return to supplier transactions

## 45. core.inventory.return_to_supplier.write
### 45.1 API Return to Supplier Transactions
- Update – Update return to supplier transaction

## 46. core.inventory.transfer.read
### 46.1 API Inventory Transfer
- Index – Query inventory transfer
- Show – Show inventory transfer

## 47. core.inventory.transfer.write
### 47.1 API/Attachments
- Create – Create attachments

### 47.2 API Inventory Transfer
- Create – Create inventory transfer
- Index – Query inventory transfer
- Show – Show inventory transfer

## 48. core.invoice.approval.bypass
### 48.1 API/Invoices
- Bypass Approvals – Bypass approvals
- Bypass Current Approval – Bypass current approvals

## 49. core.invoice.approval.write
### 49.1 API/Invoices
- Add Approver – Allows the manual addition of approvers to the list of approvals of an invoice
- Remove Approval – Allows the removal of a manual approval from the list of approvals of an invoice
- Restart Approvals – Restart approvals on submit invoices

## 50. core.invoice.create
### 50.1 API/Attachments
- Create – Create attachments

### 50.2 API/Invoices
- Create – Create invoice
- Submit for approval – Submit invoices for approval

## 51. core.invoice.delete
### 51.1 API/Invoices
- Abandon – To abandon the invoice
- Void – Void an approved invoice

## 52. core.invoice.read
### 52.1 API/Invoices
- Index – Query invoice
- Retrieve Clearance Document - Download government-stamped document that was used to create this invoice
- Retrieve Image Scan – Retrieve/download the image scan
- Retrieve/Download Legal Invoice PDF - Retrieve/download the legal invoice PDF
- Show – Show an invoice

### 52.2 API/Matching_allocations
- Index – Query the matching allocations created for tying receipts to various invoice lines
- Show - Show a single matching allocation in detail

### 52.3 API/Payment Receipts
- Retrieve Clearance Document – Download the CFDI document of payment receipt.

### 52.4 API/Remit to Addresses
- Show – Show the remit to address

### 52.5 API/Tax Registrations
- Index – Index tax registrations
- Show – Show tax registrations

## 53. core.invoice.write
### 53.1 API/Invoice Emails
- Update – Update an invoice email

### 53.2 API/Invoices
- Create – Create an invoice
- Dispute – Dispute an invoice
- Export – Allows user to mark an invoice as exported
- Flip to Advance Ship Notice – Flip an invoice to advance ship notice
- Image Scan – Scan image for an invoice
- Revalidate tolerances – Revalidate invoice tolerances
- Submit – Submit an invoice
- Submit for approval – Submit an invoice for approval
- Update – Update an invoice
- Update line accounts – Update invoice line accounts
- Withdraw dispute – Withdraw an invoice dispute

### 53.3 API/Tax Registrations
- Create – Create tax registration
- Update – Update tax registration

## 54. core.item.read
### 54.1 API/Items
- Index – Query an item
- Show – Show an item

## 55. core.item.write
### 55.1 API/Items
- Create – Create an item
- Image – Send an image
- Update – Update an item

## 56. core.legal_entity.read
### 56.1 API/Legal Entities
- Index – Query legal entities
- Show – Show legal entities

## 57. core.legal_entity.write
### 57.1 API/Legal Entities
- Create – Create a legal entity
- Update – Update a legal entity

## 58. core.object_translations.read
### 58.1 Object Translations
- Index – Query all translations
- Show – Show details of a translation

## 59. core.object_translations.write
### 59.1 Object Translations
- Create – Create a translation
- Destroy – Delete a translation
- Update – Update a translation

## 60. core.order_pad.read
### 60.1 API/Order Pad Lines
- Index – Query order pad lines
- Show – Show order pad lines

### 60.2 API/Order Pads
- Index – Query order pads
- Show – Show order pads

## 61. core.order_pad.write
### 61.1 API/Order Pad Lines
- Create – Create order pad lines
- Update – Update order pad lines

### 61.2 API/Order Pad Lines
- Create – Create an order pad
- Update – Update an order pad

## 62. core.pay.charges.read
### 62.1 API Charges
- Index – Query charge objects
- Show – Display specific charge objects

## 63. core.pay.charges.write
### 63.1 API Charges
- Export – Mark charge as exported
- Update – Update the exported flag for a specific charge object

## 64. core.pay.payment_accounts.read
### 64.1 API/Coupa_Pay/Company Payment Accounts
- Index – Query company payment accounts
- Show – Show company payment accounts

### 64.2 API/Coupa_Pay/Employee Payment Accounts
- Index – Query an employee payment accounts
- Show – Show an employee payment accounts

### 64.3 API Payment Partners
- Index – List payment partners
- Show – Show payment partners

### 64.4 API/Coupa_Pay/Supplier Payment Accounts
- Index – Query supplier payment accounts
- Show – Show supplier payment accounts

## 65. core.pay.payments.read
### 65.1 API Coupa Payment Details
- Index – Query payment details
- Show – Show payment details

### 65.2 API Coupa Pay Payments
- Index – Query payments
- Show – Show specific payments

## 66. core.pay.payments.write
### 66.1 API Coupa Pay Payments
- Export – Mark payment as exported
- Update – Update an exported flag for a specific payment object

## 67. core.pay.statements.read
### 67.1 API Coupa Pay Statements
- Index – Query statements
- Show – Show a specific statement

## 68. core.pay.statements.write
### 68.1 API Coupa Pay Statements
- Export – Mark statement as exported
- Updated – Update an exported flag for a specific statement object

## 69. core.pay.virtual_cards.read
### 69.1 API Charges
- Index – Query charge objects
- Show – Show a specific charged object

### 69.2 API Coupa Pay Statements
- Index – Query statements
- Show – Show specific statements

### 69.3 API/Coupa_Pay/Virtual Cards
- Index – Query a list of virtual card details
- Show – Show the details of a specific virtual card

## 70. core.pay.virtual_cards.write
### 70.1 API Charges
- Export – Mark charge as exported
- Update – Update an exported flag for a specific charged object

### 70.2 API/Coupa_Pay/Virtual Cards
- Update – Update a custom field for virtual cards

## 71. core.payables.allocations.read
### 71.1 API Payable Allocations
- Index – Query all allocations

### 71.2 API Invoice Payables
- List payables that are available to be allocated to this one – Show other payables that can be allocated to this invoice

## 72. core.payables.allocations.write
### 72.1 API Payables Allocations
- Create – Create a new allocation
- Export – Mark allocations as exported
- Reverse the payable allocation and associated reconciliation lines – Reverse this allocation and any associated reconciliation lines
- Update – Modify an existing allocation

## 73. core.payables.expense.read
### 73.1 API Payables Expenses
- Index – Query a list of all pay expenses
- Show – Show specific pay expenses

## 74. core.payables.expenses.write
### 74.1 API Payables Expenses
- Export – Mark pay expenses as exported
- Update – Update a specific pay expense

## 75. core.payables.external.read
### 75.1 API External Payables
- Index – Query external payables
- Show – Show external payables

## 76. core.payables.external.write
### 76.1 API External Payables
- Create – Create external payables
- Update – Update external payables
- Void – Void external payables

## 77. core.payables.invoice.read
### 77.1 API/Coupa_Pay/Invoices
- Export – Mark a Coupa Pay invoice as exported
- Index – Query a list of Coupa Pay invoice records
- Show – Show a single Coupa Pay invoice record

### 77.2 API Invoice Payables
- List payables that are available to be allocated to this one - Show other payables that can be allocated to this invoice
- Index – Query all invoice payables
- Show – Show invoice payables

## 78. core.payables.invoice.write
### 78.1 API/Coupa_Pay/Invoices
- Export – Mark a Coupa Pay invoice as exported
- Update – Update a Coupa Pay invoice record

### 78.2 API Invoice Payables
- Exported – Mark this invoice payable exported
- Mark this document as paid and stop tracking it in Coupa – Mark as paid and stop tracking in Coupa
- Stop tracking this document in Coupa – Stop tracking in Coupa
- Resume tracking this document in Coupa – Resume tracking in Coupa

## 79. core.payables.order.read
### 79.1 API Payables Order Reconciliation Lines
- Index – Query pay order reconciliation lines
- Show – Show an individual pay order reconciliation lines

### 79.2 API Payables Orders
- Index – Query a list of all pay orders
- Show – Show a specific pay order

## 80. core.payables.order.write
### 80.1 API Payables Order Reconciliation Lines
- Create - Create new order reconciliation lines

### 80.2 API Payables Orders
- Export – Mark Coupa Pay orders as exported
- Mark this document as paid and stop tracking it in Coupa – Allow API users to mark Coupa Pay orders as paid externally
- Allow API users to mark Coupa Pay orders as ready to pay – Allow API users to mark Coupa Pay orders as ready to pay

## 81. core.project.read
### 81.1 API/Project Memberships
- Index – Query project memberships
- Show – Show a project membership

### 81.2 API/Project
- Index – Query a project
- Show – Show a project

### 81.3 API/Tasks
- Index – Query tasks
- Show – Show a task

## 82. core.project.write
### 82.1 API/Project Memberships
- Create – Create a project membership
- Destroy – Delete a project membership
- Update – Update a project membership

### 82.2 API/Projects
- Cancelled – Update status to canceled
- Complete – Update status to complete
- Create – Create a project
- Draft – Update status to draft
- In progress - Update status to in progress
- Planned - Update status to planned
- Update – Update a project

### 82.3 API/Tasks
- Create – Create a task
- Destroy – Delete a task
- Update – Update a task

## 83. core.purchase_order.read
### 83.1 API/Purchase Order Changes
- Changes - Returns only the differences on the PO Change compared to the previous version of the PO
- Index – Query purchase order changes
- Show – Show purchase order changes

### 83.2 API/Purchase Order Lines
- Index – Query purchase order lines
- Show – Show purchase order lines

### 83.3 API/Purchase Order Revisions
- Index – Query purchase order revisions
- Show – Show purchase order revisions

### 83.4 API/Purchase Orders
- Index – Query purchase orders
- List – List purchase orders
- Show – Show purchase orders

### 83.5 Purchase Orders
- Index – Query all POs pending receipt for the current user
- Show - Shows details of a single PO that’s pending receipt for the current user

### 83.6 API Receivable Purchase Order Lines
- Index – Query the receivable purchase order lines
- Show – Show the receivable purchase order lines

### 83.7 API/Work Confirmation/Headers
- Index – Query the list of work confirmation headers
- New Supplier Review – Create a supplier review
- Show – Show the work confirmation header

## 84. core.purchase_order.write
### 84.1 API/Purchase Order Changes
- Add Approver – Allows the manual addition of approvers to the list of approvals of a purchase order change
- Create – Create a purchase order change
- Remove Approval – Allows the removal of a manual approval from the list of approvals of a purchase order change
- Submit for Approval – Allows submitting draft purchase order change for approval
- Update – Update a purchase order change

### 84.2 API/Purchase Order Lines
- Create – Create a purchase order line
- Destroy – Delete order lines
- Reopen for invoicing – Allow order lines to be reopened for invoicing
- Reopen for receiving - Allow order lines to be reopened for receiving
- Soft close for invoicing - Allow order lines to be soft closed for invoicing
- Soft close for receiving - Allow order lines to be soft closed for receiving
- Update – Update a purchase order line

### 84.3 API/Purchase Order Revisions
- Update – Update a purchase order revision

### 84.4 API/Purchase Orders
- Cancel – Cancel the purchase order
- Close – Close the purchase order
- Create – Create the purchase order
- Export – Mark a purchase order as exported
- Ignore window and issue – Ignore windows and issue the purchase order
- Issue – Issue a purchase order to the supplier
- Issue without send – Issue the purchase order but don’t send it to the supplier
- Reopen – Reopen a soft closed purchase order
- Update – Update a purchase order

### 84.5 API/Work Confirmation/Headers
- New supplier review – Create a supplier review

## 85. core.requisition.read
### 85.1 API/Group Buying Memberships
- Recently added members – Returns recently added users and groups

### 85.2 API/Requisition Lines
- Index – Query requisition lines
- Show – Show requisition lines

### 85.3 API/Requisitions
- To display current cart details via API – Returns the current cart for that user or create a new one
- Index – Query requisitions
- Mine – List requisitions created or requested by the user
- Show – Show the requisition

## 86. core.requisition.write
### 86.1 API/Requisition Lines
- Create – Create a requisition line
- Destroy - Delete a requisition line
- Realtime Status Update – Update open buy item with complete item information from the supplier site
- Requisition Line Detail – Return the description of the requisition line item
- Update – Update a requisition line
- Update Alternate Status – Update requisition line item with alternative item status

### 86.2 API/Requisitions
- Add Approver – Allows the manual addition of approvers to the list of approvals of a requisition
- Add to cart – Allows user to add an item in the current cart
- Create – Create a requisition
- Create as cart – Create a requisition in draft status, which will then need to be submitted manually
- Destroy – Delete requisitions
- Export – Mark requisition as an export
- Remove Approval - Allows the removal of a manual approval from the list of approvals of a requisition
- Save for later for API requisitions – Saving current cart as a new requisition
- Set as current cart – Set specified requisition as current cart for the user if possible
- Submit for Approval – Create a requisition and attempt to submit it for approval/buyer action
- Update – Update requisition
- Update requisition and submit for approval – Update requisition and submit for approval
- Update requisition without validation – Update requisition without validation

## 87. core.sourcing.pending_supplier.read
### 87.1 API/Pending Quote Supplier
- Index – Query all pending quote suppliers

## 88. core.sourcing.pending_supplier.write
### 88.1 API/Pending Quote Supplier
- Create – Create a new pending quote supplier

## 89. core.sourcing.read
### 89.1 API/Quote Award Approvables
- Index – Query list of comments on the quote award approvable document
- Show – Show the sourcing award approvable document along with the linked sourcing event

### 89.2 API/Quote Request Approvables
- Index – Query list of comments on the quote request approvable document
- Show – Show the sourcing launch approvable document along with the linked sourcing event

### 89.3 API/Quote Requests
- Index – Query all sourcing events
- Show – Show the details for a specific sourcing event

### 89.4 API/Quote Suppliers
- Show – Show the information about a particular sourcing event

## 90. core.sourcing.response.award.write
### 90.1 API/Quote Responses
- Award – Reward a response
- Remove award – Remove a reward

## 91. core.sourcing.response.read
### 91.1 API/Quote Responses
- Index – Query all responses from the sourcing event
- Show – Show the details for a specific quote response

## 92. core.sourcing.response.write
### 92.1 API/Quote Responses
- Update – Update a quote response

## 93. core.sourcing.write
### 93.1 API/Quote Award Approvables
- Add approver – Add an approver to the sourcing event award approvals
- Bypass approver – Ability to bypass approvals on sourcing event award approvals
- Bypass current approval – Bypass the current approval on a sourcing event award approval
- Create – Create a comment on the quote award approvable document
- Remove and regenerate – Remove self and regenerate sourcing event award approvals
- Remove approval – Remove approver from the sourcing event award approvals
- Withdraw – Withdraw the sourcing event award approval

### 93.2 API/Quote Request Approvables
- Add approver – Add an approver to the sourcing event launch approvals
- Bypass approver – Bypass the approvals on sourcing event launch approvals
- Bypass current approval – Bypass the current approval on sourcing event launch approvals
- Create – Create a comment on the quote request approvable document
- Remove and regenerate – Remove self and regenerate sourcing event launch approvals
- Remove approval – Remove an approver from the sourcing event launch approvals
- Withdraw – Withdraw the sourcing event launch approval

### 93.3 API/Quote Requests
- Create – Create a sourcing event
- Create a sourcing event from the requisition – Create the sourcing event from different sources
- End event – End the sourcing event
- Submit sourcing event to production – Submits the event to production
- Move a sourcing event to a test state – Move the sourcing event to the test state
- Update – Update the specific sourcing event

## 94. core.sourcing_information_sites.read
### 94.1 API/Supplier Information Sites
- Index – Query all sites of specific supplier information
- Show – Show all the attributes of supplier information site of supplier information

## 95. core.sourcing_information_sites.write
### 95.1 API/Supplier Information Sites
- Create - Create supplier information site of specific supplier information
- Destroy – Delete supplier information site of specific supplier information
- Update – Update attributes of a supplier information site of supplier information

## 96. core.supplier_information_tax_registrations.delete
### 96.1 API/Supplier Information Tax Registration
- Destroy – Delete supplier information tax registration

## 97. core.supplier_information_tax_registrations.read
### 97.1 API/Supplier Information Tax Registrations
- Show – Show supplier information tax registration

## 98. core.supplier_information_tax_registrations.write
### 98.1 API/Supplier Information Tax Registrations
- Create – Create supplier information tax registration
- Update – Update supplier information tax registration

## 99. core.supplier_sharing_settings.read
### 99.1 API Supplier Sharing Settings
- Index – Query supplier sharing settings
- Show – Show supplier sharing settings

## 100. core.supplier_sharing_settings.write
### 100.1 API Supplier Sharing Settings
- Create – Create supplier sharing settings
- Destroy – Remove supplier sharing settings
- Update – Update supplier sharing settings

## 101. core.supplier_sites.read
### 101.1 API/Supplier Sites
- Index – Query all sites of a specific supplier
- Show – Show all the attributes of supplier site of suppler

## 102. core.supplier_sites.write
### 102.1 API/Supplier Sites
- Create – Create a supplier site for a specific supplier
- Destroy – Delete a supplier site for a specific supplier
- Update – Update attributes of a supplier site for a specific supplier

## 103. core.supplier.read
### 103.1 API/Invoice Emails
- Index – Query invoice emails
- Show – Show invoice emails

### 103.2 Proxy Supplier
- Index – Query proxy suppliers

### 103.3 API/Remit to Addresses
- Index – Query remit to addresses
- Show – Show remit to address

### 103.4 API/Supplier Business Groups
- Index – Query supplier’s associations to business groups
- Show – Show supplier’s associations to a business group

### 103.5 API/Supplier Information
- Index – Query supplier information
- Show – Show supplier information

### 103.6 Supplier Invites
- Show – Show the status of an initiated background job of multiple supplier invites

### 103.7 API/Supplier Sites
- Index – Query out all sites of a specific supplier
- Show – Show all the attributes of the supplier site of a supplier

### 103.8 API/Supplier
- Index – Query supplier
- Show – Show supplier

## 104. core.supplier.risk_aware.read
### 104.1 Risk Aware Feed
- Index – Query risk aware feed
- Show – Show risk aware feed

## 105. core.supplier.risk_aware.write
### 105.1 Risk Aware Feed
- Create – Create a risk aware feed
- Update – Update risk aware feed

## 106. core.supplier.write
### 106.1 API/Invoice Emails
- Create - Create an invoice email

### 106.2 API/Remit to Addresses
- Create – Create a remit to address
- Destroy – Delete a remit to address
- Update – Update a remit to address

### 106.3 API/Supplier Business Groups
- Add – Add a supplier to a business group
- Remove – Remove a supplier from a business group
- Remove all – Remove a supplier from all business groups

### 106.4 API/Supplier Information
- Create – Create supplier information
- Export – Mark supplier information as exported
- Update – Update supplier information

### 106.5 Supplier Invites
- Create – Start a background job to invite multiple suppliers

### 106.6 API/Supplier Sites
- Create – Create a supplier site for a specific supplier
- Update – Update attributes of a supplier site for a specific supplier

### 106.7 Supplier users Integration Contacts
- Add – Add a supplier user integration contact
- Remove – Remove a supplier user integration contact

### 106.8 API/Suppliers
- Create – Create a supplier
- Create supplier user preferences – Create a supplier user preference
- Sync supplier user locale – Sync a supplier user’s locale
- Update – Update a supplier

## 107. core.uom.read
### 107.1 API/UOMs
- Show – Show details of a UOM

## 108. core.uom.write
### 108.1 API/UOMs
- Create – Create a new UOM
- Update – Update an existing UOM

## 109. core.user_group.read
### 109.1 API/Tasks
- Index – Query tasks
- Show – Show a task

### 109.2 API/User Group Memberships
- Index – Query user group memberships
- Show – Show a user group membership

### 109.3 API/User Groups
- Index – Query user groups
- Show – Show a user group

## 110. core.user_group.write
### 110.1 API/Tasks
- Create – Create a task
- Update – Update a task

### 110.2 API/User Group Memberships
- Create – Create a user group membership
- Destroy – Delete a user group membership
- Update – Update a user group membership

### 110.3 API/User Groups
- Create – Create a user group
- Update – Update a user group

## 111. core.user.read
### 111.1 API/User Addresses
- Index – Query user’s association to addresses
- Show – Show a user’s association to an address

### 111.2 API/User Business Groups
- Index – Query a user’s association to a business group

### 111.3 API/Users
- Avatar show – Show an avatar
- Index – Query user
- Show – Show a user

## 112. core.user.write
### 112.1 API/User Addresses
- Create – Create the user association to an address
- Update – Update a user’s association to an address

### 112.2 API/User Business Groups
- Add – Add a business group
- Remove - Remove a user group

### 112.3 API/Users
- Delete avatar – Delete a user’s avatar
- Avatar update – Update a user’s avatar
- Create – Create a user
- Update – Update a user

## 113. email

## 114. login

## 115. offline_access

## 116. openid

## 117. profile
### 117.1 API/Coupa Messenger
- Message action – To enable the Coupa messenger message

## 118. travel_booking.common.read
- Nothing as of R34.3.0

## 119. travel_booking.search.write
- Nothing as of R34.3.0

## 120. travel_booking.team.read
- Nothing as of R34.3.0

## 121. travel_booking.team.write
- Nothing as of R34.3.0

## 122. travel_booking.trip.read
- Nothing as of R34.3.0

## 123. travel_booking.trip.write
- Nothing as of R34.3.0

## 124. travel_booking.user.read
- Nothing as of R34.3.0

## 125. travel_booking.user.write
- Nothing as of R34.3.0

## 126. treasury.cash_management.delete
### 126.1 API/Treasury/Cash Management
- Cash Flows/Delete – Delete cash flow in the treasury

## 127. treasury.cash_management.read
### 127.1 API/Treasury/Cash Management
- Cash Flows/Index – Query for the cash flow in the treasury

## 128. treasury.cash_management.write
### 128.1 API/Treasury/Cash Management
- Cash Flows/Create – Create a cash flow in the treasury
- Cash Flows/Update – Modify a cash flow in the treasury
---
title: 'Coupa: Integration setup'
sidebar_position: 1
sidebar_label: 'Integration setup'
---

import WIP from '../\_wip.md';

# Integration setup

The following article guides you through the Coupa integration setup. It consists of two mandatory parts: Coupa configuration and Rossum hook configuration.

## Configuring Coupa

The main prerequisite is to have a valid Coupa user with needed permissions (see [OAuth 2.0 scopes](./oauth-scopes.md)). If you don't have a user with needed permission, ask your admin to create it.

:::tip

You can always (also) check the official documentation [OAuth 2.0 Getting Started with Coupa API](https://compass.coupa.com/en-us/products/core-platform/integration-playbooks-and-resources/integration-knowledge-articles/oauth-2.0-getting-started-with-coupa-api)

:::

1. Login to the Coupa web admin at URL `https://[example-company].coupacloud.com`
2. Go to the Setup, search for keyword `oauth` and click the one result **OAuth2/OpenID Connect Clients**

![Coupa: OAuth](img/coupa-setup-1.png)

3. Create a new Client and make sure that you have selected a Grant type with `Client Credentials` option and save it.

![Coupa: create client](img/coupa_create_client_grant_type.png)

4. Find the user prepared for the integration and note the values `Identifier` and `Secrets`. You will need it for every hook setup later

![Coupa: credentials and permissions](img/coupa-setup-2.png)

5. For start, these scopes should work for basic integration `core.accounting.read, core.common.read, core.invoice.create, core.invoice.read, core.invoice.write, core.purchase_order.read, core.supplier.read` - you can copy and paste it exactly like this when using the Postman collection provided by Coupa

## Configuring Rossum

Coupa service (integration) is provided by Rossum.ai in the form of webhook. To start using Coupa (either imports or exports), follow these steps:

### Webhook in UI

1. Login to your Rossum account.
1. Navigate to **Extensions → My extensions**.
1. Click on **Create extension**.
1. Set the following fields:
   1. Name: `Coupa: Import/Export [what]`
   1. Trigger events: `Manual` (later also `Scheduled`)
   1. Extension type: `Webhook`
   1. URL (see [Import configuration](./import-configuration.md) and [Export configuration](./export-configuration.md))
   1. Token owner of the extension (typically a system user)
1. Click **Create the webhook**.
1. Fill `Configuration` (see [Import configuration examples](./import-configuration.md#configuration-examples) or [Export configuration examples](./export-configuration.md#configuration-examples))
1. Fill `Secrets` fields.

### Setting hook secrets schema (optional):

Optional step is it cannot be done via UI (but can be done either via API or via [`prd` deployment tool](../sandboxes/index.md))

```json
{
  "type": "object",
  "properties": {
    "client_secret": {
      "type": "string",
      "minLength": 1,
      "description": "API OAuth Client secret"
    }
  },
  "additionalProperties": false
}
```

As a result of this change, it is easier to update the hook secrets (and to know what is in the secrets). You will see the following value in the hook secrets that can be easily modified:

```json
{
  "client_secret": "__change_me__"
}
```

## Initial test

You can invoke the webhook manually using **curl** (for example):

```bash
curl --location --request POST 'https://[company-example].rossum.app/api/v1/hooks/[hook-id]/invoke' \
--header 'Authorization: token [token]' \
--data ''
```

Then go to the Rossum Extension Logs and observe the content

## Available configuration options

<WIP />
---
title: 'Master data hub: Full dataset replace'
sidebar_label: 'Full dataset replace'
sidebar_position: 5
---

# Full dataset replace

This page describes a procedure for full replacement (the old data is fully replaced by the content of uploaded file) of the data in Master Data Hub dataset using file upload and the web UI.

## Prerequisites

1. File containing the full new data of the dataset.
1. User of Rossum organisation to which the dataset belongs to.
   1. The user has to have the Administrator role assigned.

## Dataset replacement procedure

:::warning

Please be aware the datasets in the Master Data Hub are usually part of the business logic configured in the Rossum system and changes in the datasets **might lead to errors or even block processing of the documents in Rossum**. The structure of the data (list of columns) should not be changed by the replacement unless discussed with the owner of the Data Matching configuration.

:::

First, login into Rossum application.

To open Master Data Hub, use direct URL to the dataset management:

- Rossum EU: https://elis.rossum.ai/svc/master-data-hub/web/management
- Rossum US: https://us.app.rossum.ai/svc/master-data-hub/web/management

You can alternatively navigate to the Extensions tab in Rossum, pick any Master Data Hub extension and click on the Dataset Management button to reach this page:
![Master Data Hub: dataset management screen](./img/full-dataset-replace/dataset-management-screen.png)

In the list of datasets on the left side select the dataset you want to replace and click the "Replace" button in the top right part of the window.

Click "Choose file" and select the file from location on your computer or drag and drop the file directly to the window:

![Master Data Hub: replace dataset modal](./img/full-dataset-replace/replace-dataset-modal.png)

Rossum is automatically interpreting the file and detects the header column. If the header is missing, an alphabetical keys are generated for the columns. If you run into problems, consider turning off the dynamic mode by unchecking the checkbox.

Click the "Replace" button to replace the dataset (the "Replace" button in the bottom right corner of the dialog becomes available after the file is chosen).

The replacement operation was started. You can check its status by clicking on the "Upload Status" button in the bottom left part of the main window. The dataset was replaced when the `replace` operation status is `finished`.

![Master Data Hub: replace dataset modal](./img/full-dataset-replace/operation-status-modal.png)
---
title: 'Master data hub: Using API'
sidebar_label: 'Using API'
sidebar_position: 6
---

# Using API

## Dataset differential update using API

This part describes best practices for implementation of the differential updates of the dataset stored in the Rossum's Master Data Hub (MDH) using the API.

### Recommended implementation

#### General description

The MDH's API for the replication of the datasets is file based and asynchronous. The client sending the file to the endpoint does not get the status of the replication run in the response to the call of the replication endpoint (the replication can take minutes depending on the size of the file). The first call however returns ID of the operation that can be then monitored using other API endpoint and the result of the replication job can be determined based on the operation status.

#### Implementation steps

1. Call [login endpoint](https://elis.rossum.ai/api/docs/#login) of Rossum API to obtain access token that will be used for authentication of the dataset file push call. The username and password is required for this call. There can be an integration user created for this purpose in Rossum.
1. Call the [Dataset Update](https://elis.rossum.ai/svc/data-matching/api/docs#tag/Dataset/operation/update_dataset_api_v1_dataset__dataset_name__patch) endpoint of the MDH API.
   1. It is assumed that the file that you are sending contains records to be upserted (inserted or updated) in the target dataset, and it is therefore necessary to specify the `id_keys` parameter to tell the system how to uniquely identify the record, so it can either update existing or determine that it does not exist and import it.
1. The dataset operations are asynchronous and the API calls will return the ID of the operation in the Location header of the reply.
1. The operation should be monitored after the call by repeated calls (every 30 seconds or so) of the [Get Operation](https://elis.rossum.ai/svc/data-matching/api/docs#tag/Operation/operation/get_operation_api_v1_operation__operation_id__get) endpoint.
1. If the operation finishes successfully the job ends.
1. If the operation fails or takes longer than 30 minutes the API call should be retried (3 retries are recommended).
1. If all retries fail the whole job can be considered failed and should produce an alert in the monitoring system.

## Downloading whole collections using API

It might be useful to download the whole collection to update it locally or to analyze its structure. This is currently not possible via UI but can be achieved using simple [Node.js](https://nodejs.org/en) script. Download it locally, change the token and collection name in the script and run it (no external libraries are needed):

```bash
node download_collection.js
```

Source code:

```js
const fs = require('fs');

const ROSSUM_TOKEN = '__CHANGE_ME__';
const COLLECTION_NAME = '__CHANGE_ME__';
const BASE_URL = 'https://elis.rossum.ai/svc/data-storage/api/v1';
const FIND_ENDPOINT = `${BASE_URL}/data/find`;

async function* downloadCollection(collectionName, token) {
  let startIndex = 0;
  const batchSize = 2500;
  const maxRetries = 5;
  const retryDelay = 5000; // Delay between retries in milliseconds

  while (true) {
    const payload = {
      collectionName: collectionName,
      query: {},
      projection: { _id: 0 },
      skip: startIndex,
      limit: batchSize,
      sort: { _id: -1 },
    };

    const headers = {
      'Content-Type': 'application/json',
      'Accept': 'application/json',
      'Authorization': `Bearer ${token}`,
    };

    let attempts = 0;
    let success = false;
    let data;

    // Retry logic
    while (attempts < maxRetries && !success) {
      try {
        console.log(FIND_ENDPOINT, JSON.stringify(payload));
        const response = await fetch(FIND_ENDPOINT, {
          method: 'POST',
          headers: headers,
          body: JSON.stringify(payload),
        });

        if (!response.ok) {
          throw new Error(`Fetch failed with status ${response.status}`);
        }

        data = await response.json();
        success = true;
      } catch (error) {
        attempts++;
        console.error(`Attempt ${attempts} failed: ${error.message}`);
        if (attempts < maxRetries) {
          await new Promise((resolve) => setTimeout(resolve, retryDelay)); // Wait before retrying
        } else {
          throw new Error('Max retries reached. Aborting.');
        }
      }
    }

    const documents = data.result;

    // Break the loop if no more documents are returned
    if (!documents || documents.length === 0) {
      break;
    }

    // Yield the documents as they are fetched
    yield documents;

    // Update the start index for the next batch
    startIndex += batchSize;
  }
}

(async () => {
  const fileStream = fs.createWriteStream(`${COLLECTION_NAME}.json`);
  fileStream.write('['); // Start the array

  let firstBatch = true;

  for await (const documents of downloadCollection(COLLECTION_NAME, ROSSUM_TOKEN)) {
    if (!firstBatch) {
      fileStream.write(','); // Separate batches with commas
    }
    fileStream.write(JSON.stringify(documents).slice(1, -1)); // Remove the array brackets from each batch
    firstBatch = false;
  }

  fileStream.write(']'); // End the array
  fileStream.end();
})();
```
---
title: 'Master data hub: Dataset matching configuration'
sidebar_label: 'Dataset matching configuration'
sidebar_position: 3
---

# Dataset matching configuration

## How to set up matching configuration

In the "Matching Configuration" part of the extension, you can set up the rules for how data matching works.

![Master Data Hub: matching configuration](./img/mdh-matching-configuration.png)

Start with assigning a name to your configuration. This will help you identify it later.

![Master Data Hub: configuration name](./img/mdh-configuration-name.png)

Link the queues to which you would like to apply the configuration. You can apply the same matching logic to multiple queues. The queues added in "Rossum Store Extension Settings" will be automatically included here. If you want the rules to apply only to specific queues, you can make a selection.

![Master Data Hub: link queues to matching config](./img/mdh-link-queues-to-matching-config.png)

In the next step select the dataset you want to use for matching.

![Master Data Hub: select dataset for matching](./img/mdh-select-dataset-for-matching.png)

Prepare your matching queries. Queries are executed in the given order until they find a result - that means queries after the one that returned a result are not executed. They use MongoDB syntax - [documentation can be found here](https://www.mongodb.com/docs/manual/tutorial/query-documents/). The query must be translated into valid JSON. You can try out your queries on the Dataset Management page.

![Master Data Hub: matching queries](./img/mdh-matching-queries.png)

In the next step, specify how you want to display the results using:

- "Target Schema ID" - to show the schema ID of the field where the matched result will be kept
- "Dataset Key" - to choose which attribute from the dataset should the extension return if a match is found (such as Vendor ID).
- "Label" to define the data and its format that you want to show as a value in the target field. If you don't specify a "Label", the extension will display the value from the Dataset Key.

![Master Data Hub: displaying results](./img/mdh-displaying-results.png)

Choose a default value and its label to be placed in the target annotation field. Can be used for example if no matches are found.

![Master Data Hub: default value](./img/mdh-default-value.png)

Configure the "Result Actions". These actions determine what should happen when zero, one, or multiple matches are found in the dataset based on the specified queries. Use the "Action" field to define the value that will be displayed to the user in different situations.

![Master Data Hub: result action](./img/mdh-result-action.png)

Additionally, you can set up a message that will accompany the displayed result. An "error" message type will halt the automation and prevent annotators from manually confirming the document until the value is added. "Warning" and "info" messages will display the information, but automation will continue and annotators will be able to manually confirm the document.

As a final step, save your configuration. All the changes made in the "Matching configuration" tab will be reflected in the "Configuration" section of the extension. You can also use that field to define the rules.

![Master Data Hub: configuration](./img/mdh-configuration.png)

## How to set up exact matching

Exact data matching means that the value extracted from the document has to match the value from your dataset completely. To make this work, you can use simple 'find' queries. To understand this better, let's look at the example we [discussed earlier](./dataset-management.md).

### Query 1: Finding a vendor record using extracted vendor name

Query:

```json
{ "find": { "Vendor name": "{sender_name}" } }
```

The query checks the "Vendor name" in the dataset and compares it to the value of the "Vendor name" field extracted from the document. To refer to the "Vendor name" field, we used its schema ID - `sender_name`.

In the configuration section we also defined that the value returned from the dataset in case the match is found will be "Vendor ID" (in "Dataset key").

Based on these rules, the extension found an exact match and returned Vendor ID that is now visible in Rossum and stored in the matching enum field.

![Master Data Hub: matching results](./img/mdh-matching-results.png)

### Query 2: Finding a vendor record using extracted vendor VAT number

Query:

```json
{ "find": { "vatNumber": "{sender_vat}" } }
```

This query checks "vatNumber" in the dataset and compares it to the value of extracted vendor VAT number that is referenced here by schema ID of the field that holds extracted vendor VAT number in Rossum - "sender_vat".

To learn more about the Master Data Hub configuration visit [Configuration Examples](./configuration-examples.md).

## How to set up fuzzy matching

The example we discussed covers the basic exact matching where the data needs to match completely. However, our extension offers more options and flexibility. It allows you to create complex rules and use fuzzy matching to match similar values within a certain range of error. Please note that this advanced feature is not yet available in the user interface and would require assistance from our team.

To learn more about the Master Data Hub configuration visit [Configuration Examples](./configuration-examples.md).

## How multiple matching configurations work

You have the flexibility to set up multiple configurations, and each configuration can have multiple queries within it. These configurations are executed in the specific order - from top to bottom as shown both in the UI and JSON.

Within each configuration, the queries are also processed in the top to bottom order. However, if a query produces a result, any subsequent queries won't be triggered.

These configurations pass the values from one to the next. For example, suppose the Vendor matching configuration is the first in the configuration list. If it finds a Vendor ID and stores it in a field called "vendor_match", you can use this "vendor_match" value in the next matching configuration. The value obtained from the previous query will be immediately available for use.
---
title: 'Master data hub: Dataset management'
sidebar_label: 'Dataset management'
sidebar_position: 2
---

# Dataset management

## How to prepare your dataset

Having high-quality master data is crucial for successful data matching. This means for example a solid list of vendors that includes all the necessary information like addresses, secondary names, and country of origins. The more data you have, the better the results will be because the matching logic can be tailored to each specific dataset.

## How to upload your first dataset

To upload your first dataset while the extension is active, follow these steps:

1. Go to "Dataset Management."

![Master Data Hub: dataset management](./img/mdh-dataset-management.png)

2. Click on "Upload your first dataset."

![Master Data Hub: upload first dataset](./img/mdh-upload-first-dataset.png)

3. Choose a name for your dataset and select the file you want to upload and click on the "Upload" button.

![Master Data Hub: upload](./img/mdh-upload.png)

Here you can also ask the extension to automatically detect the format of the uploaded file. If the option is not selected a standard file format is assumed (the first line of CSV and XLSX files is used as a header, the expected CSV delimiter is a semicolon, line terminator is `\r\n`, quote character is a double quote.)

If everything is done correctly, you will see a success message. Please note that dataset uploads are asynchronous, which means they won't happen instantly.

![Master Data Hub: upload success message](./img/mdh-upload-success-message.png)

In the bottom left corner of the screen, you'll find an "Upload status" button. Clicking on it will show you the progress of the dataset upload. Once the upload is finished, refresh the page to see the dataset listed in the left column.

![Master Data Hub: upload success message](./img/mdh-upload-status.png)

To add another dataset, simply click on the "Add dataset" button. This allows you to include multiple datasets for matching purposes.

![Master Data Hub: upload success message](./img/mdh-add-dataset.png)

## How to manage your dataset

Once you've uploaded the dataset, you have options to update it, replace it, or even remove it entirely. Moreover, you can also try out queries on the dataset you've uploaded.

### Replace, update or delete the dataset

#### Replace the dataset

This action erases all the information in the dataset and fills it with the new information you provide. If you set an optional attribute "Replace by keys" (where "keys" means an attribute or set of attributes that identifies one or more records in the dataset), it will update entries with matching keys, removes entries that aren't in the new dataset, and adds any new entries.

:::note[Example]

Let's say we have some data already in the dataset - like `{"id" : 1, "name" : "Rossum"}` and `{"id" : 3, "name" : "Rossum UK"}`. Now, the data in the file we're uploading is `{"id" : 1, "name" : "Rossum LTD"}` and `{"id" : 2, "name" : "Rossum Prague"}`.

If we choose to "Replace by keys" using the "id" as the key, after replacing the dataset, we'll end up with `{"id" : 1, "name" : "Rossum LTD"}` and `{"id" : 2, "name" : "Rossum Prague"}`. This happens because the entry with `"id":1` gets updated, the one with `"id":2` is entirely new and gets added, and the entry with `"id":3` is removed since it's not in the new dataset.

:::

#### Update the dataset

For this process to work, you need to set the "Update by keys" parameter (where "keys" means an attribute or set of attributes that identifies one or more records in the dataset). These keys should be unique and make each entry distinguishable. It will update the entries with matching keys and add new entries without matching keys. Everything else that's already there in the dataset stays unchanged.

:::note[Example]

The dataset already contains these records: `[{"id" : 1, "name" : "Rossum"}, {"id" : 3, "name" : "Rossum UK"}]`. And in the uploaded file, you'll find these records: `[{"id" : 1, "name" : "Rossum LTD"}, {"id" : 2, "name" : "Rossum Prague"}]`.

By choosing to "Update by keys" with the key being "id", and then updating the dataset, the outcome will be: `[{"id" : 1, "name" : "Rossum LTD"}, {"id" : 2, "name" : "Rossum Prague"}, {"id" : 3, "name" : "Rossum UK"}]`. This is because the entry with `"id":1` gets updated, the one with `"id":2` is totally new and gets added, and the entry with `"id":3` remains in the dataset since the update process doesn't remove records.

:::

#### Delete

That option is going to remove uploaded dataset.

![Master Data Hub: replace, update, delete buttons](./img/mdh-replace-update-delete.png)

### Testing your queries on uploaded dataset

The "Dataset Management" tab lets you test your queries on the uploaded dataset. When you want to find specific data, you have two options: basic find and aggregation pipeline:

![Master Data Hub: test query](./img/mdh-test-query.png)

#### Basic find

It is a straightforward way to search for data. You can learn more about it [here](https://www.mongodb.com/docs/manual/reference/method/db.collection.find/).

:::note[Example]

Finding a vendor record using the extracted vendor VAT number:

```json
{ "vatNumber": "{sender_vat}" }
```

The left side, labeled as `vatNumber`, represents the attribute in the dataset. On the right side `"{sender_vat}"`, points to a specific ID (schema ID) of the field which value was extracted from the document that the data matching is performed for. During the matching process, `"{sender_vat}"` gets replaced with the actual value stored in the `sender_vat` data point from that document. Imagine we have `sender_vat = "CZ123456"`. In that case, the query would change to `{"vatNumber": "CZ123456"}` (which means `"vatNumber"=="CZ123456"`).

You can create many expressions like this. They will be logically connected using the AND operator into a single requirement. For instance: `{"vatNumber": "{sender_vat}", "country": "{sender_country}"}` would mean both `"vatNumber"` should match `"{sender_vat}"` and `"country"` should match `"{sender_country}"`.

:::

#### Aggregation pipeline

It is a more powerful method for searching and organising the data. You can read about it [here](https://www.mongodb.com/docs/manual/reference/method/db.collection.aggregate/).

You can't always be sure that a VAT number will be present on every document. However, it is a good approach to start by attempting to match vendors using a dependable identifier, such as the VAT number. In situations where the VAT number is missing, you might have to use a method called fuzzy matching to locate the right vendor.

To perform a fuzzy search, `$search` stage needs to be called inside the aggregate pipeline.

:::note[Example]

Finding a vendor record by looking for extracted Sender Name in the dataset:

```json
[
  {
    "$search": {
      "text": {
        "path": ["companyName"],
        "query": ["{sender_name}"],
        "fuzzy": {
          "maxEdits": 1
        }
      }
    }
  },
  {
    "$limit": 3
  },
  {
    "$project": {
      "companyName": 1,
      "internalId": 1,
      "email": 1
    }
  }
]
```

The search checks for `{sender_name}` in the `companyName` field of the dataset. It gives back all the matches, arranged by score starting from the highest (which is probably the best match) and going down. It is good practice to use the `$limit` stage to only get the top X results of this fuzzy matching. Use the `$project` stage to reduce data traffic and speed up the matching - it makes sure that only the listed `{key}: 1` are returned by the query.

You can utilise both of these techniques in the user interface. Just type your query and hit the "Try" button. This will display the top ten results that match your search.

:::

To learn more about the Master Data Hub configuration visit [Configuration Examples](./configuration-examples.md).
---
title: 'Master data hub'
sidebar_position: 1
---

:::info[API documentation]

👉 https://elis.rossum.ai/svc/master-data-hub/api/docs

:::

## What is the Master Data Hub extension

Master Data Hub is like having a helpful assistant that can match important information from your documents, such as vendor names, customer names, addresses, and payment details, with the data you already have in your system or database.

Let's say you receive invoices from different vendors. With Master Data Hub, you can make sure that you only process invoices from vendors that are already in your database (ERP system). It can also help you find the right purchase order connected to an invoice and matches individual items listed on the invoice.

You can easily upload your own data in different formats - supported are `.json`, `.xml`, `.csv`, or `.xlsx` and use advanced searching to match the information extracted from your documents with the uploaded data. It opens up a whole new world of possibilities for accurate data matching!

## How to enable the Master Data Hub extension

### Step 1: Prepare your queues and schemas

To get started, you can begin by identifying the queues that contain the documents requiring this extension. Once identified, the next step is to set up a matching field. This field is a part of your schema and can be found on the validation screen. It displays the identified match or a message if no match is found.

Setting up a matching field is a one-time process per queue. You simply need to add the field in the "Fields to capture" section of the relevant queue settings. You have the flexibility to define the "Label" and unique "ID" for the field according to your preferences. The field must be an enum, as shown in the example below:

```json
{
  "rir_field_names": [],
  "constraints": {
    "required": false
  },
  "default_value": null,
  "category": "datapoint",
  "id": "vendor_match",
  "label": "Vendor ID",
  "type": "enum",
  "options": []
}
```

![Master Data Hub: matching enum](./img/mdh-matching-enum.png)

**Pro Tip**: If you want to use the result of one data matching configuration, in a query of a different matching configuration, and the data type of the subsequent query attribute is a number, you can specify `"enum_value_type" : "number"` attribute in the queue schema.

Real-life example: In the first query, you find a vendor using their VAT ID and get back vendor ID, which is a number. In the following query, you want to find all purchase orders linked to that vendor using the vendor ID. Now, since Rossum keeps all information as strings, you can guide the data matching process to treat the vendor ID as a number by using `"enum_value_type": "number"`. This ensures that the vendor ID from the annotation data is converted to a number before using it in the query to gather the purchase orders.

### Step 2: Activate the Master Data Hub extension in the Rossum Store

To activate the extension, follow these steps in the Rossum application:

1. Click on the "Add-ons" tab at the top of the app
1. Choose the "Rossum store" option to access all available extensions
1. Select the "Data matching v2" extension
1. Click on "Try extension"

![Master Data Hub: try extension](./img/mdh-try-extension.png)

### Step 3: Specify the queues to which you want to add the extension

To configure the extension, navigate to the "Rossum Store Extension Settings" and scroll down until you find the "Queues" section. In this section, you can select the specific queues where you want to use the extension.

Once the extension is activated for a queue, it will be automatically triggered whenever a new document is added to the queue or if a user makes changes to a value specified in any of the matching rules.

![Master Data Hub: select queue](./img/mdh-select-queue.png)

### Step 4: Assign the token owner (optional)

For our clients, this step is not necessary. An account with admin rights will be able to create the extension, and the owner of the extension will automatically be assigned to the account that created it. Therefore, this step is not displayed in the User Interface.  
⚠️ However, if you plan to use a different account later, please ensure that you create the extension with that account, or contact our Support team to reassign ownership, for example, to a special technical user.

For Rossum Employees: Token owner must have an "Admin" role assigned in Rossum. User queries the database and adds the results to annotation data.

![Master Data Hub: token owner](./img/mdh-token-owner.png)

### Step 5: Save the configuration

Once you have made your selections, don't forget to save your changes to ensure they take effect properly.
---
title: 'Master data hub: Configuration examples'
sidebar_label: 'Configuration examples'
sidebar_position: 4
---


TODO: `$graphLookup` example:

1. https://rossum.slack.com/archives/C029SMABXU5/p1752833334822539:

```json
{
  "$graphLookup": {
    "from": "Subsidiary",
    "startWith": "$subsidiaryList.recordRef.internalId",
    "connectFromField": "parent.internalId",
    "connectToField": "internalId",
    "as": "child_subs"
  }
},
{
  "$match": {
    "child_subs.internalId": "{subsidiary_match}"
  }
}
```

2. https://gitlab.rossum.cloud/solution-engineering/customers/nextlane/-/blob/master/production-org/default/hooks/MDH%20(Subsidiary,%20Items,%20Accounts,%20Departments,%20Tax%20Codes,%20%E2%80%A6)_%5B621672%5D.json#L82-87:

```json
                {
                  "$graphLookup": {
                    "as": "__parent_subsidiaries",
                    "from": "NS_Subsidiary_v1",
                    "startWith": "$parent.internalId",
                    "connectToField": "internalId",
                    "connectFromField": "parent.internalId"
                  }
                }
```

# Configuration examples

The following examples are showing commonly used configurations of the Rossum.ai Master Data Hub matching. All of these examples are typically nested in the following config:

```json
{
  "configurations": [
    {
      "name": "…",
      "source": {
        "dataset": "PurchaseOrder_v1",
        // highlight-start
        "queries": [
          // COPY-PASTE THE EXAMPLES HERE
        ],
        // highlight-end
      },
      "default": { … },
      "mapping": { … },
      "result_actions": { … }
    }
  ]
}
```

In most of the cases, the `dataset` key will be static. It can however be dynamic as well:

```json
{
  "configurations": [
    {
      "name": "…",
      "source": {
        "dataset": "PurchaseOrder_{queue_country}_v1"
        // …
      }
    }
  ]
}
```

## Best match with default fallback initial match returns no records

The following example selects a first record (the best match) if the first `$match` query returns any results and keeps empty (`""`) record on second position in the dropdown list. If the first `$match` query returns no results, it selects the empty (`""`) default record and appends all records returned by the last `$unionWith` query.

It essentially allows using `best_match` strategy in all circumstances—i.e., confident and non-confident matching in a single query.

```json
{
  "aggregate": [
    {
      "$match": {
        "Workday_Project_ID": "{item_project}"
      }
    },
    {
      "$setWindowFields": {
        "output": {
          "mainMatch": {
            "$count": {}
          }
        }
      }
    },
    {
      "$unionWith": {
        "coll": "nonexistentcollection", // TODO: no longer works in MongoDB 8.0 (?)
        "pipeline": [
          {
            "$documents": [
              {
                "Name": "Please select",
                "mainMatch": 0,
                "Workday_Project_ID": ""
              }
            ]
          }
        ]
      }
    },
    {
      "$setWindowFields": {
        "output": {
          "mainMatchWithDefault": {
            "$count": {}
          }
        }
      }
    },
    {
      "$match": {
        "$expr": {
          "$cond": {
            "if": {
              "$and": [
                {
                  "$gt": ["$mainMatchWithDefault", "$mainMatch"]
                },
                {
                  "$gt": ["$mainMatchWithDefault", 1]
                }
              ]
            },
            "else": {
              "$eq": [1, 1]
            },
            "then": {
              "$ne": ["$mainMatch", 0]
            }
          }
        }
      }
    },
    {
      "$unionWith": {
        "coll": "workday_project", // TODO: no longer works in MongoDB 8.0 (?)
        "pipeline": [
          {
            "$match": {
              "Workday_Project_ID": {
                "$ne": "{item_project}"
              }
            }
          }
        ]
      }
    }
  ]
}
```

## Count all records in the collection

You can quickly get a total number of records in the whole collection by calling `$count`:

```json
{
  "aggregate": [
    {
      "$count": "total"
    }
  ]
}
```

## Compound queries

Compound queries are very useful when we need to match against multiple attributes and give to each match a different importance. In the following example we use Fibonacci Sequence boosts to [fuzzy match](#fuzzy-match) against XXX, YYY and ZZZ:

```json
{
  "aggregate": [
    {
      "$search": {
        "index": "default",
        "compound": {
          "must": [
            {
              "text": {
                "path": "XXX",
                "query": "{product_code} ", // notice the extra space at the end!
                "score": {
                  "boost": {
                    "value": 8
                  }
                }
              }
            },
            {
              "text": {
                "path": "YYY",
                "query": "{product_name} ", // notice the extra space at the end!
                "score": {
                  "boost": {
                    "value": 5
                  }
                }
              }
            }
          ],
          "should": [
            {
              "text": {
                "path": "ZZZ",
                "query": "{product_label} ", // notice the extra space at the end!
                "score": {
                  "boost": {
                    "value": 3
                  }
                }
              }
            }
          ]
        }
      }
    },
    {
      "$addFields": {
        "__searchScore": {
          "$meta": "searchScore"
        }
      }
    },
    {
      "$match": {
        "__searchScore": {
          "$gt": 30 // Check the resulting `__searchScore` to set some appropriate value
        }
      }
    }
  ]
}
```

## Dummy object

Creating dummy objects can be handy when we need to create some dummy (empty) record on the fly:

```json
{
  "aggregate": [
    {
      "$unionWith": {
        "coll": "__non_existent_collection__",
        "pipeline": [
          {
            "$documents": [
              {
                "__searchScore": -1,
                "zip": "",
                "companyName": "Company Unknown",
                "contactName": ""
              }
            ]
          }
        ]
      }
    }
  ]
}
```

## Exact match

```json
{
  "find": {
    "Vendor name": "{sender_name}"
  }
}
```

The query checks the "Vendor name" in the dataset and compares it to the value of the "Vendor name" field extracted from the document. To refer to the "Vendor name" field, we used its schema ID - `sender_name`.

Even though exact match can be achieved using `find` method instead of `aggregate` (see [below](#exact-match-case-insensitive)), it is still recommended to use `aggregate` because it's often necessary to specify `$project` stage:

```json
{
  "aggregate": [
    {
      "$match": {
        "Vendor name": "{sender_name}"
      }
    },
    {
      "$project": {
        "Supplier Name": 1
      }
    }
  ]
}
```

## Exact match (case-insensitive)

```json
{
  "find": {
    "role_code": {
      "$regex": "^{item_role | re}$",
      "$options": "i"
    }
  }
}
```

The `… | re` filter escapes all regex-special characters with a backslash (`\`). It is highly recommended to use the filter when using the MongoDB's [`$regex`](https://www.mongodb.com/docs/manual/reference/operator/query/regex/). Filters `re` and `regex` are equivalent.

## Exact submatch

Sometimes it is necessary to match an exact substring. This can easily be achieved by using `$regex` like so:

```json
{
  "find": {
    "role_code": {
      "$regex": "^.*{item_role | regex}.*$"
    }
  }
}
```

The `… | regex` filter escapes all regex-special characters with a backslash (`\`). It is highly recommended to use the filter when using the MongoDB's [`$regex`](https://www.mongodb.com/docs/manual/reference/operator/query/regex/). Filters `re` and `regex` are equivalent.

## Fuzzy match

Fuzzy search is a technique used to find strings that are approximately, rather than exactly, a match for a given pattern. It solves the common problem of data entry errors and user variability by delivering relevant results even when the search query contains typos, misspellings, alternate spellings, or abbreviations. This makes applications far more robust and user-friendly, as it helps users find what they are looking for without needing to know the precise and perfect spelling of their search term.

Jump to [Fuzzy search example](#fuzzy-search-example) for the actual fuzzy search query.

### Enhancing Fuzzy Search with Dynamic Scoring

To achieve more accurate and relevant fuzzy search results, we can employ a sophisticated query that goes beyond a simple text search. This approach leverages statistical analysis to dynamically filter results based on the quality of the matches for each specific query. The process unfolds in a multi-stage pipeline.

First, an initial full-text search is performed on the desired field, in this case, "Name". This retrieves a broad set of potential matches. Following this initial search, the query calculates key statistics for the entire result set, including the total number of documents found, the average and maximum search scores, and the standard deviation of these scores. This statistical overview provides a quantitative measure of the overall match quality for the given search term.

Using these statistics, a dynamic score threshold is then calculated. For queries that return a small number of results (two or fewer), this threshold is simply set to 90% of the maximum score. For larger result sets, it is calculated by adding 1.5 times the standard deviation to the average score. This allows the threshold to adapt to the specific distribution of scores for each query, becoming stricter when there are many closely-ranked results and more lenient when matches are less distinct.

Finally, the results are filtered using a dual-threshold mechanism. The first is a static, absolute minimum score that acts as a baseline for relevance. The second is the dynamically calculated threshold. A result must clear both of these hurdles to be included in the final, sorted, and limited result set that is returned to the user.

### Establishing a Baseline for Relevance

While the dynamic threshold is calculated automatically, the static absolute minimum score requires a one-time manual analysis to establish a baseline for what constitutes a potentially relevant result. The goal is to find the "irrelevant ceiling"—the highest score a genuinely irrelevant result is likely to achieve.

To determine this value, you should execute a series of "bad" fuzzy search queries that you know have no good matches in your database. These could be common misspellings of unrelated terms or entirely nonsensical words. By observing the top search scores for these intentionally poor queries, you will find the highest score that these irrelevant results produce. For instance, you might discover that no matter how nonsensical the query, the top result never scores above 3.31. Your absolute minimum threshold should then be set just above this ceiling, such as 3.32, to effectively filter out noise from all future searches.

### The Need for the Absolute Minimum Score: A Gatekeeper Against Noise

The absolute minimum score acts as a crucial gatekeeper to filter out universally poor search results. The dynamic threshold is excellent at identifying the best results relative to the other documents found in a specific search, but it has a potential blind spot: what if the entire set of initial results is garbage?

Think of the dynamic threshold as grading on a curve. If a professor grades on a curve and the highest score on a very difficult exam is 35 out of 100, the curve might make that 35% look like an "A". However, the university might still have a rule that you must score at least 50% on the final to pass the course, regardless of the curve.

The absolute minimum score is that university rule. It establishes a "floor of relevance."

### Example Fuzzy Search Query

The following MongoDB aggregation query implements this dual-threshold fuzzy matching logic. It demonstrates the initial search, the statistical calculations, the dynamic threshold computation, and the final filtering based on both the static and dynamic scores.

...

The following query is recommended for fuzzy matching. It consists of four main building blocks:

1. **Initial Search**: It begins by performing a full-text search on the "Name" field.
2. **Statistical Analysis**: The query then calculates the total number of documents, the average search score, the maximum search score, and the standard deviation of the scores across all matched documents.
3. **Dynamic Threshold Calculation**: A dynamic score threshold is then computed automatically based on the standard deviation and average search score. Note that the statistical analysis is not used for low number of results (≤2).
4. **Dual-Threshold Filtering**: The results are filtered based on a combination of a static minimum score and the calculated dynamic threshold.

While the statistical dynamic threshold is calculated automatically based on the distribution of search scores, the static absolute minimum score must be determined manually. To determine the absolute minimum score, perform the following analysis:

1. **Run "Bad" Queries**: Execute a series of fuzzy search queries that you know have no relevant matches in your database. Use common misspellings or completely unrelated terms.
2. **Observe the Top Scores**: For each of these "bad" queries, inspect the raw `searchScore` of the top result returned by MongoDB.
3. **Find the "Irrelevant Ceiling"**: Note the highest score achieved by any of these known irrelevant results. This score is your "irrelevant ceiling." For example, you might find that no matter how nonsensical your query is, the top result never has a score above 3.31.
4. **Set Your Threshold**: Your absolute minimum score should be set slightly above this ceiling. In our example, a safe absolute minimum might be 3.32. This value now represents the lowest possible score for a result to be considered potentially relevant.

Final query:

### Fuzzy search example

```json
{
  "aggregate": [
    {
      "$search": {
        "text": {
          "path": "Name",
          "query": "{sender_name} " // Notice the extra space at the end (query would fail when sender_name is empty otherwise)!
        },
        "index": "default"
      }
    },
    {
      "$addFields": {
        "__searchScore": { "$meta": "searchScore" }
      }
    },
    {
      "$setWindowFields": {
        "output": {
          "__docCount": { "$count": {} },
          "__avgSearchScore": { "$avg": "$__searchScore" },
          "__maxSearchScore": { "$max": "$__searchScore" },
          "__stdDevSearchScore": { "$stdDevPop": "$__searchScore" }
        }
      }
    },
    {
      "$addFields": {
        "__dynamicSearchScoreThreshold": {
          "$cond": {
            "if": { "$lte": ["$__docCount", 2] },
            "then": { "$multiply": ["$__maxSearchScore", 0.9] },
            "else": {
              "$add": ["$__avgSearchScore", { "$multiply": ["$__stdDevSearchScore", 1.5] }]
            }
          }
        }
      }
    },
    {
      "$match": {
        "__searchScore": { "$gte": 3.32 },
        "$expr": {
          "$gte": ["$__searchScore", "$__dynamicSearchScoreThreshold"]
        }
      }
    },
    {
      "$sort": {
        "__searchScore": -1
      }
    },
    {
      "$limit": 10
    }
  ]
}
```

## HTTP requests

Master Data Hub extension can work not only with the existing database collections, but it can also send HTTP requests. The configuration for HTTP requests is slightly different. The following case shows an example of calling the Rossum API and getting the most urgent document based on the `sla_deadline_utc` field ([API reference](https://elis.rossum.ai/api/docs/#search-for-annotations)):

```json
{
  "configurations": [
    {
      "name": "Find the most urgent document for review (in one specific workspace)",
      "source": {
        "queries": [
          {
            "url": "https://mydomain.rossum.app/api/v1/annotations/search?page_size=1&ordering=field.sla_deadline_utc.string&sideload=content&content.schema_id=sla_deadline_utc",
            "body": {
              "query": {
                "$and": [
                  { "field.sla_deadline_utc.string": { "$emptyOrMissing": false } },
                  { "status": { "$in": ["to_review"] } },
                  {
                    "workspace": { "$in": ["https://mydomain.rossum.app/api/v1/workspaces/123456"] }
                  }
                ]
              }
            },
            "method": "POST",
            "headers": {
              "Content-Type": "application/json",
              "Authorization": "Bearer {payload.rossum_authorization_token}"
            },
            "result_path": ""
          }
        ]
      },
      "default": { "label": "---", "value": "" },
      "mapping": {
        "dataset_key": "content[0].content.value || ''",
        "label_template": "{content[0].content.value || ''}",
        "target_schema_id": "sla_deadline_most_urgent_datetime"
      },
      "result_actions": {
        "no_match_found": { "message": { "type": "error", "content": "No match found" } },
        "one_match_found": { "select": "best_match" },
        "multiple_matches_found": {
          "select": "default",
          "message": { "type": "error", "content": "Multiple matches found" }
        }
      },
      "additional_mappings": [
        {
          "dataset_key": "results[0].id || ''",
          "label_template": "{results[0].id || ''}",
          "target_schema_id": "sla_deadline_most_urgent_id"
        }
      ]
    }
  ]
}
```

Notice the structure of the `queries` source where we are constructing the HTTP request with body and headers. Also notice that it is possible to access the `rossum_authorization_token` token via `payload` variable. If necessary, you can also perform `auth` call in the source like so:

```json
{
  "configurations": [
    {
      "source": {
        // highlight-start
        "auth": {
          "url": "https://elis.rossum.ai/api/v1/auth/login",
          "body": {
            "password": "{secrets.elis_password}",
            "username": "{secrets.elis_username}"
          },
          "method": "POST",
          "headers": { "Content-Type": "application/json" }
        },
        // highlight-end
        "queries": [
          {
            "url": "https://elis.rossum.ai/api/v1/annotations/{annotation_id}/content",
            "method": "GET",
            "headers": {
              "Content-Type": "application/json",
              // highlight-start
              "Authorization": "Bearer {auth.body.key}"
              // highlight-end
            },
            "result_path": "content[?contains(schema_id, 'line_items_section')].children[].children[].children[?contains(schema_id, 'item_po_number')].content[]",

            // Optionally, you can also specify query parameters:
            "query_params": {
              "referenceNumber": "{document_id}"
            }
          }
        ]
      }
      // …
    }
  ]
}
```

Coupa OAuth:

```json
{
  "configurations": [
    {
      "name": "Coupa Invoice Existence Check",
      "source": {
        "auth": {
          "url": "https://rossum-czech-coupalink.coupacloud.com/oauth2/token",
          "method": "POST",
          "headers": {
            "Accept": "application/json",
            "Content-Type": "application/x-www-form-urlencoded"
          },
          "body": {
            "grant_type": "client_credentials",
            "client_id": "dddddd",
            "client_secret": "ssssss",
            "scope": "core.invoice.read"
          }
        },
        "queries": [
          {
            "url": "https://rossum-czech-coupalink.coupacloud.com/api/invoices/",
            "method": "GET",
            "headers": {
              "Authorization": "Bearer {auth.body.access_token}",
              "Accept": "application/json"
            },
            "query_params": {
              "fields":  "[\"id\",\"status\"]",
              "supplier_id": "{sender_name}",
              "invoice_number": "{document_id_manual}"
            },
            "body": {},
            "result_path": ""
          }
        ]
      },
      "default": {
        "label": "---",
        "value": ""
      },
      "mapping": {
        "dataset_key": "status",
        "target_schema_id": "duplicate_invoice_statuses"
      },
      "result_actions": {
        "no_match_found": {
          "message": {
            "type": "info",
            "content": "No duplicates found"
          }
        },
        "one_match_found": {
          "select": "best_match"
        },
        "multiple_matches_found": {
          "select": "default",
          "message": {
            "type": "warning",
            "content": "Multiple duplicate statuses found"
          }
        }
      }
    }
  ]
}
```

## JavaScript in-line functions

:::warning

Even though using JavaScript can be easier in some scenarios, it is typically less performant than using native MongoDB queries. Use this carefully!

:::

```json
{
  "aggregate": [
    // …
    {
      "$addFields": {
        "__order_number_sanitized": {
          "$function": {
            "body": "function(x) { return x.replace(/[^0-9a-z]/ig, '').toLowerCase(); }",
            "args": ["$Order Number"],
            "lang": "js"
          }
        }
      }
    }
    // …
  ]
}
```

## Prioritize preferred results

The following query returns results as usual but sorts the preferred results above the rest:

TODO: add note about sorting by multiple fields (their order and implicit key sorting)

```json
{
  "aggregate": [
    {
      "$match": {
        "active": true,
        "country.code": "{country_code}"
      }
    },
    {
      "$addFields": {
        "__is_preferred_result": {
          "$eq": ["$internalId", "{preferred_result_id}"]
        }
      }
    },
    {
      "$sort": {
        "__is_preferred_result": -1
      }
    }
  ]
}
```

## Remove duplicates (`$group`)

```json
{
  "aggregate": [
    // …
    {
      "$group": {
        "_id": "$vendorRegNo",
        "__tmpRoot": {
          "$first": "$$ROOT"
        }
      }
    },
    {
      "$replaceRoot": {
        "newRoot": "$__tmpRoot"
      }
    },
    // …
    {
      "$sort": {
        "__searchScore": -1 // it is important to sort the results correctly after using $group
      }
    }
  ]
}
```

## Return all collection records (sorted)

Sometimes it might be useful to always return all records and perhaps sort them by matching score. That is, always return everything but on put the best results on top.

This can be achieved by first searching and returning records with their respective `__searchScore` (see [fuzzy match](#fuzzy-match), for example) and later appending all records with zero `__searchScore` using `$unionWith`. Finally, all the results are grouped to remove duplicates and sorted by the score:

```json
{
  "aggregate": [
    // … (fuzzy search first)
    {
      // highlight-start
      "$unionWith": {
        "coll": "legal_entities_v1",
        "pipeline": [
          {
            "$addFields": {
              "__searchScore": 0
            }
          }
        ]
      }
      // highlight-end
    },
    {
      "$group": {
        "_id": "$legal_entity",
        "__tmpRoot": {
          "$first": "$$ROOT"
        }
      }
    },
    {
      "$replaceRoot": {
        "newRoot": "$__tmpRoot"
      }
    },
    {
      "$sort": {
        "__searchScore": -1
      }
    }
  ]
}
```

## Match on normalized values

```json
{
  "aggregate": [
    // …
    {
      "$addFields": {
        "__tax_id_stringified": {
          "$toString": "$Tax ID"
        }
      }
    },
    {
      "$addFields": {
        "__tax_id_normalized": {
          "$map": {
            "input": {
              "$range": [
                0,
                {
                  "$strLenCP": "$__tax_id_stringified"
                }
              ]
            },
            "in": {
              "$substrCP": ["$__tax_id_stringified", "$$this", 1]
            }
          }
        }
      }
    },
    {
      "$addFields": {
        "__tax_id_normalized": {
          "$filter": {
            "input": "$__tax_id_normalized",
            "cond": {
              "$regexMatch": {
                "input": "$$this",
                "regex": "[0-9a-zA-Z]"
              }
            }
          }
        }
      }
    },
    {
      "$addFields": {
        "__tax_id_normalized": {
          "$reduce": {
            "input": "$__tax_id_normalized",
            "initialValue": "",
            "in": {
              "$concat": ["$$value", "$$this"]
            }
          }
        }
      }
    },
    {
      "$match": {
        "__tax_id_normalized": "{sender_vat_id_normalized}"
      }
    }
  ]
}
```

## Match only if there is exactly one match

```json
{
  "aggregate": [
    // …
    {
      "$setWindowFields": {
        "output": {
          "__totalCount": {
            "$count": {}
          }
        }
      }
    },
    {
      "$match": {
        "__totalCount": 1
      }
    }
  ]
}
```

## Combine matching results from different collections (using memoization collection)

```json
{
  "aggregate": [
    // match no record with the dataset specified in the config
    {
      "$match": {
        "_id": "#"
      }
    },
    // append query results to previous results
    {
      "$unionWith": {
        "coll": "_entity_data_acc",
        "pipeline": [
          {
            "$match": {
              "sender_name": "{recipient_name}"
            }
          },
          {
            "$lookup": {
              "as": "original",
              "from": "workday_entity",
              "localField": "entity_wd",
              "foreignField": "Organization_Data.ID"
            }
          },
          {
            "$replaceRoot": {
              "newRoot": {
                "$mergeObjects": [
                  {
                    "$arrayElemAt": ["$original", 0]
                  },
                  "$$ROOT"
                ]
              }
            }
          },
          {
            "$project": {
              "original": 0
            }
          },
          {
            "$addFields": {
              "score": 999
            }
          }
        ]
      }
    },
    // append empty record at the right position to use result_actions.multiple_matches_found: best match
    // this means that if the first $unionWith does not return any results, the first "empty" record will be preselected and thus prevents automation of non confident matching
    {
      "$unionWith": {
        "coll": "nonexistentcollection",
        "pipeline": [
          {
            "$documents": [
              {
                "score": 900,
                "Organization_Data": {
                  "ID": "",
                  "Organization_Name": "Please select ..."
                }
              }
            ]
          }
        ]
      }
    },
    // append records from the main dataset with looser matching query to allow users to select the right match
    {
      "$unionWith": {
        "coll": "workday_entity_acc", // TODO: no longer works in MongoDB 8.0 (?)
        "pipeline": [
          {
            "$match": {}
          },
          {
            "$project": {
              "Organization_Data.ID": 1,
              "Organization_Data.Organization_Name": 1
            }
          },
          {
            "$addFields": {
              "score": 888
            }
          }
        ]
      }
    },
    {
      "$sort": {
        "score": -1,
        "Organization_Data.Organization_Name": 1
      }
    },
    {
      "$project": {
        "Organization_Data.ID": 1,
        "Organization_Data.Organization_Name": 1
      }
    }
  ]
}
```

## Match score steps

```json
{
  "aggregate": [
    // …
    {
      "$setWindowFields": {
        "output": {
          "__score_normalized_max": {
            "$max": "$__score_normalized"
          }
        }
      }
    },
    {
      "$match": {
        "$expr": {
          "$cond": {
            "if": {
              "$or": [
                {
                  "$and": [
                    { "$gt": ["$__score_normalized_max", 0.95] },
                    { "$gt": ["$__score_normalized", 0.95] }
                  ]
                },
                {
                  "$and": [
                    { "$gt": ["$__score_normalized_max", 0.9] },
                    { "$lte": ["$__score_normalized_max", 0.95] },
                    { "$gt": ["$__score_normalized", 0.9] },
                    { "$lte": ["$__score_normalized", 0.95] }
                  ]
                },
                {
                  "$and": [
                    { "$gt": ["$__score_normalized_max", 0.85] },
                    { "$lte": ["$__score_normalized_max", 0.9] },
                    { "$gt": ["$__score_normalized", 0.85] },
                    { "$lte": ["$__score_normalized", 0.9] }
                  ]
                }
              ]
            },
            "then": true,
            "else": false
          }
        }
      }
    }
  ]
}
```

## Unique values

The following function unwinds account localization arrays from NetSuite accounts and returns a comma-separated list of unique accounting context names:

```json
{
  "aggregate": [
    {
      "$unwind": {
        "path": "$localizationsList.accountLocalizations"
      }
    },
    {
      "$group": {
        "_id": "$localizationsList.accountLocalizations.accountingContext.name"
      }
    },
    {
      "$sort": {
        "_id": 1
      }
    },
    {
      "$group": {
        "_id": null,
        "__allAccountingContextNames": {
          "$push": "$_id"
        }
      }
    },
    {
      "$project": {
        "_id": 0,
        "__allAccountingContextNames": {
          "$function": {
            "body": function (names) {
              return names.join(", ");
            },
            "args": ["$__allAccountingContextNames"],
            "lang": "js"
          }
        }
      }
    }
  ]
}
```

## VAT ID checker against external API (VIES)

It is possible to query not only internal datasets, but also external (RESTful) API. For example, you could query the [VIES API for the VAT ID validation](https://ec.europa.eu/taxation_customs/vies/#/vat-validation).

Note that the following configuration requires the existence of two [Formula Fields](../rossum-formulas/formula-fields.md), both to be used in the VIES request body:

1. `sender_vat_id_country_code_calculated` with formula `re.sub(r'\s', '', field.sender_vat_id)[:2]`
1. `sender_vat_id_vat_number_calculated` with formula `re.sub(r'\s', '', field.sender_vat_id)[2:]`

Additional custom fields in the queue schema (such as `vies_is_valid`) to present the result in the UI might also be needed.

![VIES check result example](./img/mdh-vies-example.png)

Complete data matching configuration example (notice the highlighted part showing the actual VIES API request):

```json
{
  "configurations": [
    {
      "name": "VIES API validation",
      "source": {
        "queries": [
          {
            // highlight-start
            "url": "https://ec.europa.eu/taxation_customs/vies/rest-api/check-vat-number",
            "body": {
              "vatNumber": "{sender_vat_id_vat_number_calculated}",
              "countryCode": "{sender_vat_id_country_code_calculated}"
            },
            // highlight-end
            "method": "POST",
            "headers": {
              "Accept": "application/json",
              "Content-Type": "application/json"
            },
            "result_path": ""
          }
        ]
      },
      "default": {
        "label": "Not checked",
        "value": "not-checked"
      },
      "mapping": {
        "dataset_key": "valid",
        "label_template": "{valid}",
        "target_schema_id": "vies_is_valid"
      },
      "result_actions": {
        "no_match_found": {
          "message": {
            "type": "error",
            "content": "No match found"
          }
        },
        "one_match_found": {
          "select": "best_match"
        },
        "multiple_matches_found": {
          "select": "default",
          "message": {
            "type": "warning",
            "content": "Multiple matches found"
          }
        }
      }
    }
  ]
}
```

Some APIs, such as VIES, will return an error if the APIs input data are empty. You can avoid the error by adding an action condition for the particular data matching in configuration. When defined, the matching will be performed only if the condition is evaluated to `true`, otherwise the matching targets will be reset:

```json
{
  // …
  "action_condition": "'{sender_vat_id}' != ''"
  // …
}
```

It is also recommend creating additional [Formula Field](../rossum-formulas/formula-fields.md) that will clear the matching output field if `action_condition` is used with the following formula:

```py
None if is_empty(field.sender_vat_id) else field.vies_is_valid
```

More information about the `additional_mappings` can be found in the Master Data Hub API documentation: https://elis.rossum.ai/svc/master-data-hub/api/docs#tag/Matching-configuration
---
title: 'Business Rules Validation'
sidebar_position: 1
slug: '/learn/business-rules-validation'
---

import Deprecated from '../../\_deprecated.md';

<Deprecated />

Business Rules Validation allows users to perform validations of the extracted and calculated data. It typically runs at the end of the extension chain and the main purpose of it is to prevent confirmations (or block automation) of invalid documents.

<!-- TODO: create a page (guide) describing how does chaining of extensions work! -->

## Installation

Business Rules Validation extension is available in the Rossum store. To install the extension, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → Rossum Store**.
1. Search for **Business Rules Validation** extension and "Add" it.

A default "Rossum Store extension settings" page will open where you can configure the extension to your liking.

## Basic usage

By default, the extension has the following configuration:

```json
{
  "checks": [
    {
      "rule": "has_value({document_id})",
      "message": "Invoice number is mandatory."
    }
  ],
  "variables": {}
}
```

This configuration will ensure that document ID (invoice number) is not empty. Other checks (rules) can be added. Visit [Configuration examples](./configuration-examples.md) page for more examples.

## Available configuration options

:::info

Head over to [Configuration examples](./configuration-examples.md) page for collection of the most common real-world configurations.

:::

```json
{
  // List of business rules. All rules in this list are evaluated in order.
  "checks": [
    {
      // Rule to be evaluated (visit Expression Engine for more details).
      "rule": "has_value({document_id})",

      // Message to be shown if the rule is violated.
      "message": "Invoice number is mandatory.",

      // Type of the message (optional, default: "error").
      "type": "error",

      // Whether the rule violation blocks automation (optional, default: false). Note that error
      // messages automatically block automation.
      "automation_blocker": true,

      // Whether the rule is active (optional, default: true).
      "active": true,

      // List of queue IDs where the rule is active (optional).
      "queue_ids": [123, 456],

      // The condition that has to be met in order for validation to be applied (optional).
      // Do not confuse with the rule. This option only controls whether the rule is applied or not.
      "condition": "has_value({document_id})",

      // An option to automatically converts data types during processing (enabled by default).
      "autocast_types": "true"
    }
  ]
}
```
---
sidebar_position: 1
title: 'Business Rules Validation: Configuration examples'
sidebar_label: 'Configuration examples'
slug: '/learn/business-rules-validation/configuration-examples'
---

import Deprecated from '../../\_deprecated.md';

<Deprecated />

# Configuration examples

Here you can find examples of the most common real-world use cases for Business Rules Validation. Simply copy-paste them into extension settings and adjust as needed.

## Common Accounts Payable (AP) checks

### Invoice face value discrepancy

```json
{
  "checks": [
    {
      "rule": "{amount_total} == {amount_total_base} + {amount_total_tax}",
      "type": "error",
      "message": "Total amount is not equal to the sum of amount base and the tax",
      "automation_blocker": true
    }
  ]
}
```

### Sum of line items must match invoice total

```json
{
  "checks": [
    {
      "rule": "sum({item_amount_total}) == {amount_total}",
      "type": "error",
      "message": "The sum of line items is not equal to the total amount.",
      "automation_blocker": true
    }
  ]
}
```

## At least one field must be filled

The following Business Rules Validation configuration shows errors only when all the values are empty. It is satisfied if at least one of the values is filled.

```json
{
  "checks": [
    {
      "or": [
        {
          "rule": "has_value({aaa})",
          "message": "At least one value has to be filled."
        },
        {
          "rule": "has_value({bbb})",
          "message": "At least one value has to be filled."
        },
        {
          "rule": "has_value({ccc})",
          "message": "At least one value has to be filled."
        }
      ]
    }
  ]
}
```
---
sidebar_position: 2
title: 'Business Rules Validation: Expression Engine'
sidebar_label: 'Expression Engine'
toc_max_heading_level: 4
slug: '/learn/business-rules-validation/expression-engine'
---

import Deprecated from '../../\_deprecated.md';

<Deprecated />

# Expression Engine

Rossum Expression Engine is responsible for validating expressions, producing either a true or false result. Its syntax closely resembles that of Python and allows for utilizing data from annotated documents. The engine assesses conditions and expressions using document data. Currently, it is employed for the Business Rules Validation extension, but there are plans to extend its usage to other extensions in the future. Engine operates at both the header field and line item levels, offering the flexibility to combine them within a single expression.

Examples:

```text
{issue_date} > "2023-01-01"

{item_price} * {item_amount} == {item_total}

sum({item_total}) == {total_price}
```

## Allowed operators

Engine supports basic mathematical and logical operators:

```text
+, -, /, //, *, %, and, or, xor, ==, !=, <, >, <=, >=
```

## Data types

The Expression Engine supports four data types: integer (int), floating-point number (float), string, and date. When performing automatic casting, the engine first attempts to cast the value as a float, followed by an integer, then a date, and finally as a string.

It is important to note that when dealing with dates, the format `YYYY-MM-DD` must be followed, including the quotation marks. Without the quotation marks, a value like 2023-12-24 would be evaluated as a mathematical subtraction resulting in the number 1987.

If you need to ensure a specific data type is used, you can manually cast the value using the following conversions. For whole numbers:

```text
int(<value>)
```

For numbers with decimal digits:

```text
float(<value>)
```

For dates:

```text
date(<value>)
```

For text (this also serves as a catch-all type):

```text
str(<value>)
```

## Empty values

There are two functions to check whether a document data point has a value or is empty. The first one returns `true` if the data point is not empty (`!=''`):

```text
has_value({data_point})
```

The second one returns `true` if the data point is empty (`==''`):

```text
is_empty({empty_point})
```

Important Note: Please be aware that using `==''` and `!=''` for comparison will not work.

The expression engine automatically avoids error messages during the annotation process by skipping all lines in a table that have empty values. Whenever there's an empty value in the rule, it is skipped, including `==''`.

For headers, the skip procedure works accordingly - because there's only "one" line to skip, the whole rule is skipped.

## Aggregation function and empty values

Table column with following values `[1,2,"",2]`:

```text
sum({item_amount})==5
sum({item_amount, default=0})==5
unique_len({item_amount})==2
len({item_amount})==3
len(filter(({item_amount}),""))==3
len({item_amount, default=0})==4
```

Table column with empty values `["","",""]`:

```text
sum({item_amount})==None
unique_len({item_amount})==None
len({item_amount})==None
len(filter(({item_amount}),""))==None
len({item_amount, default=0})==3
```

Not annotated table (defined in schema):

```text
sum({item_amount})==None
sum({item_amount, default=0})==None
unique_len({item_amount})==None
len({item_amount})==None
len(filter(({item_amount}),""))==None
len({item_amount, default=0})==None
```

## Allowed functions

### Aggregation functions

To compare header fields and line items or implement business rules based on line items, the following aggregation functions are at your disposal:

```text
all(<expression>) - all lines in the table must satisfy the business rule in order to pass

any(<expression>) - at least one line in the table must fulfill the business rule in order to pass

sum(<column>) - summary of a table column (applicable to float and int, as well as string values that can be cast to float or int)

min(<column>) - minimum value in a table column

max(<column>) - maximum value in a table column

len(<column>) - number of rows in the table (i.e., the length of the row list)

unique_len(<column>) - number of unique values in a table column

first_value(<column>) - first value of a table column, often used in conjunction with a filter to exclude empty values first_value(filter(<column>,["",None]))
```

### Filter function

A filter function that removes all values in the second parameter:

```text
filter(<value or column>[, <remove_value>])
```

Example:

```text
filter({item_price}, [0,None])
```

### Default value functions

```text
{<value or column>, default=<default_value>} - basic default value function. Value extracted from the document cannot be used as a default value using this syntax.

{<value or column>, default=value(‘<other_data_point>’)} - with this setup you can use a data point as a default value.
```

Example: `{vendor, default="Rossum"}` or `value({item_delivery_date}, default={delivery_date})`

### Date functions

```text
today() - today’s date, determined using Coordinated Universal Time (UTC), which is the standard time used in all Rossum solutions

timedelta(<shift>) - adds a time delta in years, months and days.
```

Example: `today() + timedelta(days=2) > {due_date}`

### List function

The following function returns `true` if the search element is found in the document value which is a table column.

```text
list_contains(<value>, <search>)
```

Example:

```text
list_contains({item_description}, "car")
```

The function supports a list of values to be checked:

```text
list_contains(<value>, [<search>])
```

Example:

```text
list_contains({item_description}, ["car", "bus"]))
```

Keep in mind that:

- `contains` cannot be used for substring, please use regexp.
- `in` is not supported.

### String functions

#### `substring`

Returns `true` if the "search" substring is found in "value". This function is case sensitive.

```text
substring(<search>, <value>)
```

#### `regexp`

Returns `true` if the regular expression has a non-empty match to the document value.

```text
regexp(<regexp>, <value>)
```

Example:

```text
regexp("[Cc]ar$", {item_description})
```

Optional parameters:

- `ignore_case`: `regexp("rossum", {sender_name}, ignore_case=True)`

Please note that `re.search` is the underline function call and both parameters are cast to string.

#### `similarity`

Returns the Levenshtein distance between the document value and the selected string.

```text
similarity(<value>,<search>)
```

Example:

```text
similarity({item_description},"Car") > 2
```

Please find more information about Levenshtein distance here: https://en.wikipedia.org/wiki/Levenshtein_distance

### Table scope

There is a limitation in the implementation where one rule can only work with one table. This limitation ensures that error messages can be shown on the correct lines of the table.

To perform tasks like checking the number of lines in the tables, you can use aggregation functions. However, the expression engine does not permit using data points in one rule. Instead, you need to define variables to work with the data points.

```json
{
  "variables": {
    "len_first_table": ""
  }
}
```
---
title: 'Find & Replace Values'
sidebar_position: 1
slug: '/learn/find-replace-values'
---

import Deprecated from '../../\_deprecated.md';

<Deprecated />

# Find & Replace Values

_Formerly known as Value Transformations_

## Installation

Find & Replace Values extension is available in the Rossum store. To install the extension, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → Rossum Store**.
1. Search for **Find & Replace Values** extension and "Add" it.

A default "Rossum Store extension settings" page will open where you can configure the extension to your liking (visit [configuration examples](./configuration-examples.md) for inspiration).

## What can it do?

As the extension name suggests, its main purpose is to find and replace values of extracted datapoints. The most typical use case is normalizing extracted values (as in replacing unwanted characters and unifying the output). These are the tasks where this extension excels:

- Cleaning the existing value of the field and putting it into another (or the same) field based on a specific pattern.
- Finding a specific pattern inside a value and putting only it into another (or the same) field.
- Running Python conditions to determine if specified actions can be performed.
- Running regular expressions matches to determine if a transformation should be applied.
- Defining self-execution on a queue level.

For example, the following configuration removes non-alphanumeric characters from IBAN field:

```json
{
  "actions": [
    {
      "transformations": [
        {
          "pattern_to_replace": "[^a-zA-Z\\d]",
          "value_to_replace_with": ""
        }
      ],
      "source_target_mappings": [
        {
          "source": "iban",
          "target": "iban_normalized"
        }
      ]
    }
  ]
}
```

## What can't it do?

- React to "No match" scenarios: if no match is found, the value from the source field is copied "as-is" to the target field (you need to set up action condition to avoid this behavior).
- Provide conditional mapping of values: for this, you need to use another extension called [Copy & Paste Values](../copy-paste-values/index.md).

## How It Works

The main mechanism is Python `re` library, which handles regular expressions (regex).

All you need to know is that we do not use the whole spectrum of methods, but only the `substitute` function which has the following signature:

```python
re.sub(pattern, repl, string, count=0, flags=0)
```

- `pattern` is the regex pattern
- `repl` is the replacement string
- `string` is the input string (in our case, the value from a field)

In the configuration, you do not need to worry about the `string` parameter as it is taken from `source` fields configuration:

```json
{
  "source_target_mappings": [
    {
      "source": "item_description",
      "target": "item_description_normalized"
    }
  ]
}
```

However, you need to take care of `pattern` and `repl`. In our configuration, they are presented as `pattern_to_replace` and `value_to_replace_with` respectively.

See section [Available configuration options](#available-configuration-options) below for more parameters to customize the behavior.

## Available configuration options

```json
{
  // Required: List of actions to be performed.
  "actions": [
    {
      // Required: List of transformations to perform on the value of the source field.
      "transformations": [
        {
          // Required: The Python regular expression defines a pattern in the value to be found
          // and replaced.
          "pattern_to_replace": "[^a-zA-Z\\d]",

          // Required: This value will replace all pattern occurrences matching the regular
          // expression defined in the pattern_to_replace parameter.
          "value_to_replace_with": "",

          // Optional: Regular expression defines the condition for a transformation to be applied.
          // Extension will apply the transformation if the value matches the expression.
          "replace_if_this_pattern_matches": ""
        }
      ],
      // Required: List of source and target field schema IDs.
      "source_target_mappings": [
        {
          // Required: Source schema_id of the initial value to be transformed.
          "source": "iban",
          // Required: Target field's schema_id where the extension will store the transformed value.
          "target": "iban_normalized"
        }
      ],
      // Optional: Queue IDs where the extension should perform the particular action.
      // You can assign the extension to multiple queues and specify numerous actions for different
      // queues in one instance. If not present, the extension will act on all the queues to which
      // it is assigned unless excluded_queue_ids is set.
      "queue_ids": [],

      // Optional: Queue IDs where the function won't perform the action. This parameter cannot be
      // set along with queue_ids.
      "excluded_queue_ids": [],

      // Optional: Condition's definition for a particular action. Condition needs to evaluate to
      // bool (True or False). When defined, the action will be evaluated. If True, the extension
      // will apply the action's transformation; otherwise, it won’t. It's a Python condition where
      // schema fields are referenced by their schema_id enclosed in {}.
      //
      // Example: `{item_code} != '' and {document_type} == 'invoice'`
      //
      // See Python expressions documentation for more details: https://docs.python.org/3.8/reference/expressions.html
      "action_condition": "True",

      // Optional: Additional list of schema_ids that will trigger the action if a user modified
      // a field with such name.
      "additional_user_update_trigger": [],

      // Optional: If set to true, it prevents the action from overwriting the target value when
      // the user updates it manually.
      "allow_target_update": false
    }
  ]
}
```

:::tip

You can use `replace_if_this_pattern_matches` to apply a transformation. This means if the expression in this parameter is not fulfilled, the source value will be copied as-is to the target value.

:::
---
sidebar_position: 1
sidebar_label: 'Configuration examples'
title: 'Find & Replace Values: Configuration examples'
slug: '/learn/find-replace-values/configuration-examples'
---

import Deprecated from '../../\_deprecated.md';

<Deprecated />

# Configuration examples

The following examples are showing commonly used configurations when building applications using Rossum.ai platform.

## Copying item code to another field for specific line items

This configuration uses an action condition to check if the `document_type` header field has the value `tax_invoice` and checks all line items if attribute `item_code` is either `M0061` or `M0062`. The transformation is applied to line items that meet this condition.

Then the transformation removes all non-numerical characters and stores the transformed values in a separate field called `item_other` for each matched line item.

- Input: document type `tax_invoice` and item codes `[M0061, M0062]`
- Output: `[0061, 0062]`

```json
{
  "actions": [
    {
      "queue_ids": ["123"],
      "action_condition": "{document_type} == 'tax_invoice' and {item_code} in ['M0061','M0062']",
      "transformations": [
        {
          "pattern_to_replace": "[^0-9]",
          "value_to_replace_with": "",
          "replace_if_this_pattern_matches": ".+"
        }
      ],
      "source_target_mappings": [
        {
          "source": "item_code",
          "target": "item_other"
        }
      ]
    }
  ]
}
```

## Extract and normalize part of the line item description

This configuration uses two chained transformations to extract and normalize item code from the item description. The first transformation removes everything after the first space character in the string. The second one removes all hyphens from the result of the first transformation.

Notice also that there is an action condition defined in this configuration. The function will perform this action only when the Vendor Name is "Great Company." The condition is optional.

- Input: `1234-567-89 This is a line item description with the code at the beginning.`
- Output: `123456789`

```json
{
  "actions": [
    {
      "transformations": [
        {
          "pattern_to_replace": " ([\\s\\S]*)$",
          "value_to_replace_with": "",
          "replace_if_this_pattern_matches": " ([\\s\\S]*)$"
        },
        {
          "pattern_to_replace": "-",
          "value_to_replace_with": "",
          "replace_if_this_pattern_matches": "-"
        }
      ],
      "action_condition": "{sender_name} == 'Great Company'",
      "source_target_mappings": [
        {
          "source": "item_description",
          "target": "item_code"
        }
      ]
    }
  ]
}
```

## Normalize values

The following snippet removes all non-alphanumeric characters. The source datapoint IDs are `sender_vat_id` (Vendor VAT Number) and `iban`. The results have to be written to a different datapoint IDs `sender_vat_id_normalized` and `iban_normalized` in order not to affect the AI model.

```json
{
  "actions": [
    {
      "transformations": [
        {
          "pattern_to_replace": "[^a-zA-Z\\d]",
          "value_to_replace_with": ""
        }
      ],
      "source_target_mappings": [
        {
          "source": "sender_vat_id",
          "target": "sender_vat_id_normalized"
        },
        {
          "source": "iban",
          "target": "iban_normalized"
        }
      ]
    }
  ]
}
```

:::tip

Consider turning `sender_vat_id_normalized` into [formula field](../../rossum-formulas/formula-fields.md) instead where the same functionality can be rewritten as:

```python
''.join(filter(str.isalnum, fields.order_id.strip()))
```

:::

## Prepend and append values

The regular expressions use Python flavor which allows us to write references to capture groups as `\g<0>`, `\g<1>`, etc. The following example transforms order ID from `123` to `PO123/000` as an example (first prepend, later append):

```json
{
  "actions": [
    {
      "transformations": [
        {
          "pattern_to_replace": "^(?!(PO|$))(.+)$",
          "value_to_replace_with": "PO\\g<2>",
          "replace_if_this_pattern_matches": "^(?!(PO|$)).+$"
        },
        {
          "pattern_to_replace": "^(.+)(?<!/000)$",
          "value_to_replace_with": "\\g<1>/000",
          "replace_if_this_pattern_matches": "^.+(?<!/000)$"
        }
      ],
      "source_target_mappings": [
        {
          "source": "order_id",
          "target": "order_id_normalized"
        }
      ]
    }
  ]
}
```

It prepends/appends values only if they are not already present, and it handles empty values gracefully (no prepend/append).

Consider combining this approach with [named capturing groups](#using-named-capturing-groups-in-replace).

## Using named capturing groups in replace

Sometimes it is necessary to capture part of the input and either [append/prepend](#prepend-and-append-values) it or simply remove everything else. For these purposes, it can be quite handy to use **named** [capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups):

```json
{
  "actions": [
    {
      "transformations": [
        {
          "pattern_to_replace": "^[0-9]+-(?P<SKU>[0-9]+)$",
          "value_to_replace_with": "\\g<SKU>",
          "replace_if_this_pattern_matches": "^[0-9]+-[0-9]+$"
        }
      ],
      "source_target_mappings": [
        {
          "source": "item_code",
          "target": "item_code_norm"
        }
      ]
    }
  ]
}
```

Such pattern allows us to capture the SKU precisely and reference it later in `value_to_replace_with`.
---
title: 'Deprecated features'
sidebar_position: 999
---

import DocCardList from '@theme/DocCardList';

:::danger[Deprecated]

This page contains old articles, mostly consisting of deprecated extensions. We keep them here for backward compatibility purposes since many of our customers still use them.

Note that deprecated functionality won't receive any further updates and is likely to be completely removed in the future. Please consider using other available alternatives or contacting support@rossum.ai for further assistance.

:::

<DocCardList />
---
title: 'Copy & Paste Values'
slug: '/learn/copy-paste-values'
---

import Deprecated from '../../\_deprecated.md';
import WIP from '../../\_wip.md';

<Deprecated />

# Copy & Paste Values

_Formerly known as Value Operations_

## Installation

Copy & Paste Values extension is available in the Rossum store. To install the extension, follow these steps:

1. Login to your Rossum account.
1. Navigate to **Extensions → Rossum Store**.
1. Search for **Copy & Paste Values** extension and "Add" it.

A default "Rossum Store extension settings" page will open where you can configure the extension to your liking (visit [configuration examples](./configuration-examples.md) for inspiration).

## Basic usage

<WIP />

## Available configuration options

```json
{
  "operations": [
    {
      // Under what condition should the source field be copied?
      "condition": "len({line_items}) > 0 and {item_po} == ''",

      // Source field from where the source value should be copied.
      "source_field": "order_id",

      // Target field to where the source value should be copied.
      "target_field": "item_po_copy"
    }

    // … more operations
  ]
}
```
---
sidebar_position: 1
sidebar_label: 'Configuration examples'
title: 'Copy & Paste Values: Configuration examples'
slug: '/learn/copy-paste-values/configuration-examples'
---

import Deprecated from '../../\_deprecated.md';

<Deprecated />

# Configuration examples

## Copy header fields to line items conditionally

Sometimes it is necessary to copy header field value (such as Purchase Order no.) to line items for later data matching. The best way how to do this is to create a hidden field on line items (`item_po_copy`) and conditionally fill it like so:

```json
{
  "operations": [
    {
      "//": "1.1: Copy the Purchase Order no. only if it doesn't exist on the line item already:",
      "condition": "len({line_items}) > 0 and {item_po} == ''",
      "source_field": "order_id",
      "target_field": "item_po_copy"
    },
    {
      "//": "1.2: Copy the Purchase Order no. from the line items:",
      "condition": "len({line_items}) > 0 and {item_po} != ''",
      "source_field": "item_po",
      "target_field": "item_po_copy"
    }
  ]
}
```

It might be necessary to have **manual** PO number overwrite on header fields. In this case, the logic would be similar:

```json
{
  "operations": [
    {
      "//": "1.1: Copy the Purchase Order no. only if it doesn't exist on the line item already (and there is no manual overwrite):",
      "condition": "len({line_items}) > 0 and {item_order_id} == '' and {order_id_manual} == ''",
      "source_field": "order_id",
      "target_field": "item_order_id_copy"
    },
    {
      "//": "1.2: Copy the Purchase Order Manual no. only if it doesn't exist on the line item already:",
      "condition": "len({line_items}) > 0 and {item_order_id} == '' and {order_id_manual} != ''",
      "source_field": "order_id_manual",
      "target_field": "item_order_id_copy"
    },
    {
      "//": "1.3: Copy the Purchase Order no. from the line items:",
      "condition": "len({line_items}) > 0 and {item_order_id} != ''",
      "source_field": "item_order_id",
      "target_field": "item_order_id_copy"
    }
  ]
}
```
---
title: Multiple data matching results
slug: multiple-data-matching-results
authors: [mrtnzlml]
tags: [master-data-hub, rossum-formulas]
---

import QuizComponent from '@site/src/components/QuizComponent';

Data matching (Master Data Hub; MDH) results are by default returned into an "enum" field in Rossum (also known as "Options" field). Enum field allows to select only one of the values returned (it behaves as a regular [HTML select element](https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/select) with options). What if you need to select multiple values, however?

<!-- truncate -->

The solution is to return all values into a **hidden** enum field and later process the data using a simple serverless function. In our example, the hidden field is `order_id_match`:

```python
from txscript import TxScript, default_to, substitute


def rossum_hook_request_handler(payload):
    x = TxScript.from_payload(payload)

    # Do not recalculate if the input data has not changed
    # (add more fields depending on the input into your MDH query):
    recalculate_hash = f"{x.field.order_id}__{x.field.grn_id}"
    if x.field.recalculate_hash == recalculate_hash:
        return x.hook_response()

    # Get all valid enum options (result of the MDH query):
    order_line_options = default_to(x.field.order_id_match.attr.options, [])
    valid_options = [
        opt for opt in order_line_options if opt.value
    ]

    # Clear existing multivalue table:
    x.field.order_id_lines = []

    # Create new multivalue table:
    new_lines = []
    for option in valid_options:
        new_lines.append({
            "order__id": option.value
            # What about other columns? Keep reading. :)
        })

    # Insert new values into the multivalue table:
    x.field.order_id_lines = new_lines
    x.field.recalculate_hash = recalculate_hash

    return x.hook_response()
```

The function does the following:

1. Get all valid options from the `order_id_match` enum field.
2. Clear the existing `order_id_lines` table (our destination).
3. Insert the enum values into the `order_id_lines` destination table.

Additionally, it takes care of recalculating the table lazily so users can update the final table manually if needed (see the `recalculate_hash` field).

This way, all data matching results were populated into multivalue table despite the data matching supporting only enum fields.

Note that it might be a good idea to lazily load additional values in the table. In real-world solution, the chain of hooks would look like this:

```text
.---------.      .--------------------------.      .---------.
|  MDH 1  | ---> |  Custom hook from above  | ---> |  MDH 2  |
`---------`      `--------------------------`      `---------`
```

This way, we can distribute only row IDs from the first MDH extension and load the actual data in the second MDH extension. Alternatively, we could populate all the data in the first MDH hook. That is however a bit laborious when there are many columns to populate and distribute.

<QuizComponent
question="Can master data hub extension return values to a string field?"
answers={[
{ text: 'Yes, MDH can return values into any preconfigured field' },
{ text: 'No, only enum fields are supported', isCorrect: true }
]}>
The only field type that Master Data Hub (MDH) extension supports are `enum` fields. Enum fields limit the section to only one specific value. However, we can still access all the values (options) using the `x.field.order_id_match.attr.options` code which can later be distributed into a multivalue table.
</QuizComponent>
